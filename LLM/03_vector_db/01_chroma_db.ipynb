{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:08<00:00, 9.35MiB/s]  \n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=[\n",
    "        'This is a document about pineapple',\n",
    "        'This is a document about mango',\n",
    "        'This is a document about strawberry'\n",
    "    ],\n",
    "    ids=['id1', 'id2', 'id3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=['This is a query document about vietnam'],\n",
    "    n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1', 'id2', 'id3']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['This is a document about pineapple',\n",
       "   'This is a document about mango',\n",
       "   'This is a document about strawberry']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None]],\n",
       " 'distances': [[1.2225853204727173, 1.2783520221710205, 1.3223111629486084]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciQ dataset 활용 ChromaDB 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 11679/11679 [00:00<00:00, 103824.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "    num_rows: 10481\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sciq\", split=\"train\")\n",
    "dataset  = dataset.filter(lambda x: x[\"support\"] != \"\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma db 클라이언트 객체 및 콜렉션 생성\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"sciq_support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 로드\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = dataset[\"support\"][:100]\n",
    "support_embeddings = embedding_model.encode(supports).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(support_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids=[str(i) for i in range(0, 100)],\n",
    "    embeddings=support_embeddings,\n",
    "    metadatas=[{\"type\": \"support\", \"text\": text} for text in supports]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = dataset[\"question\"][:3]\n",
    "question_embeddings = embedding_model.encode(questions).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=question_embeddings,\n",
    "    n_results=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['36'], ['1'], ['2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[None], [None], [None]],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'text': 'Agents of Decomposition The fungus-like protist saprobes are specialized to absorb nutrients from nonliving organic matter, such as dead organisms or their wastes. For instance, many types of oomycetes grow on dead animals or algae. Saprobic protists have the essential function of returning inorganic nutrients to the soil and water. This process allows for new plant growth, which in turn generates sustenance for other organisms along the food chain. Indeed, without saprobe species, such as protists, fungi, and bacteria, life would cease to exist as all organic carbon became “tied up” in dead organisms.',\n",
       "    'type': 'support'}],\n",
       "  [{'text': 'Without Coriolis Effect the global winds would blow north to south or south to north. But Coriolis makes them blow northeast to southwest or the reverse in the Northern Hemisphere. The winds blow northwest to southeast or the reverse in the southern hemisphere.',\n",
       "    'type': 'support'}],\n",
       "  [{'text': 'Summary Changes of state are examples of phase changes, or phase transitions. All phase changes are accompanied by changes in the energy of a system. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. The conversion of a solid to a liquid is called fusion (or melting). The energy required to melt 1 mol of a substance is its enthalpy of fusion (ΔHfus). The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization (ΔHvap). The direct conversion of a solid to a gas is sublimation. The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation (ΔHsub) and is the sum of the enthalpies of fusion and vaporization. Plots of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves. Heating curves relate temperature changes to phase transitions. A superheated liquid, a liquid at a temperature and pressure at which it should be a gas, is not stable. A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature. Instead, they form a supercooled liquid, a metastable liquid phase that exists below the normal melting point. Supercooled liquids usually crystallize on standing, or adding a seed crystal of the same or another substance can induce crystallization.',\n",
       "    'type': 'support'}]],\n",
       " 'distances': [[1.104844093322754],\n",
       "  [0.46667468547821045],\n",
       "  [0.9318017363548279]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      "Support: Agents of Decomposition The fungus-like protist saprobes are specialized to absorb nutrients from nonliving organic matter, such as dead organisms or their wastes. For instance, many types of oomycetes grow on dead animals or algae. Saprobic protists have the essential function of returning inorganic nutrients to the soil and water. This process allows for new plant growth, which in turn generates sustenance for other organisms along the food chain. Indeed, without saprobe species, such as protists, fungi, and bacteria, life would cease to exist as all organic carbon became “tied up” in dead organisms.\n",
      "\n",
      "Question: What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
      "Support: Without Coriolis Effect the global winds would blow north to south or south to north. But Coriolis makes them blow northeast to southwest or the reverse in the Northern Hemisphere. The winds blow northwest to southeast or the reverse in the southern hemisphere.\n",
      "\n",
      "Question: Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?\n",
      "Support: Summary Changes of state are examples of phase changes, or phase transitions. All phase changes are accompanied by changes in the energy of a system. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. The conversion of a solid to a liquid is called fusion (or melting). The energy required to melt 1 mol of a substance is its enthalpy of fusion (ΔHfus). The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization (ΔHvap). The direct conversion of a solid to a gas is sublimation. The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation (ΔHsub) and is the sum of the enthalpies of fusion and vaporization. Plots of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves. Heating curves relate temperature changes to phase transitions. A superheated liquid, a liquid at a temperature and pressure at which it should be a gas, is not stable. A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature. Instead, they form a supercooled liquid, a metastable liquid phase that exists below the normal melting point. Supercooled liquids usually crystallize on standing, or adding a seed crystal of the same or another substance can induce crystallization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(questions):\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Support:\", results['metadatas'][i][0]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma DB를 활용한 키워드 기반 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"인공지능은 인간의 작업을 자동화하는 기술이다.\",\n",
    "    \"기계 학습은 데이터에서 패턴을 학습하여 예측하는 기술이다.\",\n",
    "    \"벡터 데이터베이스는 유사도를 기반으로 데이터를 검색하는 DB이다.\",\n",
    "    \"AI는 사람들이 하는 일을 대신 수행할 수 있도록 돕는 기술이다.\",\n",
    "    \"머신 러닝은 컴퓨터가 데이터를 분석하여 스스로 배우는 알고리즘이다.\",\n",
    "    \"벡터 DB는 데이터를 빠르게 검색하기 위해 유사도 기반으로 동작한다.\",\n",
    "    \"인공지능은 알고리즘을 사용하여 문제를 해결하는 시스템이다.\",\n",
    "    \"기계 학습을 통해 컴퓨터는 경험을 바탕으로 더 나은 결정을 내릴 수 있다.\",\n",
    "    \"벡터 데이터베이스는 고속 검색과 유사도 검색에 강점을 가진 데이터베이스이다.\",\n",
    "    \"AI 기술은 자동화된 시스템을 통해 사람들의 작업을 효율적으로 돕는다.\",\n",
    "    \"기계 학습 모델은 주어진 데이터를 학습하여 미래의 결과를 예측한다.\",\n",
    "    \"벡터 DB는 데이터의 유사도를 계산하여 관련성이 높은 정보를 빠르게 찾는다.\",\n",
    "    \"인공지능은 복잡한 문제를 해결하기 위해 인간의 지능을 모방하는 시스템이다.\",\n",
    "    \"기계 학습 알고리즘은 데이터를 통해 패턴을 발견하고, 이를 통해 예측을 개선한다.\",\n",
    "    \"벡터 데이터베이스는 대량의 데이터를 효율적으로 처리하고 빠른 검색을 가능하게 한다.\",\n",
    "    \"대형 언어 모델(LLM)은 자연어를 이해하고 생성할 수 있는 인공지능 모델이다.\",\n",
    "    \"트랜스포머 모델은 대형 언어 모델에서 주로 사용되는 네트워크 아키텍처이다.\",\n",
    "    \"자연어 처리(NLP)는 텍스트 데이터에서 의미를 추출하고 분석하는 기술이다.\",\n",
    "    \"파인튜닝(fine-tuning)은 사전 학습된 모델을 특정 작업에 맞게 조정하는 과정이다.\",\n",
    "    \"GPT는 대형 언어 모델 중 하나로, 텍스트 생성에 매우 뛰어난 성능을 보인다.\",\n",
    "    \"언어 모델은 문법적 의미를 이해하고, 자연스러운 문장을 생성하는데 사용된다.\",\n",
    "    \"BERT는 텍스트의 양방향 문맥을 학습하여 자연어 처리에서 뛰어난 성능을 발휘하는 모델이다.\",\n",
    "    \"자연어 생성(NLG)은 기계가 인간처럼 문장을 생성하는 기술이다.\",\n",
    "    \"LLM은 방대한 양의 텍스트 데이터를 학습하여 질문에 답하고 정보를 제공할 수 있다.\",\n",
    "    \"어텐션 메커니즘은 트랜스포머 모델에서 핵심적인 역할을 하는 기술로, 입력 문장에 대한 중요한 부분에 집중한다.\",\n",
    "    \"GPT 모델은 대화형 AI 시스템에서 자연스러운 대화 생성을 가능하게 한다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ChromaDB 클라이언트, 컬렉션 생성\n",
    "client = chromadb.PersistentClient(path='./chroma_db')\n",
    "collection = client.get_or_create_collection(name='ai_documents')\n",
    "\n",
    "# 텍스트 임베딩 모델 로드 \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    embedding = model.encode(doc).tolist()\n",
    "    collection.add(\n",
    "        ids=[str(i)], \n",
    "        embeddings=[embedding], \n",
    "        metadatas=[{\"text\": doc}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서: 자연어 처리(NLP)는 텍스트 데이터에서 의미를 추출하고 분석하는 기술이다.\n",
      "검색된 문서: 자연어 생성(NLG)은 기계가 인간처럼 문장을 생성하는 기술이다.\n"
     ]
    }
   ],
   "source": [
    "query_keyword = \"NLP\"\n",
    "\n",
    "query_embedding = model.encode(query_keyword).tolist()\n",
    "\n",
    "results = collection.query(query_embeddings=query_embedding, n_results=2)\n",
    "\n",
    "for result in results[\"metadatas\"][0]:\n",
    "    print('검색된 문서:', result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 추천 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/tmdb_5000_movies.csv')\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = chromadb.PersistentClient(path='./chroma_db')\n",
    "collection = client.get_or_create_collection(name=\"movies\")\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = [\n",
    "    {\n",
    "        \"id\": str(index),\n",
    "        \"title\": row[\"title\"],\n",
    "        \"overview\": row[\"overview\"] if pd.notna(row[\"overview\"]) else \"\"\n",
    "    } for index, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movies:\n",
    "    if movie['overview']:\n",
    "        overview_embedding = model.encode(movie['overview']).tolist()\n",
    "        collection.add(\n",
    "            ids=[movie['id']],\n",
    "            embeddings=[overview_embedding],\n",
    "            metadatas=[{'title': movie['title'], 'text': movie['overview']}]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n",
      "Cobb, a skilled thief who commits corporate espionage by infiltrating the subconscious of his targets is offered a chance to regain his old life as payment for a task considered to be impossible: \"inception\", the implantation of another person's idea into a target's subconscious.\n",
      "\n",
      "Identity Thief\n",
      "When a mild-mannered businessman learns his identity has been stolen, he hits the road in an attempt to foil the thief -- a trip that puts him in the path of a deceptively harmless-looking woman.\n",
      "\n",
      "The Master of Disguise\n",
      "A sweet-natured Italian waiter named Pistachio Disguisey at his father Fabbrizio's restaurant, who happens to be a member of a family with supernatural skills of disguise. But moments later the patriarch of the Disguisey family is kidnapped Fabbrizio's former arch-enemy, Devlin Bowman, a criminal mastermind in an attempt to steal the world's most precious treasures from around the world. And it's up to Pistachio to track down Bowman and save his family before Bowman kills them!\n",
      "\n",
      "The Death and Life of Bobby Z\n",
      "A DEA agent provides former Marine Tim Kearney with a way out of his prison sentence: impersonate Bobby Z, a recently deceased drug dealer, in a hostage switch with a crime lord. When the negotiations go awry, Kearney flees, with Z's son in tow.\n",
      "\n",
      "Absolute Power\n",
      "A master thief coincidentally is robbing a house where a murder in which the President of The United States is involved occurs in front of his eyes. He is forced to run yet may hold evidence that could convict the President. A political thriller from and starring Clint Eastwood and based on a novel by David Baldacci.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 제목 입력 -> 줄거리를 찾아 -> 줄거리로 유사도 검색\n",
    "input_title = 'Inception'\n",
    "query_text = df.loc[df['title'] == input_title, 'overview'].iloc[0]\n",
    "\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "\n",
    "for result in results['metadatas'][0]:\n",
    "    print(result['title'])\n",
    "    print(result['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silmido\n",
      "On 31 January 1968, 31 North Korean commandos infiltrated South Korea in a failed mission to assassinate President Park Chung-hee. In revenge, the South Korean military assembled a team of 31 criminals on the island of Silmido to kill Kim Il-sung for a suicide mission to redeem their honor, but was cancelled, leaving them frustrated. It is loosely based on a military uprising in the 1970s.\n",
      "\n",
      "Tae Guk Gi: The Brotherhood of War\n",
      "In 1950, in South Korea, shoe-shiner Jin-tae Lee and his 18-year-old old student brother, Jin-seok Lee, form a poor but happy family with their mother, Jin-tae's fiancé Young-shin Kim, and her young sisters. Jin-tae and his mother are tough workers, who sacrifice themselves to send Jin-seok to the university. When North Korea invades the South, the family escapes to a relative's house in the country, but along their journey, Jin-seok is forced to join the army to fight in the front, and Jin-tae enlists too to protect his young brother. The commander promises Jin-tae that if he gets a medal he would release his brother, and Jin-tae becomes the braver soldier in the company. Along the bloody war between brothers, the relationship of Jin-seok with his older brother deteriorates leading to a dramatic and tragic end.\n",
      "\n",
      "Operation Chromite\n",
      "A squad of soldiers fight in the Korean War's crucial Battle of Incheon.\n",
      "\n",
      "The Interview\n",
      "Dave Skylark and his producer Aaron Rapoport run the celebrity tabloid show \"Skylark Tonight\". When they land an interview with a surprise fan, North Korean dictator Kim Jong-un, they are recruited by the CIA to turn their trip to Pyongyang into an assassination mission.\n",
      "\n",
      "Skin Trade\n",
      "After his family is killed by a Serbian gangster with international interests. NYC detective Nick goes to S.E. Asia and teams up with a Thai detective to get revenge and destroy the syndicates human trafficking network.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 원하는 줄거리 입력 -> 유사도 검색\n",
    "# query_text = \"a movie that penetrates the subconscious and saves the world\"\n",
    "query_text = \"korea\"\n",
    "\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "\n",
    "for result in results['metadatas'][0]:\n",
    "    print(result['title'])\n",
    "    print(result['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문 pdf 내용 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\user\\anaconda3\\envs\\vectordb_env\\lib\\site-packages (from PyPDF2) (4.12.2)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = chromadb.PersistentClient(path='./chroma_db')\n",
    "client.delete_collection('papers')    # 컬렉션 삭제\n",
    "collection = client.get_or_create_collection(name='papers')\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    {'id': '1', 'title': '딥러닝', 'path': './data/deep_learning.pdf'},\n",
    "    {'id': '2', 'title': '자연어처리', 'path': './data/nlp_paper.pdf'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in papers:\n",
    "    text = extract_text_from_pdf(paper['path'])\n",
    "    embedding = model.encode(text).tolist()\n",
    "    collection.add(\n",
    "        ids=[paper['id']],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[{'title': paper['title']}],\n",
    "        documents=[text]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1', '2'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['HAL Id: hal-04206682\\nhttps://hal.science/hal-04206682v1\\nSubmitted on 14 Sep 2023\\nHAL is a multi-disciplinary open access\\narchive for the deposit and dissemination of sci-\\nentific research documents, whether they are pub-\\nlished or not. The documents may come from\\nteaching and research institutions in F rance or\\nabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL , est\\ndestinée au dépôt et à la diffusion de documents\\nscientifiques de niveau recherche, publiés ou non,\\némanant des établissements d’enseignement et de\\nrecherche français ou étrangers, des laboratoires\\npublics ou privés.\\nDeep learning\\nY ann Lecun, Y oshua Bengio, Geoffrey Hinton\\nT o cite this version:\\nY ann Lecun, Y oshua Bengio, Geoffrey Hinton. Deep learning. Nature, 2015, 521 (7553), pp.436-444.\\n\\uffff10.1038/nature14539\\uffff. \\uffffhal-04206682\\uffff 1Facebook AI Research, 770 Broadway, New York, New York 10003 USA\\n2New York University, 715 Broadway, New York, New York 10003, USA\\n3Department of Computer Science and Operations Research Université de Montréal, Pavillon André-Aisenstadt, PO Box 6128  Centre-Ville STN Montréal, Quebec H3C 3J7, Canada.\\n4Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA\\n5Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada\\nMachine-learning technology powers many aspects of modern \\nsociety: from web searches to content filtering on social net -\\nworks to recommendations on e-commerce websites, and \\nit is increasingly present in consumer products such as cameras and \\nsmartphones. Machine-learning systems are used to identify objects \\nin images, transcribe speech into text, match news items, posts or \\nproducts with users’ interests, and select relevant results  of search. \\nIncreasingly, these applications make use of a class of techniques called \\ndeep learning. \\nConventional machine-learning techniques were limited in  their \\nability to process natural data in their raw form. For decades, con -\\nstructing a pattern-recognition or machine-learning system required \\ncareful engineering and considerable domain expertise to design a fea -\\nture extractor that transformed the raw data (such as the pixel values \\nof an image) into a suitable internal representation or feature vector \\nfrom which the learning subsystem, often a classifier, could detect or \\nclassify patterns in the input. \\nRepresentation learning is a set of methods that allows a machine to \\nbe fed with raw data and to automatically discover the representations \\nneeded for detection or classification. Deep-learning methods are \\nrepresentation-learning methods with multiple levels of representa -\\ntion, obtained by composing simple but non-linear modules t hat each \\ntransform the representation at one level (starting with the raw input) \\ninto a representation at a higher, slightly more abstract level. With the \\ncomposition of enough such transformations, very complex functions \\ncan be learned. For classification tasks, higher layers of representation \\namplify aspects of the input that are important for discrimination and \\nsuppress irrelevant variations. An image, for example, comes i n the \\nform of an array of pixel values, and the learned features in the first \\nlayer of representation typically represent the presence or absence of \\nedges at particular orientations and locations in the image. The second \\nlayer typically detects motifs by spotting particular arrangements of \\nedges, regardless of small variations in the edge positions. The third \\nlayer may assemble motifs into larger combinations that correspond \\nto parts of familiar objects, and subsequent layers would detect objects \\nas combinations of these parts. The key aspect of deep learning is that \\nthese layers of features are not designed by human engineers: they \\nare learned from data using a general-purpose learning procedure. \\nDeep learning is making major advances in solving problems that \\nhave resisted the best attempts of the artificial intelligence co mmu -\\nnity for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is theref ore applica -\\nble to many domains of science, business and government. In addition \\nto beating records in image recognition1–4 and speech recognition5–7, it \\nhas beaten other machine-learning techniques at predicti ng the activ -\\nity of potential drug molecules8, analysing particle accelerator data9,10, \\nreconstructing brain circuits11, and predicting the effects of mutations \\nin non-coding DNA on gene expression and disease12,13. Perhaps more \\nsurprisingly, deep learning has produced extremely promising results \\nfor various tasks in natural language understanding14, particularly \\ntopic classification, sentiment analysis, question answering15 and lan -\\nguage translation16,17. \\nWe think that deep learning will have many more successes in the \\nnear future because it requires very little engineering by hand, so it \\ncan easily take advantage of increases in the amount of available com -\\nputation and data. New learning algorithms and architectures that are \\ncurrently being developed for deep neural networks will only acceler -\\nate this progress. \\nSupervised learning \\nThe most common form of machine learning, deep or not, is super -\\nvised learning. Imagin e that we want t o bui ld a syst em that ca n classify  \\nimages as containing, sa y, a house, a car, a person or a pet. We first  \\ncollect a large data set of images of houses, cars, people and pets, each  \\nlabelled with its category. During training, the machine is shown an  \\nimage and produces an output in the form of a vector of scores, one  \\nfor each category. We want the desired category to have the highest  \\nscore of all categories, but this is unlikely to happen before training.  \\nWe compute an objective function that measures the error (or dis -\\ntance) betwe en th e outpu t scor es and the desired patter n of scor es. The \\nmachine then modifies its internal adjustable parameters to reduce  \\nthis error . These adjustab le parameters, often called weigh ts, are real  \\nnumbers that can be seen as ‘knobs’ that define the input–output func -\\ntion of the machine. I n a typical deep-learnin g system, there may be  \\nhundreds of millions of these adjustable weights, and hundreds of  \\nmillions of labelled examp les with which to train the machine.  \\nTo properly adjust the weight vector, the learning algorithm com -\\nputes a gradient vector that, for each weight, indicates by what amount \\nthe error would increase or decrease if the weight were increased by a \\ntiny amount. The weight vector is then adjusted in the opposite direc -\\ntion to the gradient vector. \\nThe objective function, averaged over all the training examples, can Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels \\nof abstraction. These methods have dramatically improved the state-of-the-art in speech rec-ognition, visual object recognition, object detection \\nand many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the \\nbackpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in \\neach layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, \\nspeech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. Deep  learning\\nYann LeCun1,2, Yoshua Bengio3 & Geoffrey Hinton4,5\\n1 | 9 be seen as a kind of hilly landscape in the high-dimensional space of \\nweight values. The negative gradient vector indicates the direction \\nof steepest descent in this landscape, taking it closer to a minimum, \\nwhere the output error is low on average. \\nIn practice, most practitioners use a procedure called stocha stic \\ngradient descent (SGD). This consists of showing the input vector \\nfor a few examples, computing the outputs and the errors, computing \\nthe average gradient for those examples, and adjusting the weights \\naccordingly. The process is repeated for many small sets of examples \\nfrom the training set until the average of the objective function stops \\ndecreasing. It is called stochastic because each small set of examples \\ngives a noisy estimate of the average gradient over all examples. This \\nsimple procedure usually finds a good set of weights surprisingly \\nquickly when compared with far more elaborate optimization tech -\\nniques18. After training, the performance of the system is measured \\non a different set of examples called a test set. This serves to test the \\ngeneralization ability of the machine — its ability to produce sensible \\nanswers on new inputs that it has never seen during training. Many of the current practical applications of machine learning use \\nlinear classifiers on top of hand-engineered features. A two-class linear \\nclassifier computes a weighted sum of the feature vector components. \\nIf the weighted sum is above a threshold, the input is classified as \\nbelonging to a particular category. \\nSince the 1960s we have known that linear classifiers can only carve \\ntheir input space into very simple regions, namely half-spaces sepa -\\nrated by a hyperplane19. But problems such as image and speech recog -\\nnition require the input–output function to be insensiti ve to irrelevant \\nvariations of the input, such as variations in position, orientation or \\nillumination of an object, or variations in the pitch or accent of speech, \\nwhile being very sensitive to particular minute variations (for example, \\nthe difference between a white wolf and a breed of wolf-like white \\ndog called a Samoyed). At the pixel level, images of two Samoyeds in \\ndifferent poses and in different environments may be very different \\nfrom each other, whereas two images of a Samoyed and a wolf in the \\nsame position and on similar backgrounds may be very similar to each \\nother. A linear classifier, or any other ‘shallow’ classifier operating on \\nFigure 1 - Multilayer neural networks and backpropagation.  a, A multi-\\nlayer neura l network (shown by the connected dots) can distort the input  \\nspace to make the classes of data (examples o f which are on the red and  \\nblue lines) linearly separable. Not e how a regular grid  (shown on the left)  \\nin input space i s also transformed  (shown in the middle panel)  by hidden  \\nunits. This is an illustrative example with only two input units, two hidden  \\nunits and one output unit, but the networks use d for object recognition  \\nor natural language processing contain tens or hundreds of thousand s of \\nunits. Reproduced with permission from C.  Olah (http://colah.github.io/).  \\nb, The chain rule o f derivatives tells us how two small effects (that of a small  \\nchange  of x on y, and that o f y on z) are composed. A  small change Δ x in \\nx gets transformed first into a small change Δ y in y by getting multiplied  \\nby ∂y/∂x (that is, the definition of partial derivativ e). Similarly, the change  \\nΔy creates a change Δ z in z. Substituting one equation into the other  \\ngives the chain rule of derivatives — how Δ x gets turned into Δ z through  \\nmultiplication by the product of ∂y/∂x and ∂z/∂x. It als o works wh en x, \\ny and  z are vectors (and the derivatives are Jacobian matr ices). c, The  \\nequations used for computing the forward pass in a neura l net with two  \\nhidden layers and one output layer, each constituting a module through  which one can backpropagate gradients. At each layer, we first compute \\nthe total input z to each unit, which is a weighted sum of the outputs of \\nthe units in the layer below. Then a non-linear function f(.) is applied to \\nz to get the output of the unit. For simplicity, we have omitted bias terms. \\nThe non-linear functions used in neural networks include the rectified \\nlinear unit (ReLU) f(z) = max(0, z), commonly used in recent years, as \\nwell as the more conventional sigmoids, such as the hyberbolic tangent, \\nf(z) = (exp( z) − exp(− z))/(exp( z) + exp(− z)) and logistic function logistic, \\nf(z) = 1/(1 + exp(−z)). d, The equations used for computing the backward \\npass. At each hidden layer we compute the error derivative with respect to  \\nthe output of each unit, which is a weighted sum of the error derivatives \\nwith respect to the total inputs to the units in the layer above. We then \\nconvert the error derivative with respect to  the output into the error \\nderivative with respect to the input by multiplying it by the gradient of f(z). \\nAt the output layer, the error derivative with respect to the output of a unit \\nis computed by differentiating the cost function. This gives yl − tl if the cost \\nfunction for unit l is 0.5( yl − tl)2, where tl is the target value. Once the ∂ E/∂zk \\nis known, the error-derivative for the weight wjk on the connection from \\nunit j in the layer below is just yj ∂E/∂zk.\\nInput\\n(2)\\nOutput\\n(1 sigmoid)Hidden\\n(2 sigmoid)a b\\nd cy\\nyxyx\\uf0b6\\uf0b6= yz\\n\\uf0b6\\uf0b6\\nxy\\n\\uf0b6\\uf0b6zyz zy\\uf0b6\\uf0b6=Δ Δ\\nΔ Δ\\nΔ Δzyz\\nxyx\\uf0b6\\uf0b6\\n\\uf0b6\\uf0b6=\\nxz\\nyz\\nx xy\\n\\uf0b6\\uf0b6\\n\\uf0b6\\uf0b6\\n\\uf0b6\\uf0b6=\\nCompare outputs with correct \\nanswer to get error derivatives\\njkE\\nyl=yltl\\nE\\nzl=E\\nylyl\\nzll\\nE\\nyj= wjkE\\nzk\\nE\\nzj=E\\nyjyj\\nzjE\\nyk= wklE\\nzl\\nE\\nzk=E\\nykyk\\nzkwkl\\nwjk\\nwij\\nijkyl=f(zl)\\nzl= wklykl\\nyj=f(zj)\\nzj= wijxiyk=f(zk)\\nzk= wjkyj\\nOutput units \\nInput units Hidden units H2 \\nHidden units H1 wkl\\nwjk\\nwijk \\uf065 H2\\nk \\uf065 H2I \\uf065 out\\nj \\uf065 H1\\ni \\uf065 Input\\ni\\n2 | 9 rule for derivatives. The key insight is that the derivative (or gradi -\\nent) of the objective with respect to the input of a module can be \\ncomputed by working backwards from the gradient with respect to \\nthe output of that module (or the input of the subsequent modu le) \\n(Fig.\\xa01). The backpropagation equation can be applied repeatedly to \\npropagate gradients through all modules, starting from the output \\nat the top (where the network produces its prediction) all the way to \\nthe bottom (where the external input is fed). Once these gradients \\nhave been computed, it is straightforward to compute the gradients \\nwith respect to the weights of each module. \\nMany applications of deep learning use feedforward neural net -\\nwork architectures (Fig. 1), which learn to map a fixed-size input \\n(for example, an image) to a fixed-size output (for example, a prob -\\nability for each of several categories). To go from one layer to the \\nnext, a set of units compute a weighted sum of their inputs from the \\nprevious layer and pass the result through a non-linear function. At \\npresent, the most popular non-linear function is the rectified linear \\nunit (ReLU), which is simply the half-wave rectifier f(z) = max( z, 0). \\nIn past decades, neural nets used smoother non-linearities, such as \\ntanh( z) or 1/(1 + exp(−z)), but the ReLU typically learns much faster \\nin networks with many layers, allowing training of a deep supervised \\nnetwork without unsupervised pre-training28. Units that are not in \\nthe input or output layer are conventionally called hidden units. The \\nhidden layers can be seen as distorting the input in a non-linear way \\nso that categories become linearly separable by the last layer (Fig.\\xa01). \\nIn the late 1990s, neural nets and backpropagation were largely \\nforsaken by the machine-learning community and ignored by the \\ncomputer-vision and speech-recognition communities. It was widely \\nthought that learning useful, multistage, feature extractors with lit -\\ntle prior knowledge was infeasible. In particular, it was commonly \\nthought that simple gradient descent would get trapped in poor local \\nminima — weight configurations for which no small change would \\nreduce the average error. \\nIn practice, poor local minima are rarely a problem with large net -\\nworks. Regardless of the initial conditions, the system nearly always \\nreaches solutions of very similar quality. Recent theoretical and \\nempirical results strongly suggest that local minima are not a serious \\nissue in general. Instead, the landscape is packed with a combinato -\\nrially large number of saddle points where the gradient is zero, and \\nthe surface curves up in most dimensions and curves down in the Figure 2 - Inside a convolutional network.  The outputs (not the filters) \\nof each layer (horizontally) of a typical convolutional networ k architecture  \\napplied to the imag e of a Samoyed  dog (bottom left; and RGB (red, green,  \\nblue) inputs, bottom right). Each rectangula r image is a feature map  \\nraw pixels could not possibly distinguish the latter two, while putting \\nthe former two in the same category. This is why shallow classifiers \\nrequire a good feature extractor that solves the selectivit y–invariance \\ndilemma — one that produces representations that are selective to \\nthe aspects of the image that are important for discrimination, but \\nthat are invariant to irrelevant aspects such as the pose of the animal. \\nTo make classifiers more powerful, one can use generic non-linear \\nfeatures, as with kernel methods20, but generic features such as those \\narising with the Gaussian kernel do not allow the learner to general -\\nize well far from the training examples21. The conventional option is \\nto hand design good feature extractors, which requires a consider -\\nable amount of engineering skill and domain expertise. But this can \\nall be avoided if good features can be learned automatically  using a \\ngeneral-purpose learning procedure. This is the key advantage of \\ndeep learning. \\nA deep-learning architecture is a multilayer stack of simple mod -\\nules, all (or most) of which are subject to learning, and many of which \\ncompute non-linear input–output mappings. Each module in the \\nstack transforms its input to increase both the selectivity and the \\ninvariance of the representation. With multiple non-linear layers, say \\na depth of 5 to 20, a system can implement extremely intricate func -\\ntions of its inputs that are simultaneously sensitive to minute details \\n— distinguishing Samoyeds from white wolves — and insensitive to \\nlarge irrelevant variations such as the background, pose, lighting and \\nsurrounding objects. \\nBackpropagation to train multilayer architectures \\nFrom the earliest days of pattern recognition22,23, the aim of research -\\ners has been t o replace hand-engineered features wit h trainable  \\nmultilayer networks, but despite its simplicity, the solution was not  \\nwidel y understoo d unti l the mid 1980s. A s it turn s out, multilayer  \\narchitectur es ca n be traine d by simple stochastic gradient descent.  \\nAs long as the modules are relatively smooth functions of their inputs  \\nand of their internal  weights, one  can compute g radients using  the \\nbackpropagation procedure. The idea that this could be done, and  \\nthat it worked, was discovered independently by several different  \\ngroups during the 1970s  and 1980s24–27.  \\nThe backpropagation procedure to compute the gradient of an \\nobjective function with respect to the weights of a multilayer stack \\nof modules is nothing more than a practical application of the chain corresponding to the output for one of the learned features, detected at each \\nof the image positions. Information flows bottom up, with lower-level features \\nacting as oriented edge detectors, and a score is computed for each image class \\nin output. ReLU, rectified linear unit.\\nRed Green BlueSamoyed (16) ; Papillon (5.7) ; Pomeranian (2.7) ; Arctic fox (1.0) ; Eskimo dog (0.6) ; white wolf (0.4) ; Siberian husky (0.4)\\nConvolutions and ReLU\\nMax pooling\\nMax poolingConvolutions and ReLU\\nConvolutions and ReLU\\n3 | 9 remainder29,30. The analysis seems to show that saddle points with \\nonly a few downward curving directions are present in very large \\nnumbers, but almost all of them have very similar values of the objec -\\ntive function. Hence, it does not much matter which of these saddle \\npoints the algorithm gets stuck at. \\nInterest in deep feedforward networks was revived around 2006 \\n(refs\\xa031–34) by a group of researchers brought together by the Cana -\\ndian Institute for Advanced Research (CI FAR). The researchers  intro -\\nduced unsupervised learning procedures that could create layers of \\nfeature detectors without requiring labelled data. The objective in \\nlearning each layer of feature detectors was to be able to reconstruct \\nor model the activities of feature detectors (or raw inputs) in the layer \\nbelow. By ‘pre-training’ several layers of progressively more complex \\nfeature detectors using this reconstruction objective, the weights of a \\ndeep network could be initialized to sensible values. A final layer of \\noutput units could then be added to the top of the network and the \\nwhole deep system could be fine-tuned using standard backpropaga -\\ntion33–35. This worked remarkably well for recognizing handwritten \\ndigits or for detecting pedestrians, especially when the amount of \\nlabelled data was very limited36. \\nThe first major application of this pre-training approach was in \\nspeech recognition, and it was made possible by the advent of fast \\ngraphics processing units (GPUs) that were convenient to program37 \\nand allowed researchers to train networks 10 or 20 times faster. In \\n2009, the approach was used to map short temporal windows of coef -\\nficients extracted from a sound wave to a set of probabilities for the \\nvarious fragments of speech that might be represented by the frame \\nin the centre of the window. It achieved record-breaking results on a \\nstandard speech recognition benchmark that used a small vocabu -\\nlary38 and was quickly developed to give record-breaking results on \\na large vocabulary task39. By 2012, versions of the deep net from 2009 \\nwere being developed by many of the major speech groups6 and were \\nalready being deployed in Android phones. For smaller data sets, \\nunsupervised pre-training helps to prevent overfitting40, leading to \\nsignificantly better generalization when the number of labelled exam -\\nples is small, or in a transfer setting where we have lots of examples \\nfor some ‘source’ tasks but very few for some ‘target’ tasks. Once deep \\nlearning had been rehabilitated, it turned out that the pre-training \\nstage was only needed for small data sets. \\nThere was, however, one particular type of deep, feedforward net -\\nwork that was much easier to train and generalized much better than \\nnetworks with full connectivity between adjacent layers. This was \\nthe convolutional neural network (ConvNet)41,42. It achieved many \\npractical successes during the period when neural networks were out \\nof favour and it has recently been widely adopted by the computer-\\nvision community. \\nConvolutional neural networks \\nConvNets are designed to process data that come in the form of  \\nmultiple arrays, for example a colour image composed of three 2D  \\narrays containing pixel intensities in the three colour channels. Many  \\ndata modalit ies ar e in the for m of multip le arrays: 1D fo r signals and  \\nsequences, including language; 2D for images or audio spectrograms;  \\nand 3D for video or volumetric images. There are four key ideas  \\nbehind ConvNets that take advantage of the properties of natural  \\nsignal s: loca l connections, shared weights, pooling and the us e of \\nmany layers.  \\nThe architecture of a typical ConvNet (Fig. 2) is structured as a \\nseries of stages. The first few stages are composed of two types of \\nlayers: convolutional layers and pooling layers. Units in a convolu -\\ntional layer are organized in feature maps, within which each unit \\nis connected to local patches in the feature maps of the previous \\nlayer through a set of weights called a filter bank. The result of this \\nlocal weighted sum is then passed through a non-linearity such as a \\nReLU. All units in a feature map share the same filter bank. Differ -\\nent feature maps in a layer use different filter banks. The reason for this architecture is twofold. First, in array data such as images, local \\ngroups of values are often highly correlated, forming distinctive local \\nmotifs that are easily detected. Second, the local statis tics of images \\nand other signals are invariant to location. In other words, if a motif \\ncan appear in one part of the image, it could appear anywhere, hence \\nthe idea of units at different locations sharing the same weights and \\ndetecting the same pattern in different parts of the array. Mathemati -\\ncally, the filtering operation performed by a feature map is a dis crete \\nconvolution, hence the name. \\nAlthough the role of the convolutional layer is to detect local con -\\njunctions of features from the previous layer, the role of the pooling \\nlayer is to merge semantically similar features into one. Because the \\nrelative positions of the features forming a motif can vary somewhat, \\nreliably detecting the motif can be done by coarse-graining the posi -\\ntion of each feature. A typical pooling unit computes the maximum  \\nof a local patch of units in one feature map (or in a few feature maps). \\nNeighbouring pooling units take input from patches that are shifted \\nby more than one row or column, thereby reducing the dimension of \\nthe representation and creating an invariance to small shifts and dis -\\ntortions. Two or three stages of convolution, non-linearity and pool -\\ning are stacked, followed by more convolutional and fully-connected \\nlayers. Backpropagating gradients through a ConvNet is as simple as \\nthrough a regular deep network, allowing all the weights in all the \\nfilter banks to be trained. \\nDeep neural networks exploit the property that many natural sig -\\nnals are compositional hierarchies, in which higher-level features \\nare obtained by composing lower-level ones. In images, local combi -\\nnations of edges form motifs, motifs assemble into parts, and parts \\nform objects. Similar hierarchies exist in speech and text from sounds \\nto phones, phonemes, syllables, words and sentences. The pooling \\nallows representations to vary very little when elements in the previ -\\nous layer vary in position and appearance. \\nThe convolutional and pooling layers in ConvNets are directly \\ninspired by the classic notions of simple cells and complex cells in \\nvisual neuroscience43, and the overall architecture is reminiscent of \\nthe LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral path -\\nway44. When ConvNet models and monkeys are shown the same pic -\\nture, the activations of high-level units in the ConvNet explains half \\nof the variance of random sets of 160\\xa0neurons in the monkey ’s infer -\\notemporal cortex45. ConvNets have their roots in the neocognitron46, \\nthe architecture of which was somewhat similar, but did not have an \\nend-to-end supervised-learning algorithm such as backpropagation. \\nA primitive 1D ConvNet called a time-delay neural net was used for \\nthe recognition of phonemes and simple words47,48. \\nThere have been numerous applications of convolutional net -\\nworks going back to the early 1990s, starting with time-delay neu -\\nral networks for speech recognition47 and document reading42. The \\ndocument reading system used a ConvNet trained jointly with a \\nprobabilistic model that implemented language constraints. By the \\nlate 1990s this system was reading over 10% of all the cheques in the \\nUnited States. A number of ConvNet-based optical character recog -\\nnition and handwriting recognition systems were later deployed by \\nMicrosoft49. ConvNets were also experimented with in the early 1990s \\nfor object detection in natural images, including faces and hands50,51, \\nand for face recognition52. \\nImage understanding with deep convolutional networks\\nSince the early 2000s, ConvNets have been applied with great success \\nto the detection, segmentation and recognition of objects and regions \\nin images. These were all tasks in which labelled data was relatively \\nabun-dant, such as traffic sign recognition53, the segmentation of \\nbiological images54 particularly for connectomics55, and the detection \\nof faces, text, pedestrians and human bodies in natural images36,50,51,56–\\n58. A major recent practical success of ConvNets is face recognition59. \\nImportantly, images can be labelled at the pixel level, which will \\nhave applications in technology, including autonomous mobile \\nrobots and  \\n4 | 9 Figure 3 - From image to text.  Captions generated by a recurrent neural \\nnetwor k (RNN) taking, as extra input, the representation extracted by a deep  \\nconvolution neural network (CNN)  from a tes t image, with the RNN trained to  \\n‘translate’ high-level representations o f images into captions (top). Reproduced  \\nself-driving cars60,61. Companies such as Mobileye and NVIDIA are \\nusing such ConvNet-based methods in their upcoming vision sys -\\ntems for cars. Other applications gaining importance involve natural \\nlanguage understanding14 and speech recognition7. \\nDespite these successes, ConvNets were largely forsaken by the \\nmainstream computer-vision and machine-learning communities \\nuntil the ImageNet competition in 2012. When deep convolutional \\nnetworks were applied to a data set of about a million images from \\nthe web that contained 1,000 different classes, they achieved spec -\\ntacular results, almost halving the error rates of the best compet -\\ning approaches1. This success came from the efficient use of GPUs, \\nReLUs, a new regularization technique called dropout62, and tech -\\nniques to generate more training examples by deforming the existing \\nones. This success has brought about a revolution in computer vision; \\nConvNets are now the dominant approach for almost all recognition \\nand detection tasks4,58,59,63–65 and approach human performance on \\nsome tasks. A recent stunning demonstration combines ConvN ets \\nand recurrent net modules for the generation of image captions \\n(Fig.\\xa03). \\nRecent ConvNet architectures have 10 to 20 layers of ReLUs, hun -\\ndreds of millions of weights, and billions of connections be tween \\nunits. Whereas training such large networks could have taken weeks \\nonly two years ago, progress in hardware, software and algorithm \\nparallelization have reduced training times to a few hours. \\nThe performance o f ConvNet-based vision  systems has caused  \\nmost major technology companies, including Google, Facebook,  with permission from ref. 102. When the RNN is given the ability to focu s its \\nattention on a different location in th e input image (middle and bottom; the  \\nlighter patches were giv en more attention) as it generates eac h word (bold), we  \\nfound86 that it exploits this to achieve bett er ‘translation’ of images int o captions.\\nMicrosoft, IBM, Yahoo!, Twitter and Adobe, as well as a quickly \\ngrowing number of start-ups to initiate research and development \\nprojects and to deploy ConvNet-based image understanding products \\nand services. \\nConvNets are easily amenable to efficient hardware implemen -\\ntations in chips or field-programmable gate arrays66,67. A number \\nof companies such as NVIDIA, Mobileye, Intel, Qualcomm and \\nSamsung are developing ConvNet chips to enable real-time vision \\napplications in smartphones, cameras, robots and self-driving cars. \\nDistributed representations and language processing  \\nDeep-learning theory shows that  deep nets have  two different expo -\\nnential advantages over classic learning algorithms that do not use  \\ndistributed representations21. Both of these advantages arise from the  \\npower of composition  and depend  on the underlying data-generating  \\ndistribution having an  appropriate componential  structure40. First,  \\nlearning  distributed representations enable generalization to new  \\ncombinations of the values of  learned features beyond  those seen  \\nduring  training (for example, 2n combination s are possible  with n \\nbinary features)68,69. Second, composing layers of representation in  \\na deep net brings the potential for another exponential advantage70 \\n(exponential in the depth).  \\nThe hidden layers of a multilayer neural network learn to repre -\\nsent the network ’s inputs in a way that makes it easy to predict the \\ntarget outputs. This is nicely demonstrated by training a multilayer \\nneural network to predict the next word in a sequence from a local \\nVision\\nDeep CNNLanguage\\nGenerating RNN\\nA group of people \\nshopping at an outdoor \\nmarket.\\nThere are many \\nvegetables at the \\nfruit stand.\\nA woman is throwing a frisbee  in a park.\\nA little girl sitting on a bed with a teddy bear. A group of people  sitting on a boat in the water. A giraﬀe standing in a forest with\\ntrees  in the background.A dog is standing on a hardwood ﬂoor. A stop sign is on a road with a\\nmountain in the background\\n5 | 9 context of earlier words71. Each word in the context is presented to \\nthe network as a one-of-N vector, that is, one component has a  value \\nof 1 and the rest are\\xa00. In the first layer, each word creates a different \\npattern of activations, or word vectors (Fig.\\xa04). In a language model, \\nthe other layers of the network learn to convert the input word vec -\\ntors into an output word vector for the predicted next word, which \\ncan be used to predict the probability for any word in the vocabulary \\nto appear as the next word. The network learns word vectors that \\ncontain many active components each of which can be interpreted \\nas a separate feature of the word, as was first demonstrated27 in the \\ncontext of learning distributed representations for symbols. These \\nsemantic features were not explicitly present in the input.  They were \\ndiscovered by the learning procedure as a good way of factorizing \\nthe structured relationships between the input and output symbols \\ninto multiple ‘micro-rules’ . Learning word vectors turned  out to also \\nwork very well when the word sequences come from a large corpus \\nof real text and the individual micro-rules are unreliable71. When \\ntrained to predict the next word in a news story, for example, the \\nlearned word vectors for Tuesday and Wednesday are very similar, as \\nare the word vectors for Sweden and Norway. Such representations \\nare called distributed representations because their elements (the \\nfeatures) are not mutually exclusive and their many configurations \\ncorrespond to the variations seen in the observed data. These word \\nvectors are composed of learned features that were not determined \\nahead of time by experts, but automatically discovered by the neura l \\nnetwork. Vector representations of words learned from text are now \\nvery widely used in natural language applications14,17,72–76. \\nThe issue of representation lies at the heart of the debate between \\nthe logic-inspired and the neural-network-inspired paradigms for \\ncognition. In the logic-inspired paradigm, an instance of a symbol is \\nsomething for which the only property is that it is either identical or \\nnon-identical to other symbol instances. It has no internal structure \\nthat is relevant to its use; and to reason with symbols, they must be \\nbound to the variables in judiciously chosen rules of inference. By \\ncontrast, neural networks just use big activity vectors, big weight \\nmatrices and scalar non-linearities to perform the type of fast ‘intui -\\ntive’ inference that underpins effortless commonsense reasoning. \\nBefore the introduction of neural language models71, the standard \\napproach to statistical modelling of language did not exploit distrib -\\nuted representations: it was based on counting frequencies of occur -\\nrences of short symbol sequences of length up to N (called N-grams). \\nThe number of possible N-grams is on the order of VN, where V is \\nthe vocabulary size, so taking into account a context of more than a handful of words would require very large training corpora. N-grams \\ntreat each word as an atomic unit, so they cannot generalize across \\nsemantically related sequences of words, whereas neural la nguage \\nmodels can because they associate each word with a vector of real \\nvalued features, and semantically related words end up close to each \\nother in that vector space (Fig.\\xa04). \\nRecurrent neural networks \\nWhen  backpropagation  was first  introduced, its most exciting  use was \\nfor training recurrent neural networks (RNNs). For tasks that involve  \\nsequentia l inputs, such a s speec h and language, it i s oft en better to  \\nuse RNNs (Fig. 5). RNNs process an input sequence one element at a  \\ntime, maintaining in their hidden units a ‘state vector’ that implicitly  \\ncontains information about the history of all the past elements of  \\nthe sequence. When we consider the outputs of the hidden units at  \\ndifferen t discrete time st eps as if they wer e the outpu ts of different  \\nneurons in a deep multilayer network (Fig.\\xa05, right), it becomes clear  \\nhow we can apply backpropagation to train RNNs.  \\nRNNs are very powerful dynamic systems, but training them has \\nproved to be problematic because the backpropagated gradients \\neither grow or shrink at each time step, so over many time steps they \\ntypically explode or vanish77,78. \\nThanks to advances in their architecture79,80 and ways of training \\nthem81,82, RNNs have been found to be very good at predicting the \\nnext character in the text83 or the next word in a sequence75, but they \\ncan also be used for more complex tasks. For example, after reading \\nan English sentence one word at a time, an English ‘encoder’ network \\ncan be trained so that the final state vector of its hidden units is a go od \\nrepresentation of the thought expressed by the sentence. This thought \\nvector can then be used as the initial hidden state of (or as extra input \\nto) a jointly trained French ‘decoder’ network, which outputs a prob -\\nability distribution for the first word of the French translation. If a \\nparticular first word is chosen from this distribution and provided \\nas input to the decoder network it will then output a probability dis -\\ntribution for the second word of the translation and so on until a \\nfull stop is chosen17,72,76. Overall, this process generates sequences of \\nFrench words according to a probability distribution that depends on \\nthe English sentence. This rather naive way of performing machine  \\ntranslation has quickly become competitive with the state-of-the-ar t, \\nand this raises serious doubts about whether understanding a sen -\\ntence requires anything like the internal symbolic expressions that are \\nmanipulated by using inference rules. It is more compatible with t he \\nview that everyday reasoning involves many simultaneous analogies \\nFigure 4 - Visualizing the learned word vectors.  On the left is an illustration \\nof wor d representations learned for modelling language, non-linearly projected  \\nto 2D for visualization using th e t-SNE algorithm103. On the right is a 2D  \\nrepresentation of phras es learned by an English-to-French encoder–decoder  \\nrecurren t neural network75. One can observe that semantically similar words  or sequences of words are mapped to nearby representations. The distributed \\nrepresentations of words are obtained by using backpropagation to jointly learn \\na representation for each word and a function that predicts a target quantity \\nsuch as the next word in a sequence (for language modelling) or a whole \\nsequence of translated words (for machine translation)18,75.−37 −36 −35 −34 −33 −32 −31 −30 −2991010.51111.51212.51313.514\\n community organizations institutions\\n society\\n industry company organization school\\n companies\\n Community oﬃce\\n Agency\\n communities Association body\\n schools\\n agencies\\n−5.5 −5 −4.5 −4 −3.5 −3 −2.5 −2−4.2−4−3.8−3.6−3.4−3.2−3−2.8−2.6−2.4−2.2\\nover the past few monthsthat a few days\\nIn the last few daysthe past few daysIn a few months\\nin the coming monthsa few months ago&quot; the two groups\\nof the two groups\\nover the last few monthsdispute between the twothe last two decades\\nthe next six monthstwo months before being\\nfor nearly two monthsover the last two decades\\nwithin a few months\\n6 | 9 a sorted list of symbols when their input consists of an unsorted \\nsequence in which each symbol is accompanied by a real value that \\nindicates its priority in the list88. Memory networks can be trained \\nto keep track of the state of the world in a setting similar to a text \\nadventure game and after reading a story, they can answer questions \\nthat require complex inference90. In one test example, the network is \\nshown a 15-sentence version of the The Lord of the Rings  and correctly \\nanswers questions such as “where is Frodo now?”89.  \\nThe future of deep learning \\nUnsupervised learning91–98 had a catalytic effect in reviving interest in  \\ndeep learning, but has since been overshadowed by the successes of  \\npurely supervised learning. Although we have not focused on it in this  \\nReview, we expect unsupervised learning to become far more important  \\nin th e longer term. Human and animal learnin g is largely unsupervised:  \\nwe discover the structure of the world by observing it, not by being told  \\nthe name of every object.  \\nHuman vision is an active process that sequentially samples the optic \\narray in an intelligent, task-speciﬁc way using a small, high-resolution \\nfovea with a large, low-resolution surround. We expect much of the \\nfuture progress in vision to come from systems that are trained end-to-\\nend and combine ConvNets with RNNs that use reinforcement learning \\nto decide where to look. Systems combining deep learning and rein -\\nforcement learning are in their infanc y, but they already outperform \\npassive vision systems99 at classification tasks and produce impressive \\nresults in learning to play many different video games100. \\nNatural language understanding is another area in which deep learn -\\ning is poised to make a large impact over the next few years. We expect \\nsystems that use RNNs to understand sentences or whole documents \\nwill become much better when they learn strategies for selectively \\nattending to one part at a time76,86. \\nUltimately, major progress in artificial intelligence will com e about \\nthrough systems that combine representation learning with complex \\nreasoning. Although deep learning and simple reasoning have been \\nused for speech and handwriting recognition for a long time, new \\nparadigms are needed to replace rule-based manipulation of \\nsymbolic expressions by operations on large vectors101. \\n1. Krizhevsky, A., Sutskever, I. & Hinton, G. ImageNet classification with deep \\nconvolutional neural networks. In Proc. Advances in Neural Information \\nProcessing Systems 25 1090–1098  (2012).\\nThis report was a breakthrough that used convolutional nets to almost halve\\nthe error rate for object recognition, and precipitated the rapid adoption of \\ndeep learning by the computer vision community.\\n2. Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Learning hierarchical features for\\nscene labeling. IEEE Trans. Pattern Anal. Mach. Intell . 35, 1915–1929 (2013).\\n3. Tompson, J., Jain, A., LeCun, Y. & Bregler, C. Joint training of a convolutional \\nnetwork and a graphical model for human pose estimation. In Proc. Advances in \\nNeural Information Processing Systems 27  1799–1807 (2014). \\n4. Szegedy, C. et al. Going deeper with convolutions. Preprint at http://arxiv.org/\\nabs/1409.4842 (2014). \\n5. Mikolov, T., Deoras, A., Povey, D., Burget, L. & Cernocky, J. Strategies for training\\nlarge scale neural network language models. In Proc. Automatic Speech \\nRecognition and Understanding  196–201 (2011). \\n6. Hinton, G. et al. Deep neural networks for acoustic modeling in speech \\nrecognition. IEEE Signal Processing Magazine 29, 82–97 (2012).\\nThis joint paper from the major speech recognition laboratories, summarizing\\nthe breakthrough achieved with deep learning on the task of phonetic \\nclassification for automatic speech recognition, was the first major industrial \\napplication of deep learning.\\n7. Sainath, T., Mohamed, A.-R., Kingsbury, B. & Ramabhadran, B. Deep \\nconvolutional neural networks for LVCSR. In Proc. Acoustics, Speech and Signal\\nProcessing 8614–8618  (2013).\\n8. Ma, J., Sheridan, R. P., Liaw, A., Dahl, G. E. & Svetnik, V. Deep neural nets as a \\nmethod for quantitative structure-activity relationships. J. Chem. Inf. Model. 55,\\n263–274 (2015). \\n9. Ciodaro, T., Deva, D., de Seixas, J. & Damazio, D. Online particle detection with \\nneural networks based on topological calorimetry information. J. Phys. Conf. \\nSeries  368, 012030 (2012).\\n10. Kaggle. Higgs boson machine learning challenge. Kaggle  https://www.kaggle.\\ncom/c/higgs-boson (2014). \\n11. Helmstaedter,  M. et al. Connectomic reconstruction of the inner plexiform layer\\nin the mouse retina. Nature  500, 168–174 (2013).xtxt−1xt+1xUnfoldVWW\\nW W WV V V\\nU U U Uso\\nst−1ot−1ot\\nstst+1ot+1\\nFigure 5 - A recurrent neural network and the unfolding in tim e of the \\ncomputation involved in its forward computation.  The artificial neurons  \\n(for example, hidden uni ts grouped under no de s with val ues st at tim e t) get \\ninputs from other neurons a t previous time steps (this is represented with the  \\nblack square, representing a delay of one time step , on the left). In this way, a  \\nrecurrent neural network can map an input sequence with elemen ts xt into an  \\noutput sequence with elemen ts ot, with eac h ot depending on all the previous  \\nxtʹ (for tʹ ≤ t). The same parameters (matr ices U,V,W ) are used at each time  \\nstep. Many other architectures are possible, including a variant in which the  \\nnetwork can generate a sequence of outputs (for example, words), each of  \\nwhich is used as inputs for the next tim e step. The backpropagation algorithm  \\n(Fig.  1) can b e directly applied to the computational graph of the unfolded  \\nnetwork on the right, to compute the derivativ e of a total error (for example,  \\nthe log-probability of generating the right sequence o f outputs) with respect to  \\nall the stat es st and all the parameters.\\nthat each contribute plausibility to a conclusion84,85. \\nInstead of translating the meaning of a French sentence into  an \\nEnglish sentence, one can learn to ‘translate’ the meaning of an image \\ninto an English sentence (Fig. 3). The encoder here is a deep C on-\\nvNet that converts the pixels into an activity vector in its last hidden \\nlayer. The decoder is an RNN similar to the ones used for machine \\ntranslation and neural language modelling. There has been a surge of \\ninterest in such systems recently (see examples mentioned in ref. 86). \\nRNNs, once unfolded in time (Fig. 5), can be seen as very deep \\nfeedforward networks in which all the layers share the same weights. \\nAlthough their main purpose is to learn long-term dependencies, \\ntheoretical and empirical evidence shows that it is difficult to learn \\nto store information for very long78.  \\nTo correct for that, one idea is to augment the network with an \\nexplicit memory. The first proposal of this kind is the long short-term \\nmemory (LSTM) networks that use special hidden units, the natural \\nbehaviour of which is to remember inputs for a long time79. A special \\nunit called the memory cell acts like an accumulator or a gated leaky \\nneuron: it has a connection to itself at the next time step that has a \\nweight of one, so it copies its own real-valued state and accumulates \\nthe external signal, but this self-connection is multiplicatively gated \\nby another unit that learns to decide when to clear the content of the \\nmemory. \\nLSTM networks have subsequently proved to be more effective \\nthan conventional RNNs, especially when they have several layers for \\neach time step87, enabling an entire speech recognition system that \\ngoes all the way from acoustics to the sequence of characters in the \\ntranscription. LSTM networks or related forms of gated units are also \\ncurrently used for the encoder and decoder networks that perform \\nso well at machine translation17,72,76. \\nOver the past year, several authors have made different proposals to \\naugment RNNs with a memory module. Proposals include the Neural \\nTuring Machine in which the network is augmented by a ‘tape-like’ \\nmemory that the RNN can choose to read from or write to88, and \\nmemory networks, in which a regular network is augmented by a \\nkind of associative memory89. Memory networks have yielded excel -\\nlent performance on standard question-answering benchmarks. The \\nmemory is used to remember the story about which the network is \\nlater asked to answer questions. \\nBeyond simple memorization, neural Turing machines and mem -\\nory networks are being used for tasks that would normally require \\nreasoning and symbol manipulation. Neural Turing machines can \\nbe taught ‘algorithms ’. Among other things, they can learn to output \\n7 | 9 12. Leung, M. K., Xiong, H. Y., Lee, L. J. & Frey, B. J. Deep learning of the tissue-\\nregulated splicing code. Bioinformatics  30, i121–i129 (2014).\\n13. Xiong, H. Y. et al. The human splicing code reveals new insights into the genetic\\ndeterminants of disease. Science  347, 6218 (2015).\\n14. Collobert, R., et al. Natural language processing (almost) from scratch. J. Mach. \\nLearn. Res.  12, 2493–2537 (2011).\\n15. Bordes, A., Chopra, S. & Weston, J. Question answering with subgraph \\nembeddings. In Proc. Empirical Methods in Natural Language Processing http://\\narxiv.org/abs/1406.3676v3 (2014). \\n16. Jean, S., Cho, K., Memisevic, R. & Bengio, Y. On using very large target \\nvocabulary for neural machine translation. In Proc. ACL-IJCNLP  http://arxiv.org/\\nabs/1412.2007 (2015).\\n17. Sutskever, I. Vinyals, O. & Le. Q. V. Sequence to sequence learning with neural \\nnetworks. In Proc. Advances in Neural Information Processing Systems 27\\n3104–3112 (2014). \\nThis paper showed state-of-the-art machine translation results with the \\narchitecture introduced in ref. 72, with a recurrent network trained to read a \\nsentence in one language, produce a semantic representation of its meaning,\\nand generate a translation in another language.\\n18. Bottou, L. & Bousquet, O. The tradeoffs of large scale learning. In Proc. Advances\\nin Neural Information Processing Systems 20  161–168 (2007). \\n19. Duda, R. O. & Hart, P. E. Pattern Classiﬁcation and Scene Analysis  (Wiley, 1973).\\n20. Schölkopf, B. & Smola, A. Learning with Kernels  (MIT Press, 2002).\\n21. Bengio, Y., Delalleau, O. & Le Roux, N. The curse of highly variable functions \\nfor local kernel machines. In Proc. Advances in Neural Information Processing\\nSystems 18  107–114 (2005). \\n22. Selfridge, O. G. Pandemonium: a paradigm for learning in mechanisation of \\nthought processes. In Proc. Symposium on Mechanisation of Thought Processes\\n513–526 (1958). \\n23. Rosenblatt, F. The Perceptron — A Perceiving and Recognizing Automaton . Tech.\\nRep. 85-460-1 (Cornell Aeronautical Laboratory, 1957). \\n24. Werbos, P. Beyond Regression: New Tools for Prediction and Analysis in the\\nBehavioral Sciences . PhD thesis, Harvard Univ. (1974). \\n25. Parker, D. B. Learning Logic  Report TR–47 (MIT Press, 1985).\\n26. LeCun, Y. Une procédure d’apprentissage pour Réseau à seuil assymétrique\\nin Cognitiva 85: a la Frontière de l’Intelligence Artiﬁcielle, des Sciences de la \\nConnaissance et des Neurosciences  [in French] 599–604 (1985). \\n27. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by\\nback-propagating errors. Nature  323, 533–536 (1986).\\n28. Glorot, X., Bordes, A. & Bengio. Y. Deep sparse rectiﬁer neural networks. In  Proc.\\n14th International Conference on Artificial Intelligence and Statistics 315–323\\n(2011).\\nThis paper showed that supervised training of very deep neural networks is \\nmuch faster if the hidden layers are composed of ReLU.\\n29. Dauphin, Y. et al. Identifying and attacking the saddle point problem in high-\\ndimensional non-convex optimization. In Proc. Advances in Neural Information\\nProcessing Systems 27  2933–2941 (2014). \\n30. Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. & LeCun, Y. The loss \\nsurface of multilayer networks.  In Proc. Conference on AI and Statistics http://\\narxiv.org/abs/1412.0233 (2014). \\n31. Hinton, G. E. What kind of graphical model is the brain? In Proc. 19th \\nInternational Joint Conference on Artificial intelligence  1765–1775 (2005). \\n32. Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief \\nnets. Neural Comp.  18, 1527–1554 (2006).\\nThis paper introduced a novel and effective way of training very deep neural\\nnetworks by pre-training one hidden layer at a time using the unsupervised \\nlearning procedure for restricted Boltzmann machines. \\n33. Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training \\nof deep networks. In Proc. Advances in Neural Information Processing Systems 19\\n153–160 (2006). \\nThis report demonstrated that the unsupervised pre-training method \\nintroduced in ref. 32 significantly improves performance on test data and \\ngeneralizes the method to other unsupervised representation-learning \\ntechniques, such as auto-encoders.\\n34. Ranzato, M., Poultney, C., Chopra, S. & LeCun, Y. Efﬁcient learning of sparse\\nrepresentations with an energy-based model. In Proc. Advances in Neural \\nInformation Processing Systems 19 1137 –1144 (2006). \\n35. Hinton, G. E. & Salakhutdinov, R. Reducing the dimensionality of data with\\nneural networks. Science  313, 504–507 (2006).\\n36. Sermanet, P., Kavukcuoglu, K., Chintala, S. & LeCun, Y. Pedestrian detection with\\nunsupervised multi-stage feature learning. In Proc. International Conference \\non Computer Vision and Pattern Recognition  http://arxiv.org/abs/1212.0142 \\n(2013).\\n37. Raina, R., Madhavan, A. & Ng, A. Y. Large-scale deep unsupervised learning \\nusing graphics processors. In Proc. 26th Annual International Conference on \\nMachine Learning  873–880 (2009). \\n38. Mohamed, A.-R., Dahl, G. E. & Hinton, G. Acoustic modeling using deep belief \\nnetworks. IEEE Trans. Audio Speech Lang. Process.  20, 14–22 (2012).\\n39. Dahl, G. E., Yu, D., Deng, L. & Acero, A. Context-dependent pre-trained deep\\nneural networks for large vocabulary speech recognition. IEEE Trans. Audio \\nSpeech Lang. Process.  20, 33–42 (2012).\\n40. Bengio, Y., Courville, A. & Vincent, P. Representation learning: a review and new\\nperspectives. IEEE Trans. Pattern Anal. Machine Intell.  35, 1798–1828 (2013).\\n41. LeCun, Y. et al. Handwritten digit recognition with a back-propagation network.\\nIn Proc. Advances in Neural Information Processing Systems  396–404 (1990). \\nThis is the first paper on convolutional networks trained by backpropagation for the task of classifying low-resolution images of handwritten digits.\\n42. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to \\ndocument recognition. Proc. IEEE  86, 2278–2324 (1998).\\nThis overview paper on the principles of end-to-end training of modular \\nsystems such as deep neural networks using gradient-based optimization \\nshowed how neural networks (and in particular convolutional nets) can be \\ncombined with search or inference mechanisms to model complex outputs \\nthat are interdependent, such as sequences of characters associated with the\\ncontent of a document.\\n43. Hubel, D. H. & Wiesel, T. N. Receptive ﬁelds, binocular interaction, and functional\\narchitecture in the cat’s visual cortex. J. Physiol.  160, 106–154 (1962).\\n44. Felleman, D. J. & Essen, D. C. V. Distributed hierarchical processing in the\\nprimate cerebral cortex. Cereb. Cortex  1, 1–47 (1991).\\n45. Cadieu, C. F. et al. Deep neural networks rival the representation of primate\\nit cortex for core visual object recognition. PLoS Comp. Biol.  10, e1003963\\n(2014).\\n46. Fukushima, K. & Miyake, S. Neocognitron: a new algorithm for pattern \\nrecognition tolerant of deformations and shifts in position. Pattern Recognition\\n15, 455–469 (1982).\\n47. Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K. & Lang, K. Phoneme \\nrecognition using time-delay neural networks. IEEE Trans. Acoustics Speech\\nSignal Process.  37, 328–339 (1989).\\n48. Bottou, L., Fogelman-Soulié, F., Blanchet, P. & Lienard, J. Experiments with time\\ndelay networks and dynamic time warping for speaker independent isolated \\ndigit recognition. In Proc. EuroSpeech 89  537–540 (1989). \\n49. Simard, D., Steinkraus, P. Y. & Platt, J. C. Best practices for convolutional neural\\nnetworks. In Proc. Document Analysis and Recognition 958–963 (2003). \\n50. Vaillant, R., Monrocq, C. & LeCun, Y. Original approach for the localisation of \\nobjects in images. In Proc. Vision, Image, and Signal Processing  141, 245–250\\n(1994).\\n51. Nowlan, S. & Platt, J. in Neural Information Processing Systems  901–908 (1995). \\n52. Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition: a \\nconvolutional neural-network approach. IEEE Trans. Neural Networks  8, 98–113\\n(1997).\\n53. Ciresan, D., Meier, U. Masci, J. & Schmidhuber, J. Multi-column deep neural \\nnetwork for trafﬁc sign classiﬁcation. Neural Networks  32, 333–338 (2012). \\n54. Ning, F. et al. Toward automatic phenotyping of developing embryos from\\nvideos. IEEE Trans. Image Process.  14, 1360–1371 (2005).\\n55. Turaga, S. C. et al. Convolutional networks can learn to generate afﬁnity graphs\\nfor image segmentation. Neural Comput.  22, 511–538 (2010).\\n56. Garcia, C. & Delakis, M. Convolutional face ﬁnder: a neural architecture for\\nfast and robust face detection. IEEE Trans. Pattern Anal. Machine Intell.  26, \\n1408–1423 (2004). \\n57. Osadchy, M., LeCun, Y. & Miller, M. Synergistic face detection and pose \\nestimation with energy-based models. J. Mach. Learn. Res.  8, 1197–1215\\n(2007).\\n58. Tompson, J., Goroshin, R. R., Jain, A., LeCun, Y. Y. & Bregler, C. C. Efﬁcient object\\nlocalization using convolutional networks. In Proc. Conference on Computer \\nVision and Pattern Recognition  http://arxiv.org/abs/1411.4280 (2014). \\n59. Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: closing the gap to \\nhuman-level performance in face veriﬁcation. In Proc. Conference on Computer\\nVision and Pattern Recognition  1701–1708 (2014). \\n60. Hadsell, R. et al. Learning long-range vision for autonomous off-road driving.\\nJ. Field Robot.  26, 120–144 (2009). \\n61. Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Scene parsing with multiscale\\nfeature learning, purity trees, and optimal covers. In Proc. International\\nConference on Machine Learning  http://arxiv.org/abs/1202.2160 (2012). \\n62. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. \\nDropout: a simple way to prevent neural networks from overﬁtting. J. Machine \\nLearning Res.  15, 1929–1958 (2014).\\n63. Sermanet, P. et al. Overfeat: integrated recognition, localization and detection\\nusing convolutional networks. In Proc. International Conference on Learning \\nRepresentations  http://arxiv.org/abs/1312.6229 (2014). \\n64. Girshick, R., Donahue, J., Darrell, T. & Malik, J. Rich feature hierarchies for \\naccurate object detection and semantic segmentation. In Proc. Conference on\\nComputer Vision and Pattern Recognition 580–587 (2014). \\n65. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale \\nimage recognition. In Proc. International Conference on Learning Representations\\nhttp://arxiv.org/abs/1409.1556 (2014). \\n66. Boser, B., Sackinger, E., Bromley, J., LeCun, Y. & Jackel, L. An analog neural\\nnetwork processor with programmable topology. J. Solid State Circuits  26,\\n2017–2025 (1991). \\n67. Farabet, C. et al. Large-scale FPGA-based convolutional networks. In Scaling\\nup Machine Learning: Parallel and Distributed Approaches  (eds Bekkerman, R.,\\nBilenko, M. & Langford, J.) 399–419 (Cambridge Univ. Press, 2011). \\n68. Bengio, Y. Learning Deep Architectures for AI  (Now, 2009). \\n69. Montufar, G. & Morton, J. When does a mixture of products contain a product of\\nmixtures? J. Discrete Math.  29, 321–347 (2014).\\n70. Montufar, G. F., Pascanu, R., Cho, K. & Bengio, Y. On the number of linear regions\\nof deep neural networks. In  Proc. Advances in Neural Information Processing \\nSystems 27  2924–2932 (2014). \\n71. Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model. In \\nProc. Advances in Neural Information Processing Systems 13  932–938 (2001). \\nThis paper introduced neural language models, which learn to convert a word\\nsymbol into a word vector or word embedding composed of learned semantic \\nfeatures in order to predict the next word in a sequence.\\n72. Cho, K. et al. Learning phrase representations using RNN encoder-decoder\\n8 | 9 for statistical machine translation. In Proc. Conference on Empirical Methods in \\nNatural Language Processing  1724–1734 (2014).  \\n73. Schwenk, H. Continuous space language models. Computer Speech Lang.  21,\\n492–518 (2007). \\n74. Socher, R., Lin, C. C-Y., Manning, C. & Ng, A. Y. Parsing natural scenes and\\nnatural language with recursive neural networks. In Proc. International \\nConference on Machine Learning  129–136 (2011). \\n75. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed \\nrepresentations of words and phrases and their compositionality. In Proc.\\nAdvances in Neural Information Processing Systems 26  3111–3119 (2013). \\n76. Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly \\nlearning to align and translate. In Proc. International Conference on Learning\\nRepresentations  http://arxiv.org/abs/1409.0473 (2015).\\n77. Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen [in\\nGerman] Diploma thesis, T.U. Münich (1991). \\n78. Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with \\ngradient descent is difﬁcult. IEEE Trans. Neural Networks  5, 157–166 (1994). \\n79. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput.  9,\\n1735–1780 (1997). \\nThis paper introduced LSTM recurrent networks, which have become a crucial\\ningredient in recent advances with recurrent networks because they are good \\nat learning long-range dependencies. \\n80. ElHihi, S. & Bengio, Y. Hierarchical recurrent neural networks for long-term \\ndependencies. In Proc. Advances in Neural Information Processing Systems 8\\nhttp://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-\\nlong-term-dependencies (1995). \\n81. Sutskever, I. Training Recurrent Neural Networks . PhD thesis, Univ. Toronto\\n(2012).\\n82. Pascanu, R., Mikolov, T. & Bengio, Y. On the difﬁculty of training recurrent neural\\nnetworks. In Proc. 30th International Conference on Machine Learning  1310–\\n1318 (2013). \\n83. Sutskever, I., Martens, J. & Hinton, G. E. Generating text with recurrent neural\\nnetworks. In Proc. 28th International Conference on Machine Learning  1017–\\n1024 (2011). \\n84. Lakoff, G. & Johnson, M. Metaphors We Live By  (Univ. Chicago Press, 2008).\\n85. Rogers, T. T. & McClelland, J. L. Semantic Cognition: A Parallel Distributed \\nProcessing Approach  (MIT Press, 2004).\\n86. Xu, K. et al. Show, attend and tell: Neural image caption generation with visual\\nattention. In Proc. International Conference on Learning Representations  http://\\narxiv.org/abs/1502.03044 (2015). \\n87. Graves, A., Mohamed, A.-R. & Hinton, G. Speech recognition with deep recurrent\\nneural networks. In Proc. International Conference on Acoustics, Speech and \\nSignal Processing 6645–6649 (2013). \\n88. Graves, A., Wayne, G. & Danihelka, I. Neural Turing machines. http://arxiv.org/\\nabs/1410.5401 (2014). \\n89. Weston, J. Chopra, S. & Bordes, A. Memory networks. http://arxiv.org/\\nabs/1410.3916 (2014). 90. Weston, J., Bordes, A., Chopra, S. & Mikolov, T. Towards AI-complete question\\nanswering: a set of prerequisite toy tasks. http://arxiv.org/abs/1502.05698 \\n(2015).\\n91. Hinton, G. E., Dayan, P., Frey, B. J. & Neal, R. M. The wake-sleep algorithm for\\nunsupervised neural networks. Science  268, 1558–1161 (1995).\\n92. Salakhutdinov, R. & Hinton, G. Deep Boltzmann machines. In Proc. International\\nConference on Artificial Intelligence and Statistics 448–455  (2009).\\n93. Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.-A. Extracting and composing\\nrobust features with denoising autoencoders. In Proc. 25th International \\nConference on Machine Learning 1096–1103 (2008). \\n94. Kavukcuoglu, K. et al. Learning convolutional feature hierarchies for visual \\nrecognition. In Proc. Advances in Neural Information Processing Systems 23\\n1090–1098 (2010). \\n95. Gregor, K. & LeCun, Y. Learning fast approximations of sparse coding. In Proc.\\nInternational Conference on Machine Learning  399–406 (2010). \\n96. Ranzato, M., Mnih, V., Susskind, J. M. & Hinton, G. E. Modeling natural images\\nusing gated MRFs. IEEE Trans. Pattern Anal. Machine Intell. 35, 2206–2222\\n(2013).\\n97. Bengio, Y., Thibodeau-Laufer, E., Alain, G. & Yosinski, J. Deep generative\\nstochastic networks trainable by backprop. In Proc. 31st International \\nConference on Machine Learning 226–234 (2014). \\n98. Kingma, D., Rezende, D., Mohamed, S. & Welling, M. Semi-supervised learning \\nwith deep generative models. In Proc. Advances in Neural Information Processing\\nSystems 27  3581–3589 (2014). \\n99. Ba, J., Mnih, V. & Kavukcuoglu, K. Multiple object recognition with visual \\nattention. In Proc. International Conference on Learning Representations http://\\narxiv.org/abs/1412.7755 (2014). \\n100. Mnih, V. et al. Human-level control through deep reinforcement learning. Nature\\n518, 529–533 (2015).\\n101. Bottou, L. From machine learning to machine reasoning. Mach. Learn.  94,\\n133–149 (2014). \\n102. Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image \\ncaption generator. In Proc. International Conference on Machine Learning http://\\narxiv.org/abs/1502.03044 (2014).\\n103. van der Maaten, L. & Hinton, G. E. Visualizing data using t-SNE. J. Mach. Learn.\\nResearch  9, 2579–2605 (2008).\\nAcknowledgements  The authors would like to thank the Natural Sciences and \\nEngineering Research Council of Canada, the Canadian Institute For Advanced \\nResearch (CIFAR), the National Science Foundation and Office of Naval \\nResearch  for support. Y.L. and Y.B. are CIFAR fellows.\\n9 | 9',\n",
       "  \" \\n \\n \\nChowdhury, G. (2003) Natural language processing. Annual Review of \\nInformation Science and Technology, 37. pp. 51-89. ISSN 0066-4200 \\n \\n \\n \\nhttp://eprints.cdlr.strath.ac.uk/2611/\\n \\n \\n \\nThis is an author-produced versio n of a paper published in The \\nAnnual Review of Information Science and Technology  ISSN 0066-4200 . \\nThis version has been peer-reviewed, but does not include the \\nfinal publisher proof corrections, published layout, or pagination. \\n \\nStrathprints is designed to allow users to access the research \\noutput of the University of St rathclyde. Copyright © and Moral \\nRights for the papers on this site are retained by the individual \\nauthors and/or other copyright  owners. Users may download \\nand/or print one copy of any articl e(s) in Strathprints to facilitate \\ntheir private study or for non-co mmercial research. You may not \\nengage in further distribution of th e material or use it for any \\nprofitmaking activities or any commercial gain. You may freely \\ndistribute the url ( http://eprints.cdlr.strath.ac.uk) of the Strathprints \\nwebsite. \\n \\nAny correspondence concerning this service should be sent to The \\nStrathprints Administrator: eprints@cis.strath.ac.uk  \\n  Natural Language Processing \\n \\nGobinda G. Chowdhury \\n   Dept . of Computer and Information Sci ences \\n   University  of Strathcly de, Gla sgow G1 1XH, UK \\n    e-m ail: gobinda@di s.strath.ac.uk \\n \\nIntroductio n \\n \\nNatural Language Processi ng (NLP) is an area of research and appl ication that explores how \\ncomputers can be used to understand and manipulat e natural language text or speech to do useful \\nthings. NLP  researchers aim to gather knowledge on  how hum an beings  understand and use \\nlanguage so t hat  appropria te  tools and t echni ques can be develope d to m ake co mputer sy stems \\nunderstand an d manipulate natural langu ages to perform  the desired  tasks.  The foundations of \\nNLP lie in a num ber of disciplines, viz. co mputer and inform ation sciences,  linguistics, \\nmathematics,  electrical and electronic engi neering, ar tificial intelligence and robotics, \\npsychology, etc.   Applications of NLP i nclude  a number of fields of studies, such as machine \\ntranslation, natural language text processing a nd summarization, user interfaces,  multilingual and \\ncross language information retrieval (CL IR), speech  recognition, artificial intelligence and expert \\nsystems, and so on. \\n \\nOne im portant area of  application of NLP that is  relatively new and has not been covered in th e \\nprevious ARIST chapters on NLP has beco me quite prom inent  due to the prolifer ation of the \\nworld wide web and digital  libraries. Several research ers have pointed out the need for \\nappropriate research in faci litating m ulti- or cross-lingual inform ation  retrieval,  including \\nmultilingual text processing and m ultilingual user int erface sy stems,  in order t o exploit the full \\nbenefit of the www and digital libraries ( see for ex ample, Borgm an, 1997; Peters & Picchi, 1997) \\n \\nScope \\nSeveral ARIS T chapters ha ve reviewed t he field of NLP. The m ost recent ones include that by \\nWarner  in 1987, and Haas  in 1996.  Reviews of literature on large-scale NLP sy stem s, as well as \\nthe various theoretical is sues have also a ppeared  in a num ber of publications (see for exam ple, \\nJurafsky  &  Martin, 200 0; Manning & Schutze, 199 9; Mani & May bury, 1999; Sparck Jones, \\n1999;  Wilks, 1996).   Smeaton (19 99) provides  a good  overview of t he past research on the \\napplications o f NLP in various inform ation re trieval tasks. Several ARIST chapt ers have \\nappeared  on areas related to NLP, such as on  machine-readable dictionaries (Amsler, 1984; \\n 1 Evans, 1989), speech sy nthesis and reco gnition (Lange, 1993), and cross-language information \\nretrieval (Oar d & Dieke ma, 1998). Research on NLP is regularly  published in a number of \\nconferences such as the annual proceedings of AC L (Association of Com putatio nal Linguistic s) \\nand its European counterpart EACL, biennial pr oceedings of the  I nternational Conference on \\nComputational Linguistics (COLING),  annual proceedings of t he Message Understanding \\nConferences ( MUCs), Text Retrieval Conference s (TR ECs) and AC M-SIGIR (As sociation of \\nComputing Machinery  – Special Interest Group on  Inform ation Retrieval) conferences. The m ost \\nprom inent journals reporting NLP resea rch are Comp utational Linguistics  and Natural Lan guage \\nEngineering . Articles r eporting NLP research also  appear in a num ber of inform ation science  \\njournals such as Information Processing and Management, J ournal  of the American Society for \\nInformation Science and Technology, and Journal of Documentation. Several resear chers have \\nalso conducted dom ain-specific NLP stu dies and have reported them  in journals specifically  \\ndealing with the dom ain co ncerned, such as the International  Journ al of M edical Informatics  and  \\nJournal of Ch emical Information a nd C omputer Science . \\n \\nBeginning wi th the basic issues of NLP, this chapte r aims to chart the major research activities in \\nthis area sinc e the last ARI ST  Chapter in 1996 (Haas, 1996), including:  \\n(i) natural language text processing sy stems – te xt summariz ation, information extraction, \\ninform ation retrieval, etc., includi ng domain-specific applications; \\n(ii) natural language interfaces; \\n(iii) NLP in the context of www a nd digital libraries ; and \\n(iv) evaluation of NLP sy stems.  \\n \\nLinguistic research in  infor mation retrieval has not  been covered in this review, since this is a \\nhuge area and has been dealt with separately  in this v olume by  Davi d Blair. Sim ilarly, NLP iss ues \\nrelated to the inform ation retrieval tools (sear ch engines, etc.) for w eb search ar e not covered in \\nthis chapter since a separ ate chapter on indexing and r etrieval for the Web has been written in t his \\nvolum e by   Edie Rasm ussen.  \\n  \\nTools and techniques developed for building NLP systems have be en discussed in this chapter \\nalong with t he specific areas of applications for whic h the y have been built.  Alt hough m achine \\ntranslation (MT) is an important part, a nd in f act the origin, of NLP resear ch, this paper does not \\ncover this topic with sufficient detail since this  is a huge area and demands a sepa rate chapter on \\nits own. Sim ilarly, cross-language infor mation re trieval (CLIR), althoug h  is  a very important \\n 2 and big area in NLP resear ch, is not covered in great detail in this chapter . A se parate chapte r on \\nCLIR res earch appeared in ARIST (Oar d & Di ekema, 199 8). How ever, MT and CLIR have \\nbeco me two i mportant are as of resear ch in the context of the www digital libraries. This chapter \\nreviews so me works on M T and CLIR in the context of  NLP and I R  in digi tal libraries and \\nwww.  Artifi cial Intelligence techniques,  including ne ural networks etc., used in NLP have not  \\nbeen included in this chapte r. \\n \\nSome Theor etical Develo pments \\nPrevious ARIST chapters (Haas, 1996; Warner, 1987) described a num ber of theoretical \\ndevelopm ents that have influenced resear ch in NLP. The m ost recent theoretical developments \\ncan be grouped into four classes: (i) stat istical  and corpus-based methods in NL P, (ii) recent  \\nefforts to use WordNet  for NLP r esearch, (iii) the resurgence of inte rest in finite-state and other \\ncomputationally lean approaches to NLP , and (iv)   the initiation of collaborative projects to create \\nlarge grammar and NLP tools. Statistical methods are used in NLP for a num ber of purposes, e.g., \\nfor word sense disam biguation,  f or gene rating gr ammars and parsing, fo r determ ining st ylistic \\nevidences of authors and s peakers, and so on. Ch arniak (1995)  points out t hat 90% accuracy  can \\nbe obtained in assigning part-of-speech tag to a wo rd by applying s imple statistic al measures. \\nJelinek (1999 ) is a widely  cited source on the use of statistical meth ods in NLP, e specially  in \\nspeech processing. Rosenfield (2000) reviews st atisti cal language m odels for speech processi ng \\nand argues for a Bay esian approach to the integr ation of linguistic theories of data. Mihalcea & \\nMoldovan  (1999) m ention that althou gh thus far st atistical approaches have been considered the \\nbest for word sense disa mbiguation, they are useful  only in a sm all set of texts. Th ey propose th e \\nuse of WordNet  to im prove the results of statis tical analy ses of nat ural language texts. WordNet \\nis an online le xical referenc e system  developed at  Princeton University. This is an excellent NL P \\ntool containi ng English nouns, verbs, adj ectiv es and a dverbs organized into s ynony m sets, each \\nrepresenting one underl ying lexical concept. Details  of WordNet is available in Fellbau m (1998) \\nand on t he web (http://www.cogsci.princeton.edu/~wn/ ). WordNet is now used i n a num ber of \\nNLP rese arch and applications. One of the major applications of WordNet in NLP has been in \\nEurope with t he formation EuroWordNet in 19 96.  EuroWordNet is a multilingua l database wit h \\nWordNets for several European languag es  includi ng Dutch, Italian, Spanish, Germ an, French, \\nCzech and Es tonian,  structured in the same w ay as the WordNet for English \\n(http://www.hum.uva.nl/~ewn/ ).  The finite-state aut omation is  the mathematical device used to \\nimplement re gular expressions –  the standard  notation for characte rizing text sequences. \\nVariations of automata such as finite-state transducers, Hidden Markov M odels, and n-gram  \\n 3 grammars ar e important co mponents of speech r ecog nition and speech sy nthesis, spell-check ing, \\nand inform ation extraction which are the im portant ap plications of NLP. Different application s of \\nthe Finite State methods in NLP have been discu ssed by  Jurafsky  & Martin (2000), Kornai ( 1999) \\nand Roche & Shabes (1997 ). The work o f NLP rese archers has been  greatly  facili tated b y the \\navailability  of large-scal e grammar for parsi ng and ge neration. Researcher s can get access to \\nlarge-scale grammars and tools thro ugh several websites, for example Lingo \\n(http://lingo.s tanford.edu), Computational Linguistics & Phonetics (http://www.coli.uni-\\nsb.de/software.phtm l), and Parallel grammar project \\n(http://www.parc.xerox.com/istl/groups/nltt/pargram /). Another significant developm ent in recent \\nyears is the for mation of various national and inte rnational consortia and r esear ch groups that can \\nfacilitate, and  help share expertise, research  in NLP. L DC (Linguistic Data Consortium ) \\n(http://www.ldc.upenn.edu/) at the University  of Pe nnsy lvania is a typical exa mple that  create s, \\ncollects and distributes speech and text database s, lexicons, and other resources f or research a nd \\ndevelopm ent among univer sities, co mpanies and gove rnment research laboratories. The Parallel \\nGrammar pro ject is another exam ple of internati onal cooperation. T his  is a collaborative effort \\ninvolv ing res earchers fro m Xerox PARC in Califor nia, the University of St uttgar t and the \\nUniversity  of Konstanz in Germany , the Univer sity of Bergen in Norway , Fuji Xerox in Japan. \\nThe aim  of this project  is to prod uce wide coverage grammars for English, Fren ch, Germ an, \\nNorwegian, Japanese, and Urdu which are writte n collaboratively with a co mmonl y-agreed-upon \\nset of grammatical features (http://www.parc.x erox.com/istl/groups/nltt/pargram /). The recently \\nformed Global WordNet Association is y et another exam ple of cooperation. It i s a   non-\\ncommercial o rganization th at provides a platform  for discussing, sharing and con necting \\nWordNets for all languages in the world.  The first international Wor dNet conference to be held  in \\nIndia in earl y 2002 is expected to address various  problem s of NLP by researcher s from different \\nparts of the world. \\n \\nNatural Language Understanding  \\n \\nAt the core of any  NLP task there is the im portant issue of natural language understanding.  The \\nprocess of building com puter programs that unde rstand natural language involves three major  \\nproblem s: the first one relates to the t hought  process, the second one t o the representatio n and \\nmeaning of the linguistic input, and the  third one  to the world knowledge. Thus, an NLP sy stem  \\nmay begin at the word level – to determine the morpholo gical structure, nature (such as part-of-\\nspeech, mean ing) etc. of t he word – an d then may move on to the sentence level – to determ ine \\n 4 the word order, gra mmar, meaning of the entire sentence, etc.— a nd then to the context and the  \\noverall envir onment or domain. A giv en word or a  sentence may have a specific meaning or \\nconnotation in a given co ntext or dom ain, and may be related to many  other words and/or \\nsentences in t he given context.  \\n \\nLidd y (1998) and Feldm an (1999) su ggest that in  orde r to un derstand natural lang uages, it is \\nimportant to be able to dist inguish am ong the fo llowi ng seven inte rdependent levels, that peo ple \\nuse to extract meaning from  text or spo ken language s:  \\n• phonetic or phonol ogical l evel that deals with pronu nciation \\n• morphological level that deals with the smallest  parts of words, that  carry  a meaning, and  \\nsuffixes and prefixes  \\n• lexical level that deals with lexical meaning of words and parts of speech analy ses \\n• syntactic level that deals with grammar a nd structure of sentences  \\n• semantic leve l that  deals with the m eaning of words and sentences \\n• discourse level that deals with the structure of different kin ds of te xt using docu ment \\nstructures and  \\n• pragmatic lev el that deals with the knowledge  that comes fro m the outside world, i.e., \\nfrom  outside the contents o f the docum ent.  \\n \\nA natural language processing s ystem  may involve  all or some of these levels of analy sis.  \\n \\n \\nNLP Tools and Techniqu es \\nA num ber of researchers h ave attem pted to co me up with im proved technology  for perform ing \\nvarious activities that for m important parts of  NLP works. These works may  be c ategorized as \\nfollows:  \\n  \\n• Lexical and m orphological analy sis,  noun phrase generation, word segmentation, etc. \\n(Bangalore & Joshi, 1999; Barker & Cor nacchia,2000;  Chen & Chang, 1998;  Dogru & \\nSlagle, 1999; Kam -Fai et al .. 1998; Kazakov et al.. , 1999; L ovis et al.. 1998; Tol le &  \\nChen, 200 0; Zweigenbaum  & Grabar, 1999)  \\n \\n 5 • Semantic and  discourse analy sis, word meaning and knowledge representation (Kehler, \\n1997;  Mihalcea & Moldovan,1999; Me yer & Dal e, 1999; Pedersen & Bruce, 1998; \\nPoesio & Vieira,1998; Tsu da & Nakamura, 199 9)  \\n \\n• Knowledge-based approaches and tools for NLP (Argam on et al.., 1998;  Fernandez &  \\nGarcia-Serrano, 2000;   Martinez et al.., 2000, 1998).  \\n \\nDogru & Slagle (1999) pr opose a m odel of lexi con t hat involves automatic acquisition of t he \\nwords as well as representation of the semantic content of in dividual lexical entries. Kazakov et \\nal.. (1999) report  research  on word segmentation based on an automatically  generated annotated \\nlexicon of wo rd-tag pairs. Kam -Fai et al.. (199 8) repo rt the features of an NLP to ol called Chicon  \\nused for word seg mentation in Chinese text.  Zweigenbau m & Gr abar (1999)  propose a method \\nfor acquiring morphological  knowledge about words in medical literature. It takes advantage of \\ncommonly available lists of sy nony m terms to  bootstr ap the acquisition pr ocess. Although the \\nauthors experim ented with the method o n the SNOMED International Microglos sary for \\npatholog y in its French version, the y claim that since the method do es not rely on  a priori \\nlinguistic kn owledge, it is applicable to o ther la nguages such as English.  Lovis et  al.. (199 8) \\npropose the d esign of a lexicon for use i n the NLP of medical texts. \\n \\nNoun phrasin g is considered to be an im portant N LP technique used in inf ormation retrieval. One \\nof the m ajor goals of  noun phrasing re search is  to investigate the possibility  of combining \\ntraditional ke yword and s yntactic approaches with  semantic approaches to text processing in \\norder to im prove the qualit y of inf ormation retrie val. Tolle and Ch en (2000)  compared four noun  \\nphrase generation tools in order to assess thei r ability  to isolate noun phrases from  medical \\njournal abstracts databas es. The NLP tools evaluated were: Chopper  developed b y the Machin e \\nUnderstanding grou p at the  MIT Media Laborator y,  Automatic Indexer  and AZ Noun Phrase r \\ndeveloped at the  University of Arizona, and NPTool  a commercial NLP tool from   LingSoft , a \\nFinnish Compan y.  The National Library of Medicin e’s  SPECIALIST Lexicon  was used alo ng \\nwith the AZ Noun Phrase r. This experiment used a reasonably  large test set of 1.1 gi gabytes of \\ntext, com prising 714,451 a bstracts fro m the CANCER LIT database.  This study  showed that with \\nthe exception of Chopper, the NLP tools were fair ly com parable in their performa nce, measured \\nin terms of re call and precision. The study  also show ed that the SPECIALIST Lexicon increased \\nthe abilit y of the AZ Noun Phraser  to generate releva nt noun ph rases.  Pedersen and  Bruce \\n(1998) propose a corpus-based approach to wo rd-sense disam biguation that onl y requires \\n 6 inform ation that can be automatically  extract ed fro m untagged text. Barker and Cornacchia \\n(2000) describe a si mple system  for choosing noun phrases, fro m a document, based on thei r \\nlength, t heir frequency  of occurrence, an d the fre quency  of their he ad noun, using   a base noun \\nphrase ski mmer and an off-the-shelf online dictiona ry. This resear ch revealed some interestin g \\nfindings:   (1) the si mple noun phrase-based sy stem  perfor ms roughl y as well as a state-of-the-art, \\ncorpus-trained ke yphrase extractor; (2) ratings for individual ke yphrases do not neces sarily  \\ncorrelate with  ratings for sets of ke yphrases for a document; and (3) agreement am ong unbiased \\njudges on t he keyphrase rating task is poor.  Silber & McCoy  (2000) report  research that uses a  \\nlinear tim e algorithm  for calculating lexical chains, which is a method of capturi ng the \\n‘aboutness’  of a docum ent.  \\n \\nMihalcea & Moldovan (1999) argue t hat  the reduced  applicabilit y of statistical methods in word \\nsense disa mbiguation  is due basically  to the lack of widely available se mantically  tagged \\ncorpora. They report   research  that enables the automatic acquisition of sense tagged corpora, \\nand is based on (1) the information provided in WordNet, and (2) the inform ation gathered from  \\nInternet using existing search engines.   \\n \\nMartinez & Garcia-Serrano (19 98) an d Martinez et al.. (200 0)  pr opose a method for t he design of \\nstructured kn owledge m odels for NLP. The key features of their method com prise the \\ndeco mposition of linguistic  knowledge s ources in sp ecialized sub-areas to tackle the com plexity \\nproblem  and a focus on cognitive archit ectures that al low for m odularity , scalabilit y and \\nreusability . The authors clai m that their approach  profi ts from  NLP techniques, first-order logic \\nand som e modelling heuris tics (Martinez et al.. 200 0). Fernandez & Garcia-Serrano (20 00) \\ncomment that knowledge e ngineering is increasingly  regarded as a means to co mplem ent \\ntraditional for mal NLP models by  adding sy mbolic modelling and i nference cap abilities in a way  \\nthat facilitates the introduc tion and m aintenan ce of linguistic experience. They  propose an \\napproach that allows the design of linguistic a pplications that integr ates different formalisms, \\nreuses existin g language re sources and supports t he implementation of the requi red control in a \\nflexible way . Costantino (1999) argues t hat qualita tive data, particularly articles from  online news \\nagencies, ar e  not yet successfully  processed, and as a result, financial operators, notably  traders, \\nsuffer fro m qualitative data-overload. IE-Expert is a system  that com bines the t echniques of NLP, \\ninform ation extraction and expert sy stems in order to be  able to suggest invest ment decisions \\nfrom  large volume of texts (Constantino,  1999).  \\n \\n 7 Natural Language Text Processing Systems \\n \\nManipulation  of texts for knowledge extraction, fo r auto matic indexing and abstracting, or for \\nproducing text in a desired format, has been rec ognized as an im portant area of resear ch in NLP. \\nThis is broadly  classified as the area of natural la nguage text processing that allows structuring of  \\nlarge bodies of textual information with a view to retrieving particular info rmation or to de riving \\nknowledge structures that may be used for a sp ecific purpose. Aut omatic text p rocessing sy stem s \\ngenerally  take some form  of text  input and transform it into an out put of som e different form . The \\ncentral task for natural language text processi ng systems is t he translatio n of potenti ally \\nambiguous natural language queries and texts into unam biguous inte rnal representations on which \\nmatching and retrieval can  take place ( Liddy, 1998). A natural language text processing sy stem \\nmay begin  with morphological analy ses. Stemm ing of  terms, in both the queries and docum ents, \\nis done in order to get the morphological variants of the words inv olved. The le xical and synt actic \\nprocessing involve the uti lization of lexicons for deter mining the characteristi cs of the words,  \\nrecognition of their parts- of-speech, de termining the words and phrases, and for parsing of the \\nsentences.  \\n \\nPast research  concentrating on natural language  text  processing sy stem s has been reviewed  by \\nHaas (1986), Mani & May bury (1999), Smeaton ( 1999), and  Warner (1987).  S ome NLP systems \\nhave been built to process texts using particular small  sublanguages to reduce the size of the  \\noperations and the nature of the com plexities.  These domain-specifi c studies are largely  known as \\n'sublanguage analy ses' (Grish man & Kittredge, 1986). Som e of these studies are li mited to a  \\nparticular subject are a such as medic al scienc e, wherea s others deal with  a specifi c type of \\ndocum ent such as patent texts.  \\n \\n \\nAbstracting  \\n \\nAutomatic ab stracting and text summari zation are now used sy nonymously that aim to generate \\nabstracts or s ummari es of texts. This are a of N LP rese arch  is becoming m ore common in the web \\nand digital library  environment.  In simple abst racting or summari zation sy stems, parts of text – \\nsentences or paragraphs – are sel ected autom atically based on some linguistic and/or statistical \\ncriteria to produce the abstract or su mma ry. More  sophisticated sy stems may merge two or more \\n 8 sentences, or parts thereof, to generate one cohe rent sentence, or may generate sim ple summar ies \\nfrom  discrete items of data.   \\n \\nRecent interests in autom atic abstracting and text summarization are reflect ed by the huge \\nnumber of research papers appearing in a num ber of i nternational conferences an d workshops \\nincluding ACL, ACM, AA AI,  SIGIR, a nd va rious national and reg ional chapters of the \\nAssociations.   Several tech niques are used for autom atic abstractin g and text summa rization. \\nGoldstein et al.. (19 99) use conventional IR methods  and lin guistic cues for extracting and \\nranking sentences for generating news article  summar ies. A num ber of studies on text \\nsummarizatio n have been reported recently. Silber an d McCoy  (2000) claim  that their  linear ti me \\nalgorithm  for calculating lexical chains is an efficient  method for p reparing auto matic \\nsummarizatio n of  d ocuments. Chuang a nd Yang (2000) report a text summarization techniq ue \\nusing  cue phrases appeari ng in the texts of US patent abstracts.  \\n \\nRoux and  Ledora y (2000) report a proje ct, called  Aristotle, that aims to build a n autom atic \\nmedical data system  that is capable of produc ing a semantic representation of th e text in a \\ncanonical form . Song  and  Zhao (20 00) propose a method of auto matic abstracting that i ntegrates \\nthe advantages of  bot h linguistic and statistical analy sis in a corpus.  Jin and  Don g-Yan (2000)  \\npropose a m ethodol ogy for  generating autom atic abstracts that  provides an integ ration of the \\nadvantages of methods based on linguist ic analy sis and those based on statistics. \\n \\nMoens and Uy ttendaele (1997) describe the SALOM ON (Su mmar y and Anal ysis of Legal t exts \\nFOR Managi ng Online Needs) project t hat automa tically  summari zes legal text s written in Dutch. \\nThe sy stem  extracts releva nt inform ation from  the fu ll texts of Bel gian cri minal case s and us es it \\nto summariz e  each deci sion. A text gra mmar represented as a semantic network is used to \\ndeter mine the category  of each case. T he system  extracts r elevan t information about each case,  \\nsuch as the na me of the court that issues the deci sion, the decision date, the offences charged, the \\nrelevant statu tory provisions disclosed by the c ourt, as well as the legal principles applied in  the \\ncase.  R AFI (resu me automatique a fra gments i ndicateurs) is an automatic tex t summari zation \\nsystem  that transform s full text scientific and te chnical docu ments into condensed t exts \\n(Leh mam, 1999).  RAFI adopts  discourse analy sis technique usi ng a thesaurus for recognit ion \\nand selection of the  most pertinent elements of texts. The sy stem assumes a typical structure of  \\nareas fro m each scientif ic document, viz. pr evious knowledge, cont ent, method and new \\nknowledge.  \\n 9  \\nMost of the a utomatic abstracting and text su mmariz ation s ystems work satisfact orily  within a \\nsmall text collection  or wit hin a restricted  dom ain. Building r obust and dom ain-independent \\nsystems is a c omplex and resource-intensive task. Arguing that pure ly autom atic abstracting \\nsystems do not alway s produce useful results, Crave n (1988, 1993, 2000) proposes a hy brid \\nabstracting syste m in whic h som e tasks are perfo rmed by human abstractors and others b y an \\nabstractor’ s assistanc e software c alled TEXNET. However, rec ent experiments on the usefulness \\nof the autom atically  extracted key words a nd phrases from  full texts  by TEXNET in the actual \\nprocess of abstracting by  human abstract ors show ed some consider able variation am ong subjects, \\nand onl y 37%  of the subjects found t he key words and phrases to be useful in writing their \\nabstracts ( Craven, 2000).  \\n \\n \\nInformation Extraction \\n \\nKnowledge discovery  and data mining have beco me important are as of resear ch over the past few \\nyears and a num ber of infor mation scien ce  journals have published special issue s reporting \\nresearch on these topics (see for exam ple, Benoit,  2001;Qin and N orton, 1999; Raghavan et al.., \\n1998;  Trybula, 1997;  Vickery , 1997). Knowledge  discovery  and data mining research use a \\nvariety  of techniques in order to extract useful information from  source documents. Inform ation \\nextraction (IE) is a subset of  knowledge discovery  and data mining r esear ch that  ai ms to extract \\nuseful bits of textual infor mation from  natura l language texts  (Ga izauskas & Wilks, 1998). A \\nvariety  of inf ormation extraction (IE) techniques are used and the extracted infor mation can be \\nused for a num ber of purposes, for exam ple to pr epare a su mmary  of texts, to populate databases, \\nfill-in slots in fram es, identify ke ywords and phrase for inform ation retrieval, and so on. IE \\ntechniques are also used for classify ing text ite ms according to som e pre-defined categories. A n \\nearlier exa mple of text categorization s ystem is CONSTRUE, developed for Reut ers,  that \\nclassifi es news stories (Hay es, 1992). T he CONSTR UE software was subsequently generalized \\ninto a commercial product called TCS (Text Categor ization Shell). An evaluation of five text \\ncategorizatio n system s has been reported b y Yang an d Liu (1 999).  \\n \\nMorin (19 99) suggests that althoug h many IE systems can successfull y extract term s from \\ndocum ents, acquisition of  relati ons between terms is still a difficulty. PROMETHEE is a system  \\nthat extracts l exico-sy ntactic patterns rel ative to a specific concept ual relation from  technical \\n 10 corpora (Mor in,19 99). Bo ndale et al..  (1999) sug gest that IE sy stems must operate at many  \\nlevels, fro m word recognit ion to discour se analy sis at the level of th e complete d ocument. They \\nreport an appl ication of the Blank Slate Language  Processor (BSLP)  approach for the analy sis of \\na real life nat ural language corpus that consists  of responses to ope n-ended quest ionnaires in t he \\nfield of adver tising.  \\n \\nGlasgow et al .. (1998) report a sy stem  called MITA (Metlife’s Intelligent Text Analy zer) that \\nextracts information from  life insurance appli cations. Ahonen et al.. (199 8) pro pose a general \\nframework for text m ining that uses pragmatic and discourse level a nalyses of text. Sokol et al.. \\n(2000)  report  resear ch that uses  visualization and NL P  technologi es to perform  text m ining. \\nHeng-Hsou et al.. (2000) argue that IE s ystems are usu ally event-driven (i.e., are usually based on \\ndomain knowledge built on various events) and  propose an even t detection driven intelligent \\ninform ation extraction b y using the neur al netw ork paradigm . They  use the backpropagation  (BP) \\nlearning algor ithm to train the event detector, and  app ly NLP technolog y to aid t he selection of \\nnouns as feature words which are supposed to charact erize docu ments appropriat ely. These nouns \\nare stored in ontolo gy as a knowledge ba se, and are used  for the ext raction of useful inform ation \\nfrom  e-mail mes sages.  \\n \\nCowie and Lehnert (1996)  reviewed the earlier research on IE and  commented that the NLP \\nresearch co mmunity  is ill-prepared to tackle the difficult problems of sem antic f eature-tagging, \\nco-referenc e resolution, and discourse analy sis, all of which are i mportant issues of IE researc h. \\nGaizauska s and Wilks (1998) reviewed the IE  resear ch from  its origin in the Artificial \\nIntelligence world in the s ixties and seventies th rough to the m odern da ys. The y discussed the \\nmajor IE projects undertaken in different sector s, viz., Academic Resear ch. E mployment, Fault \\nDiagnosis, Finance, Law, Medicine, Military  Intelligence, Police, Software Sy stem  Requirements \\nSpecification, and Technol ogy/Product Tracking.  \\n \\nChowdhur y (1999a) revie wed research that used te mplate mining t echniques in: the extraction of \\nproper nam es from  full text docum ents, extracti on of facts fro m press rele ases, abstracting \\nscientific papers, su mmarizing new pro duct inform ation, extracting specific information  from  \\nchemical texts, and so on. He also discu ssed how some w eb sear ch engines use te mplates to \\nfacilitate infor mation retrieval. He reco mmends that  if  each web author is given a tem plate to  \\nfill-in in order to characteri ze his/her document, then eventuall y a more controlled and s ystem atic \\nmethod of creating docum ent surrogates can be achie ved. However, he warns that  a single all-\\n 11 purpose m etadata form at will not be appl icable for all authors in all the dom ains, and further \\nresea rch is ne cessary to come up with appropriate formats for ea ch.  \\n \\nArguing that IR has been the subject of resea rch and developm ent and has been delivering \\nworking sol utions for m any decades whereas IE is a more recent an d emerging technology,  \\nSmeaton (1997) comments that it is of interest to the IE community to see how a related task, \\nperhaps the m ost-relat ed task, IR, has mana ged to use the NLP base technology in its \\ndevelopm ent so far. Co mmenting o n the future challenges of IE rese archers, G aizauskas and \\nWilks (199 8) mention that t he  performance levels of  the common IE sy stem s, which stand in the \\nrange of  50 % for co mbined recall and precision, sho uld im prove significantl y to satisfy  \\ninform ation analy sts. A m ajor stum bling block  of IE systems developm ent is the cost of \\ndevelopm ent. CONSTR UE, for exam ple required 9.5 person y ears of effort (Hay es & Weinstei n, \\n1991). Portabilit y and scalabilit y are also two big issue s for IE sy stems. Since they depend \\nheavily on t he dom ain knowledge, a given IE sy stem  may work satisfactorily  in a relatively  \\nsmaller text collection, but it may  not per form well  in a  larger collection, or i n a different \\ndomain. Alter native technologies are now being used to overcom e these proble ms. Ada ms (20 01)  \\ndiscusses the merits of the NLP and the wrapper induction technol ogy in inform ation extraction \\nfrom  the web docum ents.  In contrast to NLP,  wrapper inductio n operates independentl y of \\nspecific do main knowledge. Instead of analy sing the meaning of discourse at the  sentence lev el, \\nthe wrapper technology identifies relev ant cont ent based on the textual qualitie s that surround \\ndesired data. Wrappers operate on the surface fe atures of docum ent texts that characteriz e train ing \\nexam ples. A number of vendors, such a s Jango  (purchased by Exc ite), Junglee  (purchased by  \\nAmazon), and Mohomine  employ wrap per inductio n technolog y (Adams, 2001).  \\n \\n \\nInform ation Retrieval \\n \\nInform ation r etrieval has been a major area of app lication of NLP, and consequentl y a  number of \\nresea rch projects, dealing with the various applications on NLP in IR, have taken place \\nthroughout  the world resulting in a large volum e of publications.  L ewis and Sparck Jones (1996) \\ncomment that the generic challenge for NLP in th e field of IR  is whether the neces sary NLP of \\ntexts and que ries is doable, and the specific cha llenges are whether non-statistical and statistical \\ndata can be com bined and whether data about individ ual docum ents and whole files can be \\ncombined. Th ey further comment that there ar e major challenges in making the NLP technolog y \\n 12 operate effect ively and efficiently  and also in cond ucting appr opriate evaluation tests to assess \\nwhether and how far the approach work s in an envi ronment of interactive se arching of large te xt \\nfiles. Feld man (199 9) sug gests that in order to achieve success in IR, NLP techniques should be \\napplied in co njunctio n with other techn ologies, such as visualizati on, intelligent agents and \\nspeech recog nition.   \\n \\nArguing that  syntactic phra ses ar e more meaningful than statisticall y obtained w ord pairs, and \\nthus are more powerful for discrim inating am ong documents, Narita and Ogawa (2000) \\nuse a shallow  syntactic processing instea d of st atistica l processing to autom atically identify \\ncandidate phrasal ter ms from query  texts. Com paring the performance of  Boolean and natural \\nlanguage sear ches,  Paris and Tibbo (1998) found  that in their experi ment, Boolean sear ches  had \\nbetter results than freesty le (natural language) sear ches. However, t hey conclude d that  neither \\ncould be cons idered as the best for every query . In other words, their conclusion was that \\ndifferent queries dem and different techniques.  \\n \\nPirkola (20 01) shows that languages vary significantl y in their m orpholo gical pr operties. \\nHowever,  for each language there are two variab les that describe the morphologi cal co mplexity, \\nviz.,  index of  synthesis (IS) that describes the am ount of affixation in an individual language, i .e., \\nthe average num ber of morphemes per w ord in th e lan guage;  and i ndex of f usion (IF) that \\ndescribes the ease with which two m orphem es can be separated in a la nguage. Pir kola (2001) \\nshows that calculation of t he ISs and IFs in a la nguag e is a relatively sim ple task, and once the y \\nhave been established,  the y could be utilized  fruitfully in em pirical  IR resea rch a nd system  \\ndevelopm ent.   \\n \\nVariations in presenting subject matter  greatly  affect IR and hence linguistic vari ation of \\ndocum ent tex ts is one of the greatest cha llenges to  IR. In order to inves tigate how consistently \\nnewspapers c hoose words and concepts to descr ibe an event, Lehtokangas & Jar velin (2001) \\nchose article s on the sam e news fro m three Finnish newspapers. Their experiment revealed that \\nfor short newswire  the consistency  was 83%  and fo r long articles 47% . It was also revealed that  \\nthe newspape rs were very  consis tent in using concepts to represent events, with a level of \\nconsistency  varying between 92-97% . \\n \\n \\n 13 Khoo et al.. ( 2001) rep ort an experim ent that investig ates whether inform ation obtained b y \\nmatching cause-eff ect relat ions expresse d in docu ments with the cause-effe ct rel ations expressed \\nin user queries can be used to im prove re sults in document retrieval  com pared wi th the use of \\nonly the keywords withou t considering the rela tions. Their experiment with the Wall Street \\nJournal full te xt database re vealed that ca usal relations matching where either the cause or the \\neffect is a wildcard can be used to im prove info rmation retrieval effectiveness if t he appropriate \\nweight for each ty pe of m atching can be deter mined for each query . However, the  authors stress \\nthat the results of this stud y were not as strong as the y had expected it to be .  \\n \\nChandrasekar  & Srinivas (1998) propose that c oherent text contai ns significant latent \\ninform ation, such as sy ntactic structure and patte rns of language u se, and this in formation could \\nbe used  to i mprove the perform ance of inform ation retrieval sy stems. They  describe a sy stem , \\ncalled Glean , that uses sy ntactic infor mation for eff ectively filtering irrele vant documents, and \\nthereby  improving  the pre cision of info rmation retrieval sy stem s.   \\n \\nA num ber of tracks (research groups or themes) in the TREC series of experiments deal directly \\nor indirectl y with NLP and inform ation retrieva l, such as the cross-language track, filtering tr ack, \\ninteractive tra ck,  question-answering tra ck, and the w eb track.  Reports of progress of the NLIR \\n(Natural Lan guage Information Retrieval) project are  available in the TREC reports (Perez-\\nCarballo & Strzalkowski, 2000;  S trzalkowski. et al.., 1997, 1998, 1999).   The major goal of t his \\nproject has been to dem onstrate that robust NLP techniques used f or indexi ng and searching of \\ntext docum ents perfor m better co mpared to the si mple key word and string-based methods used in \\nstatistical full-text retrieval (Strzalkowski, T. et  al.., 1999). However, results indicate that sim ple \\nlinguisticall y motivated indexing (LMI)  did n ot prove to be m ore effective than well-executed \\nstatistical approaches in English language texts.  Never theless, it was noted that m ore detailed \\nsearch topic statements responded well t o LMI com pared to  terse one-sentence search queries.  \\nThus, it was concluded t hat quer y expans ion, us ing NLP techniques,  leads to a sustainable \\nadvances in IR effectiveness (Strzalkowski et al.., 1999).   \\n \\n \\nNatural Language Interfaces \\n \\nA natural language interface is one that accepts  query  statements or commands i n natural \\nlanguage and sends data to so me system , typical ly a retrieval sy stem, which then r esults in \\n 14 appropriate  r esponses to the commands or quer y statements. A nat ural language interface should \\nbe able to translate the natural language statem ents into appro priate actions for th e system .  \\nA large num ber of natural language interfaces that work reasonably well  in narrow dom ains have \\nbeen reported in the literature (for review of  such s ystems see Chowdhur y, 1999b, Chapter \\n19;Haas, 1996; Stock, 2000).  \\n \\nMuch of the e fforts in natur al language interface desig n to date have  focused on handling rather  \\nsimple natural language qu eries. A nu mber of questio n answering sy stem s are now being \\ndeveloped tha t aim to provi de answers to  natural langu age questions, as opposed t o docum ents \\ncontaining i nformation related to the q uestion. Su ch system s often use a variety  of IE and IR \\noperations usi ng NLP tools  and techniqu es to get the correct answer  from  the source texts. Bre ck \\net al. (1999) r eport  a question answering sy stem  that uses techniques fro m knowledge \\nrepresentatio n, inform ation retrieval, and NLP. Th e authors claim  that this com bination enables \\ndomain independence and robustness in t he face of te xt variabilit y, both i n the question and in the \\nraw text documents used as knowledge s ources. Research reported i n the Question Answering \\n(QA) track of   TREC  (Text Retrieval C onferences) s how some interesting resul ts.   The basic \\ntechnolog y used by the participants in th e QA track included several steps. First, cue \\nwords/phrase like ‘who’ (as in ‘who is the prime minister of Japan’ ), ‘when’  (as in ‘When did the \\nJurassic perio d end’ ) were identified to guess wh at was needed; and then a small portion of the  \\ndocum ent collection was retrieved using standard te xt retrieval tech nology. This was followed  by \\na shallow parsing of the ret urned docum ents for id entify ing  the entities required for an answer. If \\nno appropriate answer ty pe was found then best matching passage was retri eved. This approa ch \\nworks well as long as the q uery types  recognized by the system  have broad cov erage, and the \\nsystem  can classify  questions reasonably accura tely (Voorhees,1999). In TREC-8, the first QA \\ntrack of TREC,  the m ost accurate Q A system s could answer more than 2/3 of the questions \\ncorrectly . In the second QA  track (TREC-9), the best perform ing QA sy stem , the Falcon sy stem \\nfrom  Southern Methodist University ,  was able to answer 65% of the questions  (Voorhees, 2000). \\nThese results are quite i mpressive in a domain- independent question answering environm ent. \\nHowever, the questions were still si mple in the first t wo QA tracks. In the fut ure more co mplex \\nquestions requiring answers to be obtained from  more than one documents will be handled by  QA \\ntrack rese archers.    \\n \\n Owei (2000)  argues that the drawbacks of m ost natural language interfaces to database systems \\nstem  primarily from  their weak interpretative power  which is caused by  their  i nabilit y to deal \\n 15 with the nuances in human use of natural language . The author further argues that the difficult y \\nwith NL database query  languages (DBQLs) can be overcome by  combining  c oncept based \\nDBQL paradi gms with NL approaches to enhan ce the overall ease-o f-use of the query  interface . \\n \\nZadrozn y et al. (200 0) sug gest that in an  ideal information retrieval envi ronm ent, users should be \\nable to express their interes ts or queries direc tly and naturally , by speaking, typing, and/or \\npointi ng; the computer sy stem then should be able  to provide i ntelligent answers or ask relevant \\nquestions. Ho wever, they  comment that even thou gh we build natu ral language sy stem s, this goal \\ncannot be full y achieved due to lim itatio ns of  science,  technology, business knowledge, and \\nprogramming environm ents. The specifi c problem s include (Zadrozny  et al., 2000):  \\n• Limitations of NL understanding;   \\n• Managing the  complexities of interaction (for exam ple, when using NL on devic es with \\ndiffering ban dwidth);  \\n• Lack of precise user models (for exam ple, knowing how dem ographics and personal \\ncharacteristics of a person should be reflected in the type of lang uage and dialog ue the \\nsystem  is usi ng with the user), and  \\n• Lack of m iddleware and toolkits.  \\n \\nNLP Software \\n \\nA num ber of  specific NLP  software products have  been developed  over the past decades, so me \\nof which are available for free, while others  are avail able co mmer cially. Many such NLP \\nsoftware pack ages and tool s have alread y been m entioned in t he discussions through out this \\nchapter. Some more NLP t ools and soft ware are mentioned in this section. \\n \\nPasero & Sab atier (1998)  describe principles underly ing ILLICO, a generic natural-language \\nsoftware tool for buil ding larger applications  for perfo rming specific linguistic tasks such as \\nanaly sis, synthesis, and gui ded com position. Lid dy (1998) and  Liddy et al. (20 00) discuss the \\ncommercial u se of NLP in IR with the exam ple of DR-LINK (Document Retrieval Using \\nLINguistic Knowledge) system  dem onstrating the capabilities of NLP for IR. Detailed product  \\ninform ation a nd a dem o of DR-LINK are now available online (http://www.textwise.co m/dr-\\n 16 link.htm l).   Nerbonne et al. (1998) report on GLOSSER, an intelligent assistant for Dutch \\nstudents for learning to read French. Scott (1999) describes the Kana Customer M essaging \\nSystem  that can categorize inbou nd e-m ails, forward th em to the rig ht department and generally \\nstrea mline the response process. Kana  also has an auto-suggestion function that helps a customer \\nservice repr esentative answer questions on unf amiliar t erritory . Scott  (1999)  describes another \\nsystem , called Brightware, that  uses NLP techniques  to elicit meaning from  groups of words or \\nphrases and reply  to som e e-mails automatic ally.  NLPWin is an NLP sy stem  from Microsoft  that \\naccepts sente nces and delivers detailed sy ntactic analy sis, together with a logical form   \\nrepresenting an abstraction of the m eaning (E lworthy, 2000). Scarle tt and Szpakowicz (2000) \\nreport a diagnostic evaluati on of DIPETT, a br oad-coverage parser  of English sentences.  \\n \\nThe Natural Language Processing Laboratory , Center  for Intelligent  Inform ation Retrieval at t he \\nUniversity  of Massachusett s,  distributes sour ce codes and executables to support IE sy stem  \\ndevelopm ent efforts at other sites. Each module is designed to be us ed in a dom ain-specific an d \\ntask-specific custo mizable IE system . Available software includes (Natural Lan guage …, n.d .) \\n• MARMOT Text Brackettin g Module ,  a text file translator which segments arbitrary  text \\nblocks into sentences, applies low-level s pecialists suc h as date reco gnizers, assoc iates \\nwords with part-of-speech tags, and brack ets the text into annotated noun phrases, \\nprepositional phrases, and verb phrases.  \\n• BADGER Extraction M odule, that analy zes bracketed t ext and produces c ase fr ame \\ninstantiations according to application-specific domai n guidelines.  \\n• CRYSTAL Dictionary Induction Module , that learns text extraction rules, suitable for use \\nby BADGER, from  annotated training te xts.  \\n• ID3-S Inducti ve Learning M odule , a variant on ID3 w hich induces decision trees  on the \\nbasis of training exam ples.  \\n \\n \\nWaldrop (2001) briefly  describes the features of  three NLP softwar e packages, viz.  \\n \\n• Jupiter ,  a product of t he MIT resear ch Lab that  works in the fiel d of weather forecast \\n• Movieline , a product of Carnegie Mellon that talks ab out local m ovie schedules, and   \\n 17 • MindNet from  Microsoft Rese arch, a s ystem  for automatically  extracting a massively  \\nhyperlinked  web of concepts, from , say, a standard d ictionary. \\n \\nFeldman (199 9)  m entions a num ber of NLP software packages, such as \\n• ConQuest , a part of Excalibur , that incorporates a lexicon that is im plemented as a \\nsemantic network  \\n• InQuery that parses sentences, stems words and recognizes proper nouns and concepts \\nbased on term  co-occurre nce  \\n• The Linguist X parser  from  XERO X PARC that extracts sy ntactic information, and is \\nused in InfoSeek   \\n• Text m ining sy stem s like NetOwl from SRA and KNOW-IT  from TextWise .  \\n \\nA recent surv ey of 68 E uropean university centres in com putational linguistics and NLP, carried \\nout under the auspices of a Socrates Wor king Grou p on Advanced Computing in the Humanities, \\nrevealed that Java has alr eady reached the status of second m ost commonly  taught programmi ng \\nlanguage (Black et al., 2000). In addi tion, Jav a based program s are being used to develop \\ninteractive instructional materials.  Black et al . (2000) review so me Java-bas ed coursewar e in use \\nand  discuss  the issues involved in m ore com plex nat ural language processing applications that \\nuse Java.   \\n \\n \\nInternet, Web and Digital  Library Applications of NLP  \\n \\nThe Internet and the web h ave brought  significant improvem ents in the way  we create, look for \\nand use infor mation. A hu ge volum e of inform ation is now available throug h the  Internet and \\ndigital libraries. However, these developments ha ve made so me proble ms relat ed to inform ation \\nprocessing and retrieval more prom inent.  According to a recent Survey  (Global Reach, 2001), \\n55% of the Internet users are non-Engl ish speak ers an d this is increasing rapidl y, thereby \\nreducing the percentage of net users who are na tive English speakers. However, about 8 0%  of \\nthe Internet and digital library  resources availabl e today are in English (Bian, Guo-Wei & Che n, \\n2000).  This c alls for the urgent need for the est ablishment of  m ultilingual inf ormation sy stems \\nand CLIR facilities.   How to m anipulate the large vol ume of multilingual data has beco me a \\nmajor research question. In fact, several issues ar e involved here. At the user interface level, th ere \\nhas to be a qu ery translatio n system  that should transla te the quer y from the user’s native \\n 18 language to t he language o f the sy stem . Seve ral approaches have been proposed f or quer y \\ntranslation. The dictionar y based approach uses a bilingual dictionary  to convert  terms from the \\nsource language to the target langua ge. Coverage and up-to-datene ss of the bilingual dictionar y is \\na major issue here. The corpus-based approach u ses parallel corpora for word selection, where the \\nproblem  lies with the dom ain and scale of the co rpora. Bian & Chen (2000)  propose a Chinese-\\nEnglish CLIR sy stem  on www, c alled MTIR,  that integrates the query  translation and document \\ntranslation. They  also address a nu mber of issues of  machine trans lation on the web, viz., the role \\nplayed by  the HTML tags in translation, the trade-off between the s peed and performance of the \\ntranslation syste m, and the form  in whic h the translated material i s presented.  \\n \\nStaab et al. (1999) describe the features of an  intellige nt inform ation agent called GETESS that \\nuses semantic methods and NLP capabilities in orde r to gather tourist  inform ation from  the web \\nand present it to the hum an user in an int uitive, user-friendl y way. Ceric (2000) reviews the \\nadvancem ents of the web search technology  and mentions that, among others, NLP technologies \\nwill have very good im pact on the success of th e sear ch engines.  Mock and Vem uri (1997) \\ndescribe the Intelligent News Filtering Organizationa l System  (INFOS) that is designed to filter \\nout unwanted news ite ms from  a Usenet.   INFOS  builds a profile of user interests  based on the \\nuser feedback . After the user browses e ach article , INFOS asks the user to rate the article, and \\nuses this as a criterion for selection (or rejection) of similar articl es next tim e rou nd. News \\narticles are cl assified by  a simple key word method, called the Global Hill Cli mbing (GHC), that \\nis used as a si mple quick-pass method. Articles that cannot be classified by  GHC are passed \\nthroug h a WordNet knowledgebase through a Case based reasoning (CBR) module which is a \\nslower but more accurate method. Very sm all-scale evaluation of INFOS suggests that the \\nindexing  pattern method, i. e., mapping o f the words from  the input text into t he correct concepts \\nin the WordN et abstraction hierarchy , correctly  classif ied 80% of the article s; the major rea sons \\nfor errors being the weakness of the sy stem to disa mbiguate pronouns.  \\n \\nOne of the major stum bling blocks of providin g perso nalized news delivery to us ers over the \\nInternet is the problem  involved in the autom atic association of related item s of different media \\ntype. Carrick and Watters (1997) describe a sy stem  that ai ms to det ermine to what degree any  two \\nnews ite ms re fer to the sa me news event.  This resear ch focused on deter mining the associatio n \\nbetween photographs and s tories b y usin g nam es. The algorithm  developed in co urse of this \\nresea rch wa s tested against  a test data se t as well as n ew data set s.  The pair of news ite ms an d \\n 19 photos generated b y the s ystem were checked b y human experts. The sy stem  performed,  in term s \\nof recall, precision and tim e, sim ilarly  on the new data sets as it did on t he traini ng set.  \\n \\nBecause of the volum e of text available on the web, many  researchers have proposed to use t he \\nweb as the testbed for NLP research. Grefenstette  (1999) argues that although noi sy, web text \\npresents language as it is used, and statistics de rived from  the web can have practical uses in \\nmany  NLP ap plications.  \\n \\n \\nMachine Translation and CLIR \\n \\nWith the prol iferation of the web and digita l libraries, m ultilingual inform ation retrieval has \\nbeco me a major challenge. There are two  sets of  issues here: (1) recogniti on, m anipulation an d \\ndisplay of m ultiple languages, and (2) cross-langua ge inform ation search and retrieval (Peter & \\nPicchi, 1997) .  The first set of issues relate to  the enabling technol ogy that will al low  users to \\naccess infor mation  in what ever language it is stored; while the second set im plies per mitting  \\nusers to specify  their infor mation needs in their preferred langua ge while retrieving inf ormation \\nin whatever language it is stored.  Text t ranslati on can take place at two levels: (1) translation of \\nthe full text fr om one language to anothe r for the pur pose of search and retrieval, and (2) \\ntranslation of queries fro m one language to one or  more different languages. The first option  is \\nfeasible for s mall collectio ns or for specific a pplications, as in mete orological reports (Oudet, \\n1997).  Translation of q ueries is a more practicab le approach and pro mising results have been \\nreported in the literature (discussed below).  \\n \\nOard (1997) comments that seeking infor mation from  a digital libra ry could benefit from  the \\nabilit y to query large collections once using a single l anguage. Furt herm ore, if the retrieved \\ninform ation is not available in a language that the user can read, some form  of translation will be \\nneeded. Multi lingual thesauri such  as EUROVOC help to address thi s challenge by facilitating \\ncontrolled vocabulary  search using terms fro m sever al languages, and services such as INSPEC \\nproduce Engl ish abstracts for docum ents in ot her lan guages (Oard, 1997).  However, as Oard \\nmentions, fully autom atic MT is presently neither sufficiently  fast nor sufficiently accurate to \\nadequately  support i nteractive cross-language info rmation seeking i n the web and digital librari es. \\nFortunatel y, an active and rapidl y growing r esearch co mmunity  has coalesced around t hese and \\n 20 other related issues, apply ing techniq ues drawn fro m several fields - notabl y IR a nd NLP - to \\nprovide access to large m ultilingual colle ctions. \\n \\nBorgman (1997) comments that we have hundreds (and som etimes thousands) o f years worth of \\ntextual materials in hundre ds of languag es, create d long before dat a encoding st andards existed. \\nShe  illustrates the multi-language DL challenge with exam ples drawn from  the r esearch library \\ncommunity , which t ypically handles col lections of materials in  ab out 4 00 differ ent languages.  \\n \\nRuiz and  Sri nivasan (1998) investigate an  autom atic method for C LIR that utili zes the \\nmultilingual Unified Medical Language S ystem  (UMLS) Metathesaurus to translate Spanish \\nnatural-language queries into English. They  conc lude that  the UMLS Metathesa urus-based CLIR \\nmethod is at least equivalent to, if not better,  than m ultilingual dictionar y based approaches. Dan-\\nHee  et al. (2 000), comm ent that there i s no re liable guideline as to how large machine readable \\ncorpus resources should be com piled to develop pr actical NLP soft ware package and/or com plete \\ndictionaries for hum ans an d com putational use. They propose  a ne w mathematical tool: a \\npiecewise cur ve-fitting algorithm , and suggest how  to determ ine the tolerance error of the \\nalgorithm  for good  predicti on, usin g a sp ecific corpus.  \\n \\nTwo Tele matics Application Program  projects in the Tele matics fo r Libraries se ctor, TRANS LIB \\nand CANAL/ LS, were acti ve between 1995 and 1997 (Oard,1997). Both these pr ojects \\ninvestigated cross-language searching in library  catalogs, and each included Engl ish, Spanish a nd \\nat least one other language: CANAL/LS added German and French, while TRANSLIB added \\nGreek. MULINEX, another European pr oject,  is concerned with the efficient use of m ultilingual \\nonline inf ormation. The pr oject ai ms  to process multilingual inf ormation and pr esent it to the \\nuser in a way  which facilitates finding and ev aluating t he desired information quic kly and \\naccurat ely (MULINEX, n.d.). TwentyOne , started in 1996, is a EU f unded pr oject which has the \\ntarget to develop a tool  for efficient dissemina tion of multimedia inform ation in the field of \\nsustainable development (Twenty One, n.d.). Deta ils of these and CLIR res earch projects in the \\nUS and other parts of the world have been reviewed by Oard & Diekam a (1998).  \\n \\nMagnini et al.   (2000) repor t two projects where NLP has been used for im proving  the \\nperformance i n the public adm inistration  sector. The first project, GIST, is concerned with \\nautomatic multilingual generation of inst ructiona l text s for form -filling. The second project, \\n 21 TAMIC, ai ms at providing an interface f or inte ractive acce ss to infor mation, centered on NLP \\nand supposed to be used by the clerk but with the active participation of the citizen. \\n \\nPowell and Fox (1998) describe a federa ted search sy stem, call ed SearchDB-M L Lite ,  for \\nsearching heterogeneous multilingual the ses and di ssertations collections on the World Wide Web \\nNDLTD: Net worked Digit al Library  of Theses and Dissertations ( NDLTD, n.d.). A markup \\nlanguage, called SearchDB , was developed for describing the characteristic s of a search engine  \\nand its interface, and a prot ocol was built  for re questing word trans lations between languages. A \\nreview of the results generated from  query ing over 50  sites sim ultaneously revealed that in some \\ncases more so phisticated query  mapping i s necessary  to retrieve resul ts sets that truly correspond \\nto the orig inal quer y. The authors report that an extended version o f the SearchDB  markup \\nlanguage is being develope d that can reflect th e default and available query m odifiers for each \\nsearch engine; work is also underway to implem ent a mapping s ystem  that uses t his inform ation \\n \\nA num ber of companies now provide m achine translation service, for exa mple (McMurchie, \\n1998): \\n• Berlitz International Inc. that offers pr ofessional translation service in 20 countri es \\n• Lernout & Hauspie  has an Internet Translation Division  \\n• Orange, Calif -based Language Force Inc.  that has a product called Universal tra nslator \\nDeluxe  \\n• IBM MT services through i ts WebSphere Translation Server. \\n \\nA large num ber of resear ch papers are av ailable that discuss various resea rch projects dealing \\nwith MT and CLIR with reference to specific  languages, for exam ple in Chinese (Kwok et al. \\n2000;  Lee et al., 1999), Japanese (Jie  & Akahor i , 2000; Ma, et al . 2000; Ogura  et al. 2000)  , \\nPortugese (Barahona & Al feres, 1999),  Sinhalese (Herath &  Herat h, 1999), Spanish (Weigard & \\nHoppenbro uwers,1998; M arquez et al., 2000),  Thai (I sahara et al.,2 000), T urkish  (Say , 1999), \\nand so on.  Some studies have considered m ore than two languages; see for exam ple Ide, 2000. \\nThese papers address various issues of MT, for example,  \\n \\n• Use of cue phrases in deter mining relationships am ong the lexical units in a disc ourse \\n(Say, 199 9); \\n 22 • Generation of semantic maps of terms (Ma et al., 200 0);  \\n \\n• Creation of language-specific seman tic dictionaries (Ogura et al., 2000); \\n \\n• Discourse an alysis (Jie  & Akahori, 2000); \\n \\n• Lexical analysis (Ide, 2000 ; Lee et al., 1999);  \\n \\n• Part-of-speech taggin g (Isahara et al., 2000; Marquez et al., 2000) \\n \\n• Query  translation (Kwok et al.,  2000) \\n \\n• Transliteratio n of foreign words for inf ormation retrieval (Jeong, et al., 1999) \\n \\nWeigard & Hoppenbr ouwers (1998) rep ort the way  an English/S panish lexicon,  includin g an \\nontology , is constructed for  NLP tasks in an ESPRIT project called TREVI.  E mphasizing the \\npoint t hat the re has not been an y stud y  of natu ral language information retrieval in Swedish, \\nHedlund et al. (2001) describe the features of Swedish language and point out a number of \\nresea rch proble ms. They  further stress th at separat e research in NL P in Swedish is required \\nbecause the r esear ch result s and tools for other langua ges do not  quite appl y to Swedish becau se \\nof the uni que features of the language.  \\n \\nCommenting on the pr ogress of MT research, Jurafsk y & Martin (2 000; p. 825) c omment that \\n“machine translation s ystem  design is hard wo rk, req uiring careful selection of m odels and \\nalgorithm s and com bination into a useful sy stem .” Th ey further comment that “ despite half a \\ncentury  of research, machine translation is fa r from solved; hum an language is a rich and \\nfascinating ar ea whose tre asures have only begun to be explored”.  \\n \\n \\n \\nEvaluation \\n \\nEvaluation is an im portant area in any  system development activity , and inform ation science \\nresearchers h ave long been struggling to come up with appropriate evaluation mechanisms for \\n 23 large-sc ale information sy stems. Conseq uently , NLP resear chers ha ve also been try ing to develop \\nreliable methods for evaluating robust NLP sy stems.  However, a  single set of evaluation criteria \\nwill not be applicable for all NLP tasks. Differe nt eval uation parameters may be required for each \\ntask, such as IE and automatic abstra cting which ar e significantly  different in nature co mpared to \\nsome other N LP tasks such as MT, CLIT or natural language user interfac es.  \\n \\nThe ELSE (Evaluation in Language and Speech Engineering) proj ect under the  contract from  the \\nEuropean Commi ssion aimed to study  the possible implementation of com parative evaluation in \\nNLP sy stems. Com parative evaluation in  Langua ge En gineering has been used since 1984 as a \\nbasic paradigm  in the DARPA research  program  in the US on h uman language technolog y since \\n1984 . Com parative evaluation consists o f a set of participants that com pare the results of their \\nsystems using sim ilar task s and related data with  metrics that were agreed upon.  Usually  this \\nevaluation is performed in a num ber of s uccessive evaluation ca mpaigns with more co mplex task \\nto perform  at every  cam paign. ELSE proposition departs fro m the DARPA research program in \\ntwo way s: first by consider ing usabilit y criteria in the evaluation, and second by trading \\ncompetitive aspects for more contrastive and collabor ative ones through the use of \\nmultidimensional results (Paroubek & Blasband, 1999). The ELS E consortium has identified the \\nfollowing fi ve types of eva luation (Paro ubek & Blasband, 19 99):  \\n• Basic r esearch evaluation : tries to validate a new idea or to assess t he am ount of \\nimprovem ent it brings o ver older m ethods.  \\n• Technolog y evaluation:  tries to asses s the perfor mance and appropriateness of a \\ntechnolog y for solving a problem  that is well-defined, sim plified and abstracted.  \\n• Usage evaluation: tries to a ssess the usability  of a technology for solving a real problem  \\nin the field. It  invol ves the end-users in the environm ent intended f or the deplo yment of \\nthe sy stem  under test.  \\n• Impact evalu ation: tries to measure the socio-econom ic consequences of a technology .  \\n• Program  evaluation:  attempts to determ ine how worth while a fundi ng pro gram has been \\nfor a given te chnolog y.  \\n \\nEAGLES (The Expert Advisory Group on La nguage E ngineering St andards – Evaluation \\nWorkgroup) ( Centre for .., 2000), phase one (EAGLES-I: 1993—1995) and pha se two \\n 24 (EAGLES-II:1997—1998) , is an Europe an Initiative  t hat proposed a user-centred evaluation of \\nNLP sy stems.   The EAGLES work takes as its starting point an exist ing Standard,  viz.  ISO 9126, \\nwhich is concerned primari ly with the de finiti on of quality  characteristics to be used in the \\nevaluation of software products. \\n \\nThe DiET project (1997-1999) was designed to  develop data, m ethods and t ools for the glass-box \\nevaluation of NLP co mponents, buildi ng on the r esults  of previo us projects covering different \\naspects of assessment and evaluation. T he webpage of the DiET project (DiET, 1997) sa ys that \\nthe project “will extend and develop test-suit es with annotated test item s for grammar, \\nmorphology  and discourse, for English,  French  and Germ an. DiET will provide user-support i n \\nterms of database technology , test-suite constr uction t ools and graphic interfaces.”, and that it \\n“will result in a tool-package for in-house and ex ternal quality  assurance and evaluation, which \\nwill enable the co mmerci al user to assess and com pare Language Technology pr oducts”. \\n \\nMUC, the Message Understanding Conference s, which have now ceased, was t he pioneer in \\nopening an international platfo rm for sharing researc h on NLP s ystems. In particular, MUC \\nresearchers were involved i n the evaluation of IE s ystems applied to a co mmon task.   The first \\nfive MUCs had focused on analy zing free text, iden tifying events of a specified t ype, and filli ng a \\ndata base te mplate with information about each  such event (MUC- 6, 1996). After MUC-5, a \\nbroad set of o bjectives was defined for t he fort hcom ing MUCs, such as:  to pus h inform ation \\nextraction sy stems towards greater portabilit y to new domains, and to encourage evaluations of \\nsome basic language anal ysis technologi es. In MUC-7 (the last MUC), the m ultilingual NE \\n(nam ed entities) evaluation was run using training and test articles from  comparable dom ains for \\nall languages (Chinchor, n .d.). The pape rs in the MUC-7 conference  report some interestin g \\nobservations by system  developers who were non- native speakers of the langua ge of their s ystem  \\nand sy stem  developers who were native speakers of the language o f their sy stem. Results of \\nMUC-3 throu gh MUC-7 ha ve been summarized by Chinchor (n .d.).  \\n \\n \\nConclusion \\n \\nResults of some NLP experim ents reported in this  paper show encouraging  results. However, one \\nshould not forget that m ost of these experi mental systems end in the lab; ver y few experimental  \\nsystems are converted to re al systems or products. On e of the major stum bling blocks  of NLP \\n 25 resea rch, as i n areas like information retrieval research, has been the absence of la rge test \\ncollections and re-usable experimental methods a nd tools. Fort unately, the situation has chan ged \\nover the past few y ears. Several national and internati onal research groups are now  working \\ntogether to build and re-use large test collections  and experimental tools and tech niques. Since the \\norigin of the Message Understanding Conferences, group research efforts have proliferated wi th \\nthe regular conferences and  workshops, f or exam ple,  the TREC seri es and other conferences \\norganized by  NAACL ( North Americ an Chapter of the Associ ation for Com putational \\nLinguistics), EACL (European ACL), and so on .  These group research efforts help research ers \\nshare their expertise by  building re-usable N LP tools, t est collections, and experimental \\nmethodologie s.  References to som e re-usable N LP tools and coope rative research grou ps hav e \\nbeen made earlier in this pa per (see under the heading Some Theoretical Developments ). \\n \\nSome recent studies on ev aluation also show prom ising results.  Very  small-sc ale evaluation of \\nINFOS suggests that the indexing pattern m ethod, i.e., mapping of the words from  the input text \\ninto the corre ct concepts in th e WordNet abstraction hierarchy , correctly  classifie d 80% of the \\narticles (Mock and Vem uri, 199 7). Som e large-scale experi ments with NLP als o show \\nencouraging r esults. For exam ple, Kwok  et al. (2000 ,1999) report  that their PIRCS sy stem  can \\nperform  the t asks of English-Chinese query  translation with an effectiveness of over 80%. \\nStrzalkowski et al. (TREC-8;199 8) repor t that b y using the algorith m of auto matic expansion of \\nqueries, using  NLP techniques, they obta ined a 37%  improvement of average precision over a \\nbaseline wher e no expansion was used. There are c onflicting results too. For  example, Elworth y \\n(2000) rep orts that the NLP sy stem , using the Micros oft prod uct NLPWin, perfo rmed much \\npoorer in t he TREC-9 test set co mpared with the TREC-8 test set. While tr ying to find out the \\nreasons for this discrepancy, Elworth y (2000) co mments that an important challenge for the \\nfuture work may be looki ng at how t o build a system  that merges definitive, pre-encoded \\nknowledge, a nd ad-hoc docu ments of unknown relia bility.  \\n \\nAs alre ady mentioned earlier (in the sect ion on  Abstracting), Craven’ s study  with TEXNET \\n(Craven, 199 6) shows a limited success (onl y 37%).  Gaizauska s and Wilks m ention that the  \\nperformance l evels of  the common IE syste ms, stand in the range of   50%  for co mbined recall \\nand precision. Such low succes s rate s are not accep table in  large-scale operatio nal inform ation \\nsystems.   \\n \\n 26 Smith (1998)  suggests that there are two possibl e scenarios  for the future relations between \\ncomputers an d hum ans: (1) in the user-friendlin ess scenario, com puters beco me smart enough  to \\ncommunicat e in natural language, and (2) in th e com puter friendliness s cenario humans adapt \\ntheir practices in order to c ommunicate with, and make use of, com puter s. He f urther argues that \\nthe use of com puter-fri endly encoding of natural language texts on the web is sym ptomatic of  a \\nrevolutionar y trend toward the com puterizati on of hum an knowledge .  Petreley  (2000, p.102) \\nraises a very  pertinent que stion about  natural la nguage user interfaces: “will the natural language \\ninterface hav e to wait until voice recognition b ecom es more commonplace?”. This statement \\nappears to be quite legitim ate when we see that  although a large num ber of natural language user \\ninterfaces were built, m ost at the laboratory  level,  and a few at the commer cial level (for details \\nof these see, Haas, 1996; Chowdhur y, 1999b, Chap ters 18-21), na tural language user interfaces \\nare not still very  comm on. The im pediments to pr ogress to the natur al language interfaces lie on \\nseveral planes including   language issues. Zadrozn y et al. (200 0) mention that ex cept for very \\nrestricted domains, we do not know how to compute the meaning of a sentence based on \\nmeanings of its words and its context. Anothe r problem  is caus ed by the lack of precise user \\nmodels. Zadrozny  et al. (2000) m aintain that even assum ing that we can have any piece of \\ninform ation about a person , we do not know how  could we use this knowledge t o make this \\nperson' s inter action with a dialogue s ystem most effective and pleasant. \\n \\nMT invol ves a num ber of  difficult probl ems, mainly  because hu man language i s at times quite \\nambiguous an d full of  special constructions, and excep tions to rules.   Despite that there has been \\na steady  developm ent, and MT resear ch has now reac hed a stage w here the benefits can be \\nenjoyed by people. A number of web search tools, vi z. Altavista, Google, L ycos and AOL offer \\nfree MT facil ities of web inform ation re sources. A num ber of co mpanies also provide MT \\nservices commerci ally. For exam ple, the IBM WebS phere Translation Server for Multiplatforms \\nis a machine translation ser vice available  comme rciall y for translating web documents in a \\nnumber of languages, such as English, French, It alian, Spanish, C hinese, Japanese and Korean.  \\nIn June 2001, Autodesk, a US software company began to offer MT services to its European \\ncusto mers at a cost which is 50% less co mpared to the hum an transl ation services  (Schenker, \\n2001).   Tho ugh  m achine translations are not alway s perfect and do not produce as good \\ntranslations a s hum an trans lators would produ ce, the results, and evidences of interests in \\nimproving t he perform ance level of MT sy stem s, are very enco uraging.  \\n 27 One area of a pplication of NLP that has drawn much resea rch attent ion, but where the results a re \\nyet to reach the general public with an acceptable level of performance, is the  natural language  \\nquestion-answering sy stem . While so me sy stems, as reported in this chapter, produce accept able \\nresults, there are still many failures and surprises.  Results of s ystems reported un der the QA track \\nof TREC (reported under t he heading of  natural langu age interfaces in this paper)  show prom ising \\nresults with some si mple type of natural  langua ge que ries. However,  these sy stems ar e still at  \\nexperi mental stages, and much resear ch is needed  before robust QA sy stems can be built that are \\ncapable of accepting user queries in any form  of natural language and pro ducin g natural lang uage \\nanswers r etrieved form  a num ber of distributed inform ation resources.  Scalabilit y and portabil ity \\nare the main challenges facing natural l anguage  text processing resear ch. Adams (2001) argues \\nthat current NLP sy stem s establish patterns th at are valid for a specific domain and for a \\nparticular tas k only; as soon as the topic, context or the user  changes, entirely  new patterns need \\nto be established. Sparck Jones (1999)  rightly warns t hat advanced NLP techniqu es such as \\nconcept extraction, are too expensive for larg e-scal e NLP applications. The resear ch co mmunity, \\nhowever, is making conti nuous efforts. The reason for not having reliable NLP systems that work \\nat a high level of perform ance with high  degr ee of sophistication may largely b e, not the \\ninefficiency  of the sy stem s or resear chers, but  the complexities and idios yncrasies of hum an \\nbehaviour an d comm unication patterns.  \\n \\nRefe renc es \\nAdam s, K.C. (2001). The Web as a database: New ex traction technologies & content \\nmanagement,  Online ; 25, 27-32 \\nAhonen, H.; Heinonen, O.; Klem ettinen , M. & Verkam o, A.I. (1998). Appl ying data mining \\ntechniques for descriptive phrase extract ion in digital docum ent collections.    IEEE International \\nForum on Re search and Technology. A dvances in Digital Librarie s - ADL'98,   22-24 April 1998,  \\nSanta Barbara, CA.  Los      Alam itos, CA: IEEE Com puter Societ y,  pp. 2-11  \\n \\nAmsler, R.A.(1984).  Machine-readable dictionaries. In: M. E. Will iams, (ed.) Annual  Review of \\nInformation Science and Technology (AR IST: Volum e 19, White Pla ins, NY: Knowledge \\nIndustr y Publications Inc. for the American Society  for Inform ation  Science. pp.161-2 09.  \\nArgam on, S.; Dagan, I. & Kry molowski, Y. (199 8). A memory-based approach to learning \\nshallow natural language p atterns. In 17t h Intern ationa l Conference on Com putational Ling uistics \\n(COLING '98), August 1 0-14, 1 998, Uni versité  de Montréal, Montr éal, Québec, Canada , \\nMontreal: ACL.  pp.  67-7 3. \\nBangalore, S. & Joshi, A.K. (199 9). Su pertagging: an approach to alm ost parsing. Computatio nal \\nLinguistics , 25, 237-265. \\n 28 Barahona, P.& Alfer es, J.J. (Eds.). (1999).  Progress i n Artificial Intelligence. 9th Portuguese \\nConference o n Artificial Intelligence, EPIA'99. Proceedings , 21-2 4 Sept. 1 999    Evora, Port ugal.  \\nBerlin: Springer-Verlag. \\nBarker, K.& Cornacchia, N. (2000).  Using noun phrase heads to extract document ke yphrases In: \\nH.J. Hamilton (Ed.) Advances in Artifici al Intelligenc e. Proceedings of 13th Biennial Conference \\nof the Canadi an Society for Computati onal  Studies of Intelligence, AI 2000 . 14-17 May 20 00,  \\nMontreal,    B erlin: Springe r-Verlag.  pp. 40-52   \\n \\nBenoit, G. (2 001) Data m ining. I n: Cro nin, B. (ed.).  Annual Revi ew of Information Science and \\nTech nology (ARI ST): Volume 36 . Med ford, NJ: Inform ation toda y for ASIS, pp.  \\nBian, Guo-Wei & Chen, Hsin-Hsi (2000). Cross-language inform ation access to multilingual \\ncollections on the Internet . Journal of the American S ociety for Information Scie nce, 51, 2 81-296.  \\nBlack, W.J.; Rinaldi, F. &  McNaught, J. (2000). Natural language processing in Java: \\napplications in education and knowledg e managemen t. Proceedings of the Second International \\nConference o n the Practical Application of Java.  12-14 April 2000,     Manche ster.  Practical \\nApplication Company: Blackpool.     pp. 157-70  \\n \\nBondale, N.; Maloor, P.; V aidyanathan , A.; Sengupta,  S. &  Rao, P. V.S. (1999) . Extraction of \\ninform ation from  open-ended questionna ires usi ng natural language processing techniques.  \\nComputer Science and Info rmatics ,    29,  15-22 . \\n \\nBorgman, C.L. (1997). M ulti-Media, Multi-Cultura l, and Multi- Lingual Digit al Libraries: Or \\nHow Do We Exchange Data In 400  Languages? D-Lib M agazine . [Online ] Avail able \\nhttp://www.dlib.org/dlib/j une97/06borgman.ht ml \\nBreck, E.; Burger, J.; House, D.; Light, M. & Mani, I. (1999) Question answering from  large \\ndocum ent collections.  Question Answering Systems. Papers from the 1999 AAAI Fall \\nSymposium,     5-7 Nov. 1999,     North Falmouth, MA.  Menlo Park, CA:  AAAI Press.  pp. 26-\\n31   \\n \\nCarrick, C. and Watters, C. (199 7). Au tomatic associ ation of news item s. Information Processing \\n& Management , 33, 615-632. \\nCentre for Language Technology ( 2000). EAG LES-ll Inform ation Page:  Evaluation of NLP \\nSystem s . [Online]  Available:  http://w ww.cst.ku.dk/projects/eagl es2.htm l \\nCeric, V. (2000). Advancements and trends in th e World Wide Web search. In: D.  Kalpic & V.H. \\nDobric (Eds.).  Proceedings of the 22nd International Conference o n Information Technology \\nInterfaces,  13-16 June 2000,    P ula, Croatia. SRCE University Com puter Centre, Univ. Zagr eb, \\npp. 2 11-20  \\n \\nChandrasekar , R. & Srinivas, B. (1998). Glean: usi ng syntactic infor mation in docu ment filteri ng. \\nInformation Processing & Management , 34, 623-640 \\nCharniak, E. (1995).  Natural language learni ng. ACM Computing S urveys, 27,  317-33 19. \\n 29 Chen, J.N. & Chang, J.S. ( 1998). Topic al clustering of MRD senses based on inform ation \\nretrieval tech niques. Comp utatio nal Li nguistics , 24, 61-96.  \\nChinchor,  N.  A. Overview of MUC-7/MET-2. [ Online]  Available: \\nhttp://www.itl.nist.gov/iaui /894.02/related_project s/muc/proceedings/ muc_7_pr oceedings/overvie\\nw.htm l \\nChowdhur y, G. G. (1999a) . Tem plate mining f or infor mation extraction from  digital docum ents. \\nLibrary Trends , 48, 182-208.  \\nChowdhur y, G.G. (1999b) . Introductio n to modern in formation retrieval . Londo n: Librar y \\nAssociation Publishing .  \\nChuang, W. & Yang, J. (2000).  E xtracting sente nce seg ments for text summarization: a machine \\nlearning appr oach. In:  Proceedings of the 23rd annual internation al ACM SIGIR \\nconfe rence on Research and  developm ent in info rmation retrie val, ACM, pp. \\n152-159.  \\n \\nCostantino, M . (1999). Natural language  processi ng and expert system techniques for equity derivatives \\ntrading: the IE-Exp ert system . In: D. Kalp ic & V. H.  Dobric (Eds). Proceedings of the 21st International \\nConference on  Information Technology Interfaces, Pula, Cr oatia,  15- 18 June, 1999.  Univ. Za greb , Zagreb, \\nCroatia,    pp. 63-9  \\n  \\nCowie, J. & Lehnert, W. ( 1996).  Infor mation extraction. Commu nications of  the ACM , 39, 80 – \\n91  \\nCraven, T. C. (2000) . Abstracts produced using com puter assistan ce. Journal of the American  \\nSociety for Information Sci ence,  51, 74 5-756  \\nCraven, T.C. (1988).  Text network displ ay editi ng wit h special reference to the production  of  \\ncusto mized a bstracts. Canadian  Journa l of Informati on Science , 13, 59-68. \\nCraven, T.C. (1996). An experi ment in the use of  tools for com puter-assist ed abstracting. In: \\nASIS’ 96: Proceedings of the 59th ASIS Annual Meeting 1 996. Baltim ore, MD, October 21-2 4, \\n1996 . Vol. 3 3, Medford, N J: Information Toda y, pp. 203-2 08.  \\nCraven, T.C. (1993).  A com puter-aided abstracting tool kit. Canadian Journa l of Information \\nScience , 18, 1 9-31. \\nDan-Hee,  Y.; Gomez, P.C. & Song, M.   (2000) . An algorithm  for predicting the  relationship \\nbetween le mmas and corpus size. ETRI Journal,  22,  20-31  \\n \\nDiET: Diagnostoc and Ev aluation Tool s for natu ral language appli cations (1997 ). [Online]  \\nAvailable: http://www.dfki.de/lt/projects/diet-e.htm l \\nDogru, S.&  Slagle, J.R.(1999).  Implementing a semantic lexicon.   In: W. Tepfen hart & W.  Cy re \\n(Eds.) Conceptual Structures: Standards and Prac tices. 7th International Confe rence on \\nConceptual Structures, IC CS'99  Proceedings,  12-15 July 1999,     Blacksburg, VA.    Berlin:  \\nSpringer-Verlag  pp. 154-67  \\n \\n 30 Elworthy, D. (2000). Question answering usi ng a large NLP system . The Ninth Text \\nREtrievalConference (T REC 9)   [Online]  Availa ble: \\nhttp://trec.nist.gov/pubs/tr ec9/papers/m src-qa.pdf \\nEvans, M. (1989). Com puter-readable Dictionaries.  . In: M.E. Willi ams (Ed). An nual Review of \\nInform ation Science and Technolog y (ARIST): Volume 24. Am sterdam , The Netherlands: \\nElsevier Science Publishers B.V. for the Am erican Society  for I nformation Science. 85-117.   \\nFellbaum , C. (ed.) (1998).  WordNet : an electronic le xical databas e. Cambridge , Mass : MIT \\nPress \\nFeldman, S. (1999).  NLP meets the jabberwocky . Online , 23, 62-72.  \\nFernandez, P.M. & Garci a-Serrano, A.M. (200 0). The role of knowledge-based technology in \\nlanguage appl ications developm ent. Expe rt System s with Applications 19, 31- 44  \\nGaizauskas, R. & Wilks, Y. (1998).  Information extraction: be yond docum ent retrieval. Journal \\nof Documentation , 54, 70- 105.  \\nGlasgow, B.; Mandell, A.; Binney, D.; Ghem ri, L. & Fisher, D. (1998). MITA: an inform ation-\\nextraction approach to the analy sis of free -form  text i n life insurance applications.  AI M agazine,    \\n 19,     59-71   \\n  \\nGlobal Reach  (2001) . Global Internet Statis tics (by  language). [ Online] . Availabl e: \\nhttp://www.eurom ktg.co m/globstats/ \\nGoldstein, J.; Kantrowitz, M.; Mittal, V. & Ca rbonell, J.  (1999). S ummarizing text docum ents: \\nsentence s election and eva luation m etrics. In: Proceeding of the 22nd Annual I nternational \\nConference on Resear ch and Development in In form ation Retrieval. ACM, pp. 121-128.  \\nGrefenstette, G. (1999).  The World Wid e Web as a resource for exam ple-based machine \\ntranslation tasks.  Translati ng and the C omputer 21. Proceedings of the Twenty-first International \\nConference o n Translating and t he Computer     10- 11 Nov. 19 99,     Lond on:  As lib/IMI , p p. 12  \\nGrish man, R.  & Kittredge, R. (Eds.) (1986). Analyzing language in restrict ed domains: \\nsublanguage descriptions and processi ng. London: Lawrenc e Erlbaum  Associ ates  \\nHaas, S. W. ( 1996). Natural language pr ocessing: toward large-scal e robust s ystems. In: M.E. \\nWilliam s (Ed.). Annual Review of Infor mation Science and Technology (ARIST): Volum e 31. \\nMedford, NJ: Learned Inform ation Inc. for the Am erican Society  for Inform ation  Science. pp. 83-\\n119.  \\nHayes, P. (1992)  Intellig ent high-volume text processing using sha llow, domain-specific techniques. \\nIn: J acobs, P. S., (ed. ). Text-based intellig ent systems , Hillsda le, NJ, Lawrence Erlbaum, pp. 227-241. \\nHayes,  P. & Weinstein,  S. (1991). Cons true-TIS : a system for c ontent-based inde xing of a database of \\nnews stories. I n: Rapp aport, A. & S mith, R. (eds.), Innova tive ap plications of artificial intellig ence 2, \\nCam bridge, MA , MIT Press, pp. 51-64. \\nHedlund, T.; Pirkola, A. & Jarvelin, K. (2001).  Aspects of Swedish m orpholog y and sem antics \\nfrom  the perspectives of mono- and  cross-language information retrieval. Information Processing \\n& Management , 37, 147-161.  \\nHeng-Hsou Chang; Yau-Hwang Ko & Jang-Pong Hs u (2000).  An  event-driven and ont ology-\\nbased approach for the delivery  and infor mation extraction of e-m ails. Proceedings International \\n 31 Symposium on Multimedia Software Engineering,     11-13 Dec. 2 000,  Taipei, Taiwan.  Los \\nAlam itos, CA: IEEE Com puter Society , pp. 103-9      \\n \\nHerath, S. & Herath, A. (1999). Alg orithm to de termine the subject in flexible w ord order \\nlanguage based machine tr anslations: a case study for  Sinhalese. Communicatio ns of COLIPS, 9,  \\n1-17  \\n \\nIde, N (2000). Cross-ling ual sense determination: can it wo rk? Computers a nd the Human ities,     34,  223-\\n34  \\n \\nIsahara, H.; Ma, Q.; Sornlertla mvanich,  V. & Ta kahashi, N. (200 0). ORCHID: b uilding lingui stic \\nresources in Thai.  Literary & Linguistic Computing, 15,  465-7 8    \\n \\nJelinek, F. (1999). Statistic al Methods fo r Speech Recogniti on (Language, Speech, and \\nCommunication). MIT Pre ss. \\nJeong, K.S.; Mayeng, S.H. ; Lee, J.S.; Choi, K.S.(1 999). Autom atic identification  and back-\\ntransliteration of foreign w ords for inf ormation retrieval. Informati on Processing &  Management , \\n35, 5 23-540. \\nJie Chi Yang & Akahori, K. (2000). A discourse struct ure analy sis of technical Ja panese texts a nd \\nits im plementation on the WWW. Computer Assisted Lan guage Learning,  13, 119-4 1  \\n \\nJin, Song and Dong-Yan,  Zhao (2000). Study  of automatic abstr acting based on corpus and \\nhierarchical d ictionary ,   Journal of Software, 11, 30 8-14  \\n \\nJurafsky , D. & Martin, J.H. (2000).  Speech and language processing: an introduction t o natural \\nlanguage processing, computati onal linguistics and s peech recognition . Upper Saddle River, NJ: \\nPrentice Hall.   \\nKam -Fai Wong; Lum , V.Y.&  Wai-Ip Lam  (1998). Chicon-a Chinese text manipulation language.  \\nSoftware - Practice and Experience, 28,  681-7 01  \\n \\nKazakov, D.; Manandhar, S. &  Erjavec, T. (19 99). Learning word segmentation rules for tag \\nprediction.  I n: S. Dzeroski, S. & P.  Fla ch (Eds.) Inductive Logic Programming. 9th  \\nInternational Workshop, ILP-99 Proceedings,  24-27 June 199 , Bled , Slovenia. B erlin:  Sprin ger-\\nVerlag , pp. 1 52-16 1  \\nKehler, A. (1997). Current  theories of centering fo r pronoun interpretation: a critical evaluation. \\nComputation al Ling uistics , 23, 467-475. \\nKhoo, C.S.G;  Myaeng, S.H  & Oddy, R. N (2001). Usi ng cause-effect relations in text to im prove \\ninform ation retrieval preci sion. Information Processing &  Management , 37, 1 19-145 \\nKim, T.; Si m, C.; Sanghwa, Y. & Jung, H. (1999). Fr om to-CLIR: web-based natural language \\ninterface for cross-language information retrieval. Information Processing &  Management , 35, \\n559-5 86 \\n 32 King, M. (1996). Evaluating natu ral language processing s ystems. Communications of t he ACM , \\n39, 7 3-80  \\nKornai, A. (ed.) (19 99). E xtended Finit e State M odels of  Languag e (Studies in Natural Language \\nProcessing), Ca mbridge  University  Press. \\n \\nKwok, K.L; Grunfeld, L.; Dinstl, N. & Chan, M. (20 00). TREC-9 cross language, web and \\nquestion-answering track experim ents using PIRCS.  The Ninth Te xt REtrieval Conference ( TREC \\n9). [Online]  Available:  http://trec.nist.gov/pubs/trec9/t9_proceedings.htm l \\nKwok, K.L.; Grunfield, L. & Chen, M. (1999).  TREC -8 Ad-hoc, query  filtering t rack experiments \\nusing PIRCS.  The Eighth  text retrieval Conference (TREC-8). [ Online]  Available:  \\nhttp://trec.nist .gov/ pubs/tre c8/papers/queenst8.pdf \\nLange, H. (1993). Speech Synthesis and Speech  Reco gnition: Tom orrow’ s Human-Co mputer \\nInterfaces? In: M.E. Willia ms (Ed.). Annual Review of Information Science and Technology  \\n(ARIST): Vo lume 28. Med ford, NJ: Learned Info rmation Inc. for the A merican S ociety  for \\nInform ation Science. pp.15 3-185 \\nLee, K.H; Ng , M.K.M &  Lu, Q. (1999). Te xt seg mentation for Chinese spell c hecking. Journal \\nof the Americ an Society for Information Science , 50, 7 51-75 9.  \\nLehmam, A. (1999).  Text structuration l eading to an a utomatic summary  system : RAFI. \\nInformation Processing & Management ,  35,  181-191  \\nLehtokangas, R. & Jarveli n, K. (2001). Consiste ncy of textual expression in newspaper artic les: \\nan argument for se mantically base query  expansion. Journal of Doc umentation , 57, 535-548  \\nLewis, D.D. & Sparck Jones, K. (1996). Natura l language processi ng for i nformation retrieval. \\nCommunications of the AC M, 39(1), 9 2 – 101   \\nLiddy , E. (1998). Enhanced text retr ieval using nat ural language pr ocessing. Bulletin of the \\nAmerican Society for Information Scien ce, 24, 14- 16.  \\nLiddy, E.; Diamond, T . & McKenna, M (2000). DR-LINK in TIPSTER  III.  Information Retrieva l,     3,  \\n291-311   \\n \\nLovis, C.; Baud , R.; Rassino ux, A.M.; Michel, P.A .& Sc herter, J.R. (1 998). Medical dictionaries for patient \\nencoding system s: a methodology.  Artificia l Intellig ence in  Medicine,  14, 201—214.  \\n \\nMa, Q.; Kanzaki, K.; Murata, M.; Uti yama, M.; Uchi moto, K. &  I sahara, H. Sel f-organizing \\nsemantic maps of Japanese nouns i n terms of adnom inal constituents. In: S. Herath & A. Herat h, \\n(Eds.) Proceedings of the IEEE-INNS-ENNS Interna tional Joi nt Conference on Neural Networks. \\nIJCNN 2000. Neural Computing: New Challe nges and Perspecti ves for the New Millennium . 24-\\n27 Jul y 2000.     Com o,  Italy.  Los Alam itos, CA: IEEE Com put. Soc , ,     pp. 91-96      \\n \\nMagnini, B.; Not, E.; Stoc k, O. & Strappara va, C. (2000). Natural language processing for \\ntransparent c ommunication between pu blic adm inistration and citizens. Artificial Intelligenc e and \\nLaw,    8, 1-34  \\nMani, I.  &  Maybury, M.T. (199 9). Advances in automatic text summarization . Cam bridge, MA: \\nMIT Press  \\n 33 Manning, C. D. & Schutze, H. (199 9). Foundations of statistical natural language processing . \\nCambridge, MA: MIT Press \\nMarquez, L.; Padro, L. &  Rodriguez, H. (200 0). A machine learning approach t o POS taggin g  \\nMachine Learning ,    39, 59-91  \\nMartinez, P.; de Miguel, A.; Cuadra, D.; Nieto,  C. & Castro, E. (2000).  Data conceptual \\nmodelling through natural l anguage: identification and validation of  relationship cardinalities.   \\nChallenges of  Information Technology Manageme nt in the 21st Ce ntury. 200 0 Information \\nResources Management A ssociation Int ernational C onference,     21-24 Ma y 2000,     Anch orage, \\nAK. Hershey , PA: Idea Group Pu blishing . pp. 500-504  \\n \\nMartinez, P. &  Garci a-Serrano, A. (1998) .   A know ledge-based methodology applied to \\nlinguistic eng ineering .  In:  R.N. Horspool (Ed.)  S ystems Implementation 2 000. IFIP TC2 \\nWG2.4 Work ing Conferen ce on S ystems Im plementa tion 2 000: Languages, Methods and T ools,  \\n   23-2 6 Feb. 1998 ,     Berli n. Lon don: Chapman & Hall  pp. 1 66-179      \\nMcMurchie, L.L (1998) Software speak s user’ s language. Computing Can ada, 24, 19-21. \\nMeyer , J.& Dale, R. (1999). Building hybrid knowledge represent ations from text. In: Edwards, J. \\n(ed.),  Proceedings of the 23r d Austr alasian Computer  Science Confer ence. ACSC 2000, IEEE \\nComput . Soc , Lo s Alamit os, CA , pp. 158 -65 \\n  \\nMihalcea, R. & Moldovan, D.I. (1999). Automatic acquisition of sense tagged corpora.  In:  A.N. \\nKumar & I. Russell (Eds.). Proceedings of the Twelfth Interna tional Florida AI Research Society \\nConference,      3-5 Ma y 1999,  Orlando , FL.   Menlo Park, CA: AAAI Press , pp. 29 3-7      \\nMock, K.J. &  Vem uri, V.R. (1997).  Informati on filtering via hi ll clim bing, wordnet and index \\npatterns. Information Processing & Management , 33, 633- 644. \\nMoens, Marie-Francine &  Uyttendaele, Caro line (199 7), Autom atic text structuring and \\ncategorizatio n as a first ste p in summ arizing legal cas es. Information Processing &  Management , \\n33,  727-7 37 \\nMorin, E. ( 1999). Autom atic acquisition of se mantic relations between terms from  technical \\ncorpora. In:  P. Sandrini(E d.). TKE'99. Terminology and Knowledge Engineering. Proceedings \\nFifth Internat ional Congress on Term inology and Knowledge Engi neering .     Innsbruck, Aust ria , \\n23-27 Au g. 1999.     Vienn a: TermNet   pp. 2 68-78  \\n \\nMUC-6 (1996). [Online ] Available : http:// www.cs. nyu.edu/cs/fa culty /grishm an/muc6.ht ml \\nMULINEX: Multilingual Indexing, Navigation and Editing E xtensions for the World Wide Web. \\n[Online] . Available: http:// mulinex.dfki.de/ \\nNarita, M.&  Ogawa, Y. (2 000). T he use of phrases from  query  texts in inform ation retrieval. \\nSIGIR Forum , 34, 318-20  \\nNatural Language Processi ng Laborator y, Universi ty of Massachusetts. [Online]  Available: \\nhttp://www-nlp.cs.umass. edu/nlplic.html \\nNDLTD: N etworked Digital Library  of Theses and D issertations. [Online ] Avail able:   \\nhttp://www.ndltd.org \\n 34 Nerbonne, J.; Dokter, D. & Sm it, P. (1998). Morphological Processi ng and Com puter-Assist ed \\nLanguage Learning.  Com puter Assisted Lan guage Learning , 11, 543-5 9  \\nOard, D. W. (1997).  Servin g users in m any lang uages: cross-language inform ation retrieval for \\ndigital librari es, D-Lib M agazine.  [Online]  Available:  \\nhttp://www.dlib.org/ dlib/decem ber97/oard/12oard.ht ml \\nOard, D. W & Diekama,  A.R. (1998). Cross-langua ge Inform ation Retrieval.  In: M.E. Willi ams \\n(Ed.). Annual Review of Information Scie nce and Tech nology (ARIS T): Volum e 33. Medford,  NJ: \\nLearned Information Inc. for the American So ciety  for Inform ation  Science. pp. 223-2 56   \\nOgura, K.; Nakaiwa, H.; Matsuo, Y.; Ooy ama, Y. &  Bond, F. ( 2000) T he electronic dictio nary. \\nGoi-Taikei-a Japanese l exicon and its applications. NTT Review, 12, 53- 8   \\nOudet, B. (1997). Multil ingualism  on the Internet. Scientific Ameri can, 276 (3), 77-78. \\nOwei, V. (2000) Natural language quer ying of databases: an information extraction appr oach in \\nthe conceptual query  language. International Jo urnal of Human-Co mputer Studie s, 53,  439-9 2 \\nParis, L.A.H. &  Tibbo, H.R. (1998). Fr eesty le vs. Boolean: a co mparison of par tial and exact \\nmatch retriev al systems. Information Pr ocessing & Management,  34, 175-90 \\nParoubek, P. &  Blasband, M. (199 9). Executive Summary  of a Blueprint for a General \\nInfrastructure for Natural Language Processing Sy stems Evaluation  \\nUsing Sem i-Automatic Quantitative Black B ox Approach in a Multilingual Environm ent.  \\n[Online]  Avai lable:  http://www.li msi.fr/TLP/ELSE/Pream bleXwh yXwhatXrev3.htm  \\nPasero, R. & Sabatier, P. (1998)  Lingui stic Ga mes fo r Language L earning: A Special Use of the \\nILLICO Library . Comp uter Assisted La ngua ge Learning , 11, 561 -85  \\n \\nPede rsen, T . &  Bruce, R . (1998).  Knowle dge lean  word-sense disambiguation.  Proceedings Fifteenth \\nNatio nal Conference on  Artifi cial In tellig ence (AAAI-98). Tenth Con ference on Innovative App licatio ns of \\nArtificial In tellig ence.     26-30 July 199 8, Madison. Men lo Park, CA: WI AAAI Press/MI T Press  pp. 800-\\n5  \\n \\nPerez-Ca rballo, J. &  Strzalkowski,  T. (20 00). Natural langu age info rmation  retrie val: p rogress report. \\nInformation Processi ng & Mana gement , 36, 155-178 \\nPeters, C. & Picchi, E. (1997).  Across Languages, Across Cultures: Issues in \\nMultilingu ality and Dig ital Libr aries,  D-Lib Magazine . [Online] Available: \\nhttp://www.dlib.org/dlib/m ay97/peters/05peters.htm l \\nPetreley , N. ( 2000).  Waiting for innovati ons to hi t the mainstream : What about natural language?  \\nInfoWorld, 22(4), 102 \\nPirkola, A. (2 001). M orphological t ypology of lan guag es for IR. Journal of Docu mentation , 57, \\n330-3 48  \\nPoesio, M. & Vieira, R. (1998). A corpus-based investigation of definite description use. \\nComputation al Ling uistics , 24, 183-216 \\n 35 Powell, J. &  Fox, E.A. (1998). Multilin gual federated searching across heterogeneous \\ncollections. D-Lib M agazin e. [Online]  Available: \\nhttp://www.dlib.org/ dlib/septem ber98/powell/09pow ell.htm l \\nQin, J. & Norton, M.J. (E ds.) (1999).  Introduction . Special Issue:  Knowledge discovery  in \\nbibliographic databases. Library Trends , 48, 1-8.  \\nRaghavan, V.V.; Deogun, J .S.; & Server, H. (Eds .) (19 98). Special t opical issue: Knowledge \\ndiscovery  and data mining. Journal of the American S ociety for Information Scie nce, 49(5). \\nRoche,  E. and Shabes, Y. (eds.) (1997). Fi nite-State  Language Pr ocessing (Language, Speech \\nand Comm unication),  MIT Press. \\n \\n \\nRosenfield, R. (2000). Two decades of st atisti cal language modeling: where do we go from  here? \\nProceedings of the IEEE. 88, 8, 1270-8. \\n \\nRoux, M.&  Ledoray , V. (2000)  Unders tanding of m edico-technical reports.  Artificial \\nIntelligence in Medicine,  18, 14 9-72  \\n \\nRuiz, M.E. &  Srinivasan, P. (1998).  Cr oss-La nguage Inform ation Retrieval: an analy sis of \\nerrors. Proceedings of t he 61st ASIS Annual Meeting , Pittsburgh, P A, October 25-29, pp.153- 65 \\nSay, B (1999). Modeling cue phrases in Turkish: a case study .  In: V. Matousek, V. et al (Eds.). \\nText, Speech and Dial ogue . Second Inter national Workshop, TDS'99 Proceedings , 13-17 Sept.  \\n1999,     Plze n, Czech Rep ublic.  Berlin:   Springer-Verlag  pp. 337-40      \\nScarlett, E.; & Szpakowicz, S (2000) .  The pow er of the TSNLP: lessons fro m a diagnostic \\nevaluation of a broad-coverage pa rser.  I n: H.J. Ha milton (Ed.) Advances in Artificial \\nIntelligence. 13th Biennial Conference of the Ca nadian Society for Computational Studies of \\nIntelligence, AI 2000 Proceedings, 14-17 May 20 00, Montreal. Berlin: Sprin ger-Verlag  pp. 1 38-\\n50  \\n \\nSchenker, J.L. (2001). The gist of translation: how long will it be be fore machines make the web \\nmultilingual? Time , 158, Ju ly 16,  2001, 54.  \\nScott, J. (199 9). E-m ail Managem ent: the key to reg aining contr ol.  Internet Business, De cember \\n1999 ,     60— 65   \\nSilber, H.G.& McCoy , K.F. (2000) Effi cient text summarization using lexical chains   In: H.  \\nLieber man(Ed.).  Proceedings of  IUI 2000 Inter national Conference on Intelligent User \\nInterfaces,   9-12 Jan. 20 00, New Orleans, LA. New York: ACM pp . 252- 5    \\nSmeaton A.F. (1999) . Using NLP or NLP Resources f or Inform ation Retrieval Tasks. In: T. \\nStrzalkowski (Ed.), Natura l Lang uage I nformation R etrieval , Klu wer Academ ic Publishers, 99-\\n111,  \\nSmeaton, A.F. (1997).  Inf ormation retrie val: still butti ng heads with natural language \\nprocessing?  In: M.T. Pazienza (Ed.). Information Ext raction. A Multidisciplinar y Approach t o an \\nEmerging Inf ormation Technolo gy Internation al Summer School, S CIE-97 ,   14-18 Jul y 1997, \\n 36 Frascati, Italy . Berlin: Springer-Verlag pp. 115-38  \\n \\nSmith, D. (19 98). Com puterizing Com puter Science. Communicatio ns of the AC M, 41, 21- 23  \\nSokol, L .; Murphy , K.; Brooks, W.& Mattox, D. (2 000).   Visualizing text-based data mining  \\nProceedings of the Fourth International Confer ence on the Practica l Application of Knowledge \\nDiscovery  and Data Mining, 11- 13 Apri l 200 0, Manc hester.  Black pool: Practical Application \\nCompany,  pp. 57-61  \\nSong Jin &  Zhao Dong-Y an (2000) . Study of automatic abstracting based on corpus and \\nhierarchical d ictionary. Journal of S oftware,11, 30 8-14 \\nSparck Jones, K. (1999) . What is the role for NLP in te xt retrieval. In T. Strzalko wski (Ed.). \\nNatural lan guage inf ormation retrieval . Kluwer, pp. 1—25. \\nStaab, S.; Braun, C.; Brude r, I.; Dusterhoft, A.; Heuer, A.; Klettke, M.; Neu mann, G.; Prager, B.; \\nPretzel, J.; Schnurr, H.-P.; Studer, R.; Uszkoreit, H.& Wrenger, B. (1999) GETE SS-searching the \\nWeb exploiti ng Germ an texts.  Cooperative Information Agents  III. Third International \\nWorkshop, CIA'99 Proceedings,  31 July-2 Aug. 1 999,     Upp sala, Sweden.  Ber lin: Springer-\\nVerlag  pp. 1 13-24  \\n \\nStock, O. (20 00). Natural language proc essing and int elligent interfaces. Annals of M athematics \\nand Artificial  Intelligence ,     28,  39-41  \\nStrzalkowski, T.;  Fang, L;  Perez-C arballo, J. & Jin, W. (1997). Natural La nguage Informati on \\nRetrieval TREC-6 Report ,  NIST Special  Publicatio n500-24 0: The Sixth Text REtrieval \\nConference ( TREC 6).  [Online]  Available: http://tre c.nist.gov/ pubs/trec6/t6_pr oceedings.ht ml \\nStrzalkowski, T.;  Perez-Ca rballo, J.; Karlgren, J. ; Hul th, A.  Tapanainen, P.; & Lahtinen, T.  \\n(1999).  Natur al language i nformation re trieval: TREC -8 report. NIST Special P ublication 500-\\n246:The Ei ghth Text REtrieval Conference (TREC 8)     [Online]  Available: \\nhttp://trec.nist .gov/ pubs/tre c8/papers/ge8adhoc2.pdf \\nStrzalkowski, T.; Stein, G.; Wise, G.B.; Perez-C arballo, J; Tapanainen, P.; Jarvinen, T.; \\nVoutilainen, A. & Karlgren, J. (1998). Natural languagee inform ation retrieval: TREC-7 report. \\nNIST Special Publication 500-242: The Seventh Text REtrieval Conference (TR EC 7) [Online]  \\nAvailable: http://trec.nist.gov/pubs/trec7/t7_proceedings.htm l \\nTolle, K.M. &  Chen, H. (2000).  Com paring no un phrasing techniques  for use with medical \\ndigital librar y tools. Journ al of the American Socie ty for Information Science , 51, 352-3 70.  \\nTrybula, W.J.  (1997) . Data mining and knowledge dis covery . In: M.E. William s (Ed.). Annual \\nReview of Informat ion Science and Technology (ARIST): Volume 32. M edford, NJ: Learned Information \\nInc. for the American S ociety  for Information  Science , pp.197-2 29. \\nTsuda, K.&  Nakam ura, M . (1999). The extraction method of the w ord meaning class. In: L.C. \\nJain, (Ed.)  Third Internati onal Conference on Knowl edge-Based Intelligent Inf ormation \\nEngineering Systems.  31 Aug.-1 Sept. 1999, Adelaide, SA, Australia.  Piscataway , NJ:     I EEE , \\npp. 5 34-7   \\n \\n 37  38Twenty-One: development of a multimedia information dissemination and transaction tool. \\n[Online] Available:   http://twentyone.tpd.tno.nl/twentyone/ \\nVickery, B. (1997). Knowledge discovery from databases: an introductory review. Journal of \\nDocumentation , 53, 107-122. \\nVoorhees, E. (1999). The TREC-8 question answering track report. [Online] Available: http://trec.nist.gov/pubs/trec8/papers/qa-report.pdf \\nVoorhees, E. (2000). The TREC-9 question answering track report. [Online] Available: http://trec.nist.gov/pubs/trec9/papers/qa-report.pdf \\nWaldrop, M.M (2001). Natural language processing, Technology Review , 104, 107-108 \\nWarner, A. J. (1987). Natural language processi ng. In: Williams, Martha E. ed. Annual Review of \\nInformation Science and Technology (ARIST): Volume 22. Amsterdam, The Netherlands: Elsevier Science \\nPublishers B.V. for the American Society for Information Science, 79-108. \\nWeigard, H.& Hoppenbrouwers, S. (1998). Experiences with a multilingual ontology-based \\nlexicon for news filtering. In: A.M. Tjoa & R.R.  Wagner (Eds.). Proceedings Ninth International \\nWorkshop on Database and Expert Systems Applications,  26-28 Aug. 1998, Vienna. Los \\nAlamitos, CA: IEEE Computer Society  pp. 160-5   \\nWilks, Y. (1996). Natural language processing, Communications of the ACM , 39, 60 \\nYang, Y. & Liu, X (1999). A re-examination of text categorization methods. In: SIGIR ’99 Proceedings of the 22\\nnd Annual International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval. ACM, pp. 42-49. \\nZadrozny, W.; Budzikowska, M.; Chai, J.& Kambha tla, N. (2000).  Natural language dialogue for \\npersonalized interaction. Communications of the ACM, 43, 116-120.  \\n \\nZweigenbaum, P.& Grabar, N. (1999) Automa tic acquisition of morphological knowledge for \\nmedical language processing.  In: W. Horn, et al (Eds.). Artificial Intelligence in Medicine. Joint \\nEuropean Conference on Artificial Intelligence in Medicine and Medical Decision Making, \\nAIMDM'99  Proceedings, 20-24 June 1999, Aalborg, Denmark.  Berlin: Springer-Verlag  pp. \\n416-20  \\n \\n \\n \"],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'title': '딥러닝'}, {'title': '자연어처리'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "results = collection.get(\n",
    "    include=['embeddings', 'documents', 'metadatas']\n",
    ")\n",
    "\n",
    "for emb in results['embeddings']:\n",
    "    print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\" \\n \\n \\nChowdhury, G. (2003) Natural language processing. Annual Review of \\nInformation Science and Technology, 37. pp. 51-89. ISSN 0066-4200 \\n \\n \\n \\nhttp://eprints.cdlr.strath.ac.uk/2611/\\n \\n \\n \\nThis is an author-produced versio n of a paper published in The \\nAnnual Review of Information Science and Technology  ISSN 0066-4200 . \\nThis version has been peer-reviewed, but does not include the \\nfinal publisher proof corrections, published layout, or pagination. \\n \\nStrathprints is designed to allow users to access the research \\noutput of the University of St rathclyde. Copyright © and Moral \\nRights for the papers on this site are retained by the individual \\nauthors and/or other copyright  owners. Users may download \\nand/or print one copy of any articl e(s) in Strathprints to facilitate \\ntheir private study or for non-co mmercial research. You may not \\nengage in further distribution of th e material or use it for any \\nprofitmaking activities or any commercial gain. You may freely \\ndistribute the url ( http://eprints.cdlr.strath.ac.uk) of the Strathprints \\nwebsite. \\n \\nAny correspondence concerning this service should be sent to The \\nStrathprints Administrator: eprints@cis.strath.ac.uk  \\n  Natural Language Processing \\n \\nGobinda G. Chowdhury \\n   Dept . of Computer and Information Sci ences \\n   University  of Strathcly de, Gla sgow G1 1XH, UK \\n    e-m ail: gobinda@di s.strath.ac.uk \\n \\nIntroductio n \\n \\nNatural Language Processi ng (NLP) is an area of research and appl ication that explores how \\ncomputers can be used to understand and manipulat e natural language text or speech to do useful \\nthings. NLP  researchers aim to gather knowledge on  how hum an beings  understand and use \\nlanguage so t hat  appropria te  tools and t echni ques can be develope d to m ake co mputer sy stems \\nunderstand an d manipulate natural langu ages to perform  the desired  tasks.  The foundations of \\nNLP lie in a num ber of disciplines, viz. co mputer and inform ation sciences,  linguistics, \\nmathematics,  electrical and electronic engi neering, ar tificial intelligence and robotics, \\npsychology, etc.   Applications of NLP i nclude  a number of fields of studies, such as machine \\ntranslation, natural language text processing a nd summarization, user interfaces,  multilingual and \\ncross language information retrieval (CL IR), speech  recognition, artificial intelligence and expert \\nsystems, and so on. \\n \\nOne im portant area of  application of NLP that is  relatively new and has not been covered in th e \\nprevious ARIST chapters on NLP has beco me quite prom inent  due to the prolifer ation of the \\nworld wide web and digital  libraries. Several research ers have pointed out the need for \\nappropriate research in faci litating m ulti- or cross-lingual inform ation  retrieval,  including \\nmultilingual text processing and m ultilingual user int erface sy stems,  in order t o exploit the full \\nbenefit of the www and digital libraries ( see for ex ample, Borgm an, 1997; Peters & Picchi, 1997) \\n \\nScope \\nSeveral ARIS T chapters ha ve reviewed t he field of NLP. The m ost recent ones include that by \\nWarner  in 1987, and Haas  in 1996.  Reviews of literature on large-scale NLP sy stem s, as well as \\nthe various theoretical is sues have also a ppeared  in a num ber of publications (see for exam ple, \\nJurafsky  &  Martin, 200 0; Manning & Schutze, 199 9; Mani & May bury, 1999; Sparck Jones, \\n1999;  Wilks, 1996).   Smeaton (19 99) provides  a good  overview of t he past research on the \\napplications o f NLP in various inform ation re trieval tasks. Several ARIST chapt ers have \\nappeared  on areas related to NLP, such as on  machine-readable dictionaries (Amsler, 1984; \\n 1 Evans, 1989), speech sy nthesis and reco gnition (Lange, 1993), and cross-language information \\nretrieval (Oar d & Dieke ma, 1998). Research on NLP is regularly  published in a number of \\nconferences such as the annual proceedings of AC L (Association of Com putatio nal Linguistic s) \\nand its European counterpart EACL, biennial pr oceedings of the  I nternational Conference on \\nComputational Linguistics (COLING),  annual proceedings of t he Message Understanding \\nConferences ( MUCs), Text Retrieval Conference s (TR ECs) and AC M-SIGIR (As sociation of \\nComputing Machinery  – Special Interest Group on  Inform ation Retrieval) conferences. The m ost \\nprom inent journals reporting NLP resea rch are Comp utational Linguistics  and Natural Lan guage \\nEngineering . Articles r eporting NLP research also  appear in a num ber of inform ation science  \\njournals such as Information Processing and Management, J ournal  of the American Society for \\nInformation Science and Technology, and Journal of Documentation. Several resear chers have \\nalso conducted dom ain-specific NLP stu dies and have reported them  in journals specifically  \\ndealing with the dom ain co ncerned, such as the International  Journ al of M edical Informatics  and  \\nJournal of Ch emical Information a nd C omputer Science . \\n \\nBeginning wi th the basic issues of NLP, this chapte r aims to chart the major research activities in \\nthis area sinc e the last ARI ST  Chapter in 1996 (Haas, 1996), including:  \\n(i) natural language text processing sy stems – te xt summariz ation, information extraction, \\ninform ation retrieval, etc., includi ng domain-specific applications; \\n(ii) natural language interfaces; \\n(iii) NLP in the context of www a nd digital libraries ; and \\n(iv) evaluation of NLP sy stems.  \\n \\nLinguistic research in  infor mation retrieval has not  been covered in this review, since this is a \\nhuge area and has been dealt with separately  in this v olume by  Davi d Blair. Sim ilarly, NLP iss ues \\nrelated to the inform ation retrieval tools (sear ch engines, etc.) for w eb search ar e not covered in \\nthis chapter since a separ ate chapter on indexing and r etrieval for the Web has been written in t his \\nvolum e by   Edie Rasm ussen.  \\n  \\nTools and techniques developed for building NLP systems have be en discussed in this chapter \\nalong with t he specific areas of applications for whic h the y have been built.  Alt hough m achine \\ntranslation (MT) is an important part, a nd in f act the origin, of NLP resear ch, this paper does not \\ncover this topic with sufficient detail since this  is a huge area and demands a sepa rate chapter on \\nits own. Sim ilarly, cross-language infor mation re trieval (CLIR), althoug h  is  a very important \\n 2 and big area in NLP resear ch, is not covered in great detail in this chapter . A se parate chapte r on \\nCLIR res earch appeared in ARIST (Oar d & Di ekema, 199 8). How ever, MT and CLIR have \\nbeco me two i mportant are as of resear ch in the context of the www digital libraries. This chapter \\nreviews so me works on M T and CLIR in the context of  NLP and I R  in digi tal libraries and \\nwww.  Artifi cial Intelligence techniques,  including ne ural networks etc., used in NLP have not  \\nbeen included in this chapte r. \\n \\nSome Theor etical Develo pments \\nPrevious ARIST chapters (Haas, 1996; Warner, 1987) described a num ber of theoretical \\ndevelopm ents that have influenced resear ch in NLP. The m ost recent theoretical developments \\ncan be grouped into four classes: (i) stat istical  and corpus-based methods in NL P, (ii) recent  \\nefforts to use WordNet  for NLP r esearch, (iii) the resurgence of inte rest in finite-state and other \\ncomputationally lean approaches to NLP , and (iv)   the initiation of collaborative projects to create \\nlarge grammar and NLP tools. Statistical methods are used in NLP for a num ber of purposes, e.g., \\nfor word sense disam biguation,  f or gene rating gr ammars and parsing, fo r determ ining st ylistic \\nevidences of authors and s peakers, and so on. Ch arniak (1995)  points out t hat 90% accuracy  can \\nbe obtained in assigning part-of-speech tag to a wo rd by applying s imple statistic al measures. \\nJelinek (1999 ) is a widely  cited source on the use of statistical meth ods in NLP, e specially  in \\nspeech processing. Rosenfield (2000) reviews st atisti cal language m odels for speech processi ng \\nand argues for a Bay esian approach to the integr ation of linguistic theories of data. Mihalcea & \\nMoldovan  (1999) m ention that althou gh thus far st atistical approaches have been considered the \\nbest for word sense disa mbiguation, they are useful  only in a sm all set of texts. Th ey propose th e \\nuse of WordNet  to im prove the results of statis tical analy ses of nat ural language texts. WordNet \\nis an online le xical referenc e system  developed at  Princeton University. This is an excellent NL P \\ntool containi ng English nouns, verbs, adj ectiv es and a dverbs organized into s ynony m sets, each \\nrepresenting one underl ying lexical concept. Details  of WordNet is available in Fellbau m (1998) \\nand on t he web (http://www.cogsci.princeton.edu/~wn/ ). WordNet is now used i n a num ber of \\nNLP rese arch and applications. One of the major applications of WordNet in NLP has been in \\nEurope with t he formation EuroWordNet in 19 96.  EuroWordNet is a multilingua l database wit h \\nWordNets for several European languag es  includi ng Dutch, Italian, Spanish, Germ an, French, \\nCzech and Es tonian,  structured in the same w ay as the WordNet for English \\n(http://www.hum.uva.nl/~ewn/ ).  The finite-state aut omation is  the mathematical device used to \\nimplement re gular expressions –  the standard  notation for characte rizing text sequences. \\nVariations of automata such as finite-state transducers, Hidden Markov M odels, and n-gram  \\n 3 grammars ar e important co mponents of speech r ecog nition and speech sy nthesis, spell-check ing, \\nand inform ation extraction which are the im portant ap plications of NLP. Different application s of \\nthe Finite State methods in NLP have been discu ssed by  Jurafsky  & Martin (2000), Kornai ( 1999) \\nand Roche & Shabes (1997 ). The work o f NLP rese archers has been  greatly  facili tated b y the \\navailability  of large-scal e grammar for parsi ng and ge neration. Researcher s can get access to \\nlarge-scale grammars and tools thro ugh several websites, for example Lingo \\n(http://lingo.s tanford.edu), Computational Linguistics & Phonetics (http://www.coli.uni-\\nsb.de/software.phtm l), and Parallel grammar project \\n(http://www.parc.xerox.com/istl/groups/nltt/pargram /). Another significant developm ent in recent \\nyears is the for mation of various national and inte rnational consortia and r esear ch groups that can \\nfacilitate, and  help share expertise, research  in NLP. L DC (Linguistic Data Consortium ) \\n(http://www.ldc.upenn.edu/) at the University  of Pe nnsy lvania is a typical exa mple that  create s, \\ncollects and distributes speech and text database s, lexicons, and other resources f or research a nd \\ndevelopm ent among univer sities, co mpanies and gove rnment research laboratories. The Parallel \\nGrammar pro ject is another exam ple of internati onal cooperation. T his  is a collaborative effort \\ninvolv ing res earchers fro m Xerox PARC in Califor nia, the University of St uttgar t and the \\nUniversity  of Konstanz in Germany , the Univer sity of Bergen in Norway , Fuji Xerox in Japan. \\nThe aim  of this project  is to prod uce wide coverage grammars for English, Fren ch, Germ an, \\nNorwegian, Japanese, and Urdu which are writte n collaboratively with a co mmonl y-agreed-upon \\nset of grammatical features (http://www.parc.x erox.com/istl/groups/nltt/pargram /). The recently \\nformed Global WordNet Association is y et another exam ple of cooperation. It i s a   non-\\ncommercial o rganization th at provides a platform  for discussing, sharing and con necting \\nWordNets for all languages in the world.  The first international Wor dNet conference to be held  in \\nIndia in earl y 2002 is expected to address various  problem s of NLP by researcher s from different \\nparts of the world. \\n \\nNatural Language Understanding  \\n \\nAt the core of any  NLP task there is the im portant issue of natural language understanding.  The \\nprocess of building com puter programs that unde rstand natural language involves three major  \\nproblem s: the first one relates to the t hought  process, the second one t o the representatio n and \\nmeaning of the linguistic input, and the  third one  to the world knowledge. Thus, an NLP sy stem  \\nmay begin at the word level – to determine the morpholo gical structure, nature (such as part-of-\\nspeech, mean ing) etc. of t he word – an d then may move on to the sentence level – to determ ine \\n 4 the word order, gra mmar, meaning of the entire sentence, etc.— a nd then to the context and the  \\noverall envir onment or domain. A giv en word or a  sentence may have a specific meaning or \\nconnotation in a given co ntext or dom ain, and may be related to many  other words and/or \\nsentences in t he given context.  \\n \\nLidd y (1998) and Feldm an (1999) su ggest that in  orde r to un derstand natural lang uages, it is \\nimportant to be able to dist inguish am ong the fo llowi ng seven inte rdependent levels, that peo ple \\nuse to extract meaning from  text or spo ken language s:  \\n• phonetic or phonol ogical l evel that deals with pronu nciation \\n• morphological level that deals with the smallest  parts of words, that  carry  a meaning, and  \\nsuffixes and prefixes  \\n• lexical level that deals with lexical meaning of words and parts of speech analy ses \\n• syntactic level that deals with grammar a nd structure of sentences  \\n• semantic leve l that  deals with the m eaning of words and sentences \\n• discourse level that deals with the structure of different kin ds of te xt using docu ment \\nstructures and  \\n• pragmatic lev el that deals with the knowledge  that comes fro m the outside world, i.e., \\nfrom  outside the contents o f the docum ent.  \\n \\nA natural language processing s ystem  may involve  all or some of these levels of analy sis.  \\n \\n \\nNLP Tools and Techniqu es \\nA num ber of researchers h ave attem pted to co me up with im proved technology  for perform ing \\nvarious activities that for m important parts of  NLP works. These works may  be c ategorized as \\nfollows:  \\n  \\n• Lexical and m orphological analy sis,  noun phrase generation, word segmentation, etc. \\n(Bangalore & Joshi, 1999; Barker & Cor nacchia,2000;  Chen & Chang, 1998;  Dogru & \\nSlagle, 1999; Kam -Fai et al .. 1998; Kazakov et al.. , 1999; L ovis et al.. 1998; Tol le &  \\nChen, 200 0; Zweigenbaum  & Grabar, 1999)  \\n \\n 5 • Semantic and  discourse analy sis, word meaning and knowledge representation (Kehler, \\n1997;  Mihalcea & Moldovan,1999; Me yer & Dal e, 1999; Pedersen & Bruce, 1998; \\nPoesio & Vieira,1998; Tsu da & Nakamura, 199 9)  \\n \\n• Knowledge-based approaches and tools for NLP (Argam on et al.., 1998;  Fernandez &  \\nGarcia-Serrano, 2000;   Martinez et al.., 2000, 1998).  \\n \\nDogru & Slagle (1999) pr opose a m odel of lexi con t hat involves automatic acquisition of t he \\nwords as well as representation of the semantic content of in dividual lexical entries. Kazakov et \\nal.. (1999) report  research  on word segmentation based on an automatically  generated annotated \\nlexicon of wo rd-tag pairs. Kam -Fai et al.. (199 8) repo rt the features of an NLP to ol called Chicon  \\nused for word seg mentation in Chinese text.  Zweigenbau m & Gr abar (1999)  propose a method \\nfor acquiring morphological  knowledge about words in medical literature. It takes advantage of \\ncommonly available lists of sy nony m terms to  bootstr ap the acquisition pr ocess. Although the \\nauthors experim ented with the method o n the SNOMED International Microglos sary for \\npatholog y in its French version, the y claim that since the method do es not rely on  a priori \\nlinguistic kn owledge, it is applicable to o ther la nguages such as English.  Lovis et  al.. (199 8) \\npropose the d esign of a lexicon for use i n the NLP of medical texts. \\n \\nNoun phrasin g is considered to be an im portant N LP technique used in inf ormation retrieval. One \\nof the m ajor goals of  noun phrasing re search is  to investigate the possibility  of combining \\ntraditional ke yword and s yntactic approaches with  semantic approaches to text processing in \\norder to im prove the qualit y of inf ormation retrie val. Tolle and Ch en (2000)  compared four noun  \\nphrase generation tools in order to assess thei r ability  to isolate noun phrases from  medical \\njournal abstracts databas es. The NLP tools evaluated were: Chopper  developed b y the Machin e \\nUnderstanding grou p at the  MIT Media Laborator y,  Automatic Indexer  and AZ Noun Phrase r \\ndeveloped at the  University of Arizona, and NPTool  a commercial NLP tool from   LingSoft , a \\nFinnish Compan y.  The National Library of Medicin e’s  SPECIALIST Lexicon  was used alo ng \\nwith the AZ Noun Phrase r. This experiment used a reasonably  large test set of 1.1 gi gabytes of \\ntext, com prising 714,451 a bstracts fro m the CANCER LIT database.  This study  showed that with \\nthe exception of Chopper, the NLP tools were fair ly com parable in their performa nce, measured \\nin terms of re call and precision. The study  also show ed that the SPECIALIST Lexicon increased \\nthe abilit y of the AZ Noun Phraser  to generate releva nt noun ph rases.  Pedersen and  Bruce \\n(1998) propose a corpus-based approach to wo rd-sense disam biguation that onl y requires \\n 6 inform ation that can be automatically  extract ed fro m untagged text. Barker and Cornacchia \\n(2000) describe a si mple system  for choosing noun phrases, fro m a document, based on thei r \\nlength, t heir frequency  of occurrence, an d the fre quency  of their he ad noun, using   a base noun \\nphrase ski mmer and an off-the-shelf online dictiona ry. This resear ch revealed some interestin g \\nfindings:   (1) the si mple noun phrase-based sy stem  perfor ms roughl y as well as a state-of-the-art, \\ncorpus-trained ke yphrase extractor; (2) ratings for individual ke yphrases do not neces sarily  \\ncorrelate with  ratings for sets of ke yphrases for a document; and (3) agreement am ong unbiased \\njudges on t he keyphrase rating task is poor.  Silber & McCoy  (2000) report  research that uses a  \\nlinear tim e algorithm  for calculating lexical chains, which is a method of capturi ng the \\n‘aboutness’  of a docum ent.  \\n \\nMihalcea & Moldovan (1999) argue t hat  the reduced  applicabilit y of statistical methods in word \\nsense disa mbiguation  is due basically  to the lack of widely available se mantically  tagged \\ncorpora. They report   research  that enables the automatic acquisition of sense tagged corpora, \\nand is based on (1) the information provided in WordNet, and (2) the inform ation gathered from  \\nInternet using existing search engines.   \\n \\nMartinez & Garcia-Serrano (19 98) an d Martinez et al.. (200 0)  pr opose a method for t he design of \\nstructured kn owledge m odels for NLP. The key features of their method com prise the \\ndeco mposition of linguistic  knowledge s ources in sp ecialized sub-areas to tackle the com plexity \\nproblem  and a focus on cognitive archit ectures that al low for m odularity , scalabilit y and \\nreusability . The authors clai m that their approach  profi ts from  NLP techniques, first-order logic \\nand som e modelling heuris tics (Martinez et al.. 200 0). Fernandez & Garcia-Serrano (20 00) \\ncomment that knowledge e ngineering is increasingly  regarded as a means to co mplem ent \\ntraditional for mal NLP models by  adding sy mbolic modelling and i nference cap abilities in a way  \\nthat facilitates the introduc tion and m aintenan ce of linguistic experience. They  propose an \\napproach that allows the design of linguistic a pplications that integr ates different formalisms, \\nreuses existin g language re sources and supports t he implementation of the requi red control in a \\nflexible way . Costantino (1999) argues t hat qualita tive data, particularly articles from  online news \\nagencies, ar e  not yet successfully  processed, and as a result, financial operators, notably  traders, \\nsuffer fro m qualitative data-overload. IE-Expert is a system  that com bines the t echniques of NLP, \\ninform ation extraction and expert sy stems in order to be  able to suggest invest ment decisions \\nfrom  large volume of texts (Constantino,  1999).  \\n \\n 7 Natural Language Text Processing Systems \\n \\nManipulation  of texts for knowledge extraction, fo r auto matic indexing and abstracting, or for \\nproducing text in a desired format, has been rec ognized as an im portant area of resear ch in NLP. \\nThis is broadly  classified as the area of natural la nguage text processing that allows structuring of  \\nlarge bodies of textual information with a view to retrieving particular info rmation or to de riving \\nknowledge structures that may be used for a sp ecific purpose. Aut omatic text p rocessing sy stem s \\ngenerally  take some form  of text  input and transform it into an out put of som e different form . The \\ncentral task for natural language text processi ng systems is t he translatio n of potenti ally \\nambiguous natural language queries and texts into unam biguous inte rnal representations on which \\nmatching and retrieval can  take place ( Liddy, 1998). A natural language text processing sy stem \\nmay begin  with morphological analy ses. Stemm ing of  terms, in both the queries and docum ents, \\nis done in order to get the morphological variants of the words inv olved. The le xical and synt actic \\nprocessing involve the uti lization of lexicons for deter mining the characteristi cs of the words,  \\nrecognition of their parts- of-speech, de termining the words and phrases, and for parsing of the \\nsentences.  \\n \\nPast research  concentrating on natural language  text  processing sy stem s has been reviewed  by \\nHaas (1986), Mani & May bury (1999), Smeaton ( 1999), and  Warner (1987).  S ome NLP systems \\nhave been built to process texts using particular small  sublanguages to reduce the size of the  \\noperations and the nature of the com plexities.  These domain-specifi c studies are largely  known as \\n'sublanguage analy ses' (Grish man & Kittredge, 1986). Som e of these studies are li mited to a  \\nparticular subject are a such as medic al scienc e, wherea s others deal with  a specifi c type of \\ndocum ent such as patent texts.  \\n \\n \\nAbstracting  \\n \\nAutomatic ab stracting and text summari zation are now used sy nonymously that aim to generate \\nabstracts or s ummari es of texts. This are a of N LP rese arch  is becoming m ore common in the web \\nand digital library  environment.  In simple abst racting or summari zation sy stems, parts of text – \\nsentences or paragraphs – are sel ected autom atically based on some linguistic and/or statistical \\ncriteria to produce the abstract or su mma ry. More  sophisticated sy stems may merge two or more \\n 8 sentences, or parts thereof, to generate one cohe rent sentence, or may generate sim ple summar ies \\nfrom  discrete items of data.   \\n \\nRecent interests in autom atic abstracting and text summarization are reflect ed by the huge \\nnumber of research papers appearing in a num ber of i nternational conferences an d workshops \\nincluding ACL, ACM, AA AI,  SIGIR, a nd va rious national and reg ional chapters of the \\nAssociations.   Several tech niques are used for autom atic abstractin g and text summa rization. \\nGoldstein et al.. (19 99) use conventional IR methods  and lin guistic cues for extracting and \\nranking sentences for generating news article  summar ies. A num ber of studies on text \\nsummarizatio n have been reported recently. Silber an d McCoy  (2000) claim  that their  linear ti me \\nalgorithm  for calculating lexical chains is an efficient  method for p reparing auto matic \\nsummarizatio n of  d ocuments. Chuang a nd Yang (2000) report a text summarization techniq ue \\nusing  cue phrases appeari ng in the texts of US patent abstracts.  \\n \\nRoux and  Ledora y (2000) report a proje ct, called  Aristotle, that aims to build a n autom atic \\nmedical data system  that is capable of produc ing a semantic representation of th e text in a \\ncanonical form . Song  and  Zhao (20 00) propose a method of auto matic abstracting that i ntegrates \\nthe advantages of  bot h linguistic and statistical analy sis in a corpus.  Jin and  Don g-Yan (2000)  \\npropose a m ethodol ogy for  generating autom atic abstracts that  provides an integ ration of the \\nadvantages of methods based on linguist ic analy sis and those based on statistics. \\n \\nMoens and Uy ttendaele (1997) describe the SALOM ON (Su mmar y and Anal ysis of Legal t exts \\nFOR Managi ng Online Needs) project t hat automa tically  summari zes legal text s written in Dutch. \\nThe sy stem  extracts releva nt inform ation from  the fu ll texts of Bel gian cri minal case s and us es it \\nto summariz e  each deci sion. A text gra mmar represented as a semantic network is used to \\ndeter mine the category  of each case. T he system  extracts r elevan t information about each case,  \\nsuch as the na me of the court that issues the deci sion, the decision date, the offences charged, the \\nrelevant statu tory provisions disclosed by the c ourt, as well as the legal principles applied in  the \\ncase.  R AFI (resu me automatique a fra gments i ndicateurs) is an automatic tex t summari zation \\nsystem  that transform s full text scientific and te chnical docu ments into condensed t exts \\n(Leh mam, 1999).  RAFI adopts  discourse analy sis technique usi ng a thesaurus for recognit ion \\nand selection of the  most pertinent elements of texts. The sy stem assumes a typical structure of  \\nareas fro m each scientif ic document, viz. pr evious knowledge, cont ent, method and new \\nknowledge.  \\n 9  \\nMost of the a utomatic abstracting and text su mmariz ation s ystems work satisfact orily  within a \\nsmall text collection  or wit hin a restricted  dom ain. Building r obust and dom ain-independent \\nsystems is a c omplex and resource-intensive task. Arguing that pure ly autom atic abstracting \\nsystems do not alway s produce useful results, Crave n (1988, 1993, 2000) proposes a hy brid \\nabstracting syste m in whic h som e tasks are perfo rmed by human abstractors and others b y an \\nabstractor’ s assistanc e software c alled TEXNET. However, rec ent experiments on the usefulness \\nof the autom atically  extracted key words a nd phrases from  full texts  by TEXNET in the actual \\nprocess of abstracting by  human abstract ors show ed some consider able variation am ong subjects, \\nand onl y 37%  of the subjects found t he key words and phrases to be useful in writing their \\nabstracts ( Craven, 2000).  \\n \\n \\nInformation Extraction \\n \\nKnowledge discovery  and data mining have beco me important are as of resear ch over the past few \\nyears and a num ber of infor mation scien ce  journals have published special issue s reporting \\nresearch on these topics (see for exam ple, Benoit,  2001;Qin and N orton, 1999; Raghavan et al.., \\n1998;  Trybula, 1997;  Vickery , 1997). Knowledge  discovery  and data mining research use a \\nvariety  of techniques in order to extract useful information from  source documents. Inform ation \\nextraction (IE) is a subset of  knowledge discovery  and data mining r esear ch that  ai ms to extract \\nuseful bits of textual infor mation from  natura l language texts  (Ga izauskas & Wilks, 1998). A \\nvariety  of inf ormation extraction (IE) techniques are used and the extracted infor mation can be \\nused for a num ber of purposes, for exam ple to pr epare a su mmary  of texts, to populate databases, \\nfill-in slots in fram es, identify ke ywords and phrase for inform ation retrieval, and so on. IE \\ntechniques are also used for classify ing text ite ms according to som e pre-defined categories. A n \\nearlier exa mple of text categorization s ystem is CONSTRUE, developed for Reut ers,  that \\nclassifi es news stories (Hay es, 1992). T he CONSTR UE software was subsequently generalized \\ninto a commercial product called TCS (Text Categor ization Shell). An evaluation of five text \\ncategorizatio n system s has been reported b y Yang an d Liu (1 999).  \\n \\nMorin (19 99) suggests that althoug h many IE systems can successfull y extract term s from \\ndocum ents, acquisition of  relati ons between terms is still a difficulty. PROMETHEE is a system  \\nthat extracts l exico-sy ntactic patterns rel ative to a specific concept ual relation from  technical \\n 10 corpora (Mor in,19 99). Bo ndale et al..  (1999) sug gest that IE sy stems must operate at many  \\nlevels, fro m word recognit ion to discour se analy sis at the level of th e complete d ocument. They \\nreport an appl ication of the Blank Slate Language  Processor (BSLP)  approach for the analy sis of \\na real life nat ural language corpus that consists  of responses to ope n-ended quest ionnaires in t he \\nfield of adver tising.  \\n \\nGlasgow et al .. (1998) report a sy stem  called MITA (Metlife’s Intelligent Text Analy zer) that \\nextracts information from  life insurance appli cations. Ahonen et al.. (199 8) pro pose a general \\nframework for text m ining that uses pragmatic and discourse level a nalyses of text. Sokol et al.. \\n(2000)  report  resear ch that uses  visualization and NL P  technologi es to perform  text m ining. \\nHeng-Hsou et al.. (2000) argue that IE s ystems are usu ally event-driven (i.e., are usually based on \\ndomain knowledge built on various events) and  propose an even t detection driven intelligent \\ninform ation extraction b y using the neur al netw ork paradigm . They  use the backpropagation  (BP) \\nlearning algor ithm to train the event detector, and  app ly NLP technolog y to aid t he selection of \\nnouns as feature words which are supposed to charact erize docu ments appropriat ely. These nouns \\nare stored in ontolo gy as a knowledge ba se, and are used  for the ext raction of useful inform ation \\nfrom  e-mail mes sages.  \\n \\nCowie and Lehnert (1996)  reviewed the earlier research on IE and  commented that the NLP \\nresearch co mmunity  is ill-prepared to tackle the difficult problems of sem antic f eature-tagging, \\nco-referenc e resolution, and discourse analy sis, all of which are i mportant issues of IE researc h. \\nGaizauska s and Wilks (1998) reviewed the IE  resear ch from  its origin in the Artificial \\nIntelligence world in the s ixties and seventies th rough to the m odern da ys. The y discussed the \\nmajor IE projects undertaken in different sector s, viz., Academic Resear ch. E mployment, Fault \\nDiagnosis, Finance, Law, Medicine, Military  Intelligence, Police, Software Sy stem  Requirements \\nSpecification, and Technol ogy/Product Tracking.  \\n \\nChowdhur y (1999a) revie wed research that used te mplate mining t echniques in: the extraction of \\nproper nam es from  full text docum ents, extracti on of facts fro m press rele ases, abstracting \\nscientific papers, su mmarizing new pro duct inform ation, extracting specific information  from  \\nchemical texts, and so on. He also discu ssed how some w eb sear ch engines use te mplates to \\nfacilitate infor mation retrieval. He reco mmends that  if  each web author is given a tem plate to  \\nfill-in in order to characteri ze his/her document, then eventuall y a more controlled and s ystem atic \\nmethod of creating docum ent surrogates can be achie ved. However, he warns that  a single all-\\n 11 purpose m etadata form at will not be appl icable for all authors in all the dom ains, and further \\nresea rch is ne cessary to come up with appropriate formats for ea ch.  \\n \\nArguing that IR has been the subject of resea rch and developm ent and has been delivering \\nworking sol utions for m any decades whereas IE is a more recent an d emerging technology,  \\nSmeaton (1997) comments that it is of interest to the IE community to see how a related task, \\nperhaps the m ost-relat ed task, IR, has mana ged to use the NLP base technology in its \\ndevelopm ent so far. Co mmenting o n the future challenges of IE rese archers, G aizauskas and \\nWilks (199 8) mention that t he  performance levels of  the common IE sy stem s, which stand in the \\nrange of  50 % for co mbined recall and precision, sho uld im prove significantl y to satisfy  \\ninform ation analy sts. A m ajor stum bling block  of IE systems developm ent is the cost of \\ndevelopm ent. CONSTR UE, for exam ple required 9.5 person y ears of effort (Hay es & Weinstei n, \\n1991). Portabilit y and scalabilit y are also two big issue s for IE sy stems. Since they depend \\nheavily on t he dom ain knowledge, a given IE sy stem  may work satisfactorily  in a relatively  \\nsmaller text collection, but it may  not per form well  in a  larger collection, or i n a different \\ndomain. Alter native technologies are now being used to overcom e these proble ms. Ada ms (20 01)  \\ndiscusses the merits of the NLP and the wrapper induction technol ogy in inform ation extraction \\nfrom  the web docum ents.  In contrast to NLP,  wrapper inductio n operates independentl y of \\nspecific do main knowledge. Instead of analy sing the meaning of discourse at the  sentence lev el, \\nthe wrapper technology identifies relev ant cont ent based on the textual qualitie s that surround \\ndesired data. Wrappers operate on the surface fe atures of docum ent texts that characteriz e train ing \\nexam ples. A number of vendors, such a s Jango  (purchased by Exc ite), Junglee  (purchased by  \\nAmazon), and Mohomine  employ wrap per inductio n technolog y (Adams, 2001).  \\n \\n \\nInform ation Retrieval \\n \\nInform ation r etrieval has been a major area of app lication of NLP, and consequentl y a  number of \\nresea rch projects, dealing with the various applications on NLP in IR, have taken place \\nthroughout  the world resulting in a large volum e of publications.  L ewis and Sparck Jones (1996) \\ncomment that the generic challenge for NLP in th e field of IR  is whether the neces sary NLP of \\ntexts and que ries is doable, and the specific cha llenges are whether non-statistical and statistical \\ndata can be com bined and whether data about individ ual docum ents and whole files can be \\ncombined. Th ey further comment that there ar e major challenges in making the NLP technolog y \\n 12 operate effect ively and efficiently  and also in cond ucting appr opriate evaluation tests to assess \\nwhether and how far the approach work s in an envi ronment of interactive se arching of large te xt \\nfiles. Feld man (199 9) sug gests that in order to achieve success in IR, NLP techniques should be \\napplied in co njunctio n with other techn ologies, such as visualizati on, intelligent agents and \\nspeech recog nition.   \\n \\nArguing that  syntactic phra ses ar e more meaningful than statisticall y obtained w ord pairs, and \\nthus are more powerful for discrim inating am ong documents, Narita and Ogawa (2000) \\nuse a shallow  syntactic processing instea d of st atistica l processing to autom atically identify \\ncandidate phrasal ter ms from query  texts. Com paring the performance of  Boolean and natural \\nlanguage sear ches,  Paris and Tibbo (1998) found  that in their experi ment, Boolean sear ches  had \\nbetter results than freesty le (natural language) sear ches. However, t hey conclude d that  neither \\ncould be cons idered as the best for every query . In other words, their conclusion was that \\ndifferent queries dem and different techniques.  \\n \\nPirkola (20 01) shows that languages vary significantl y in their m orpholo gical pr operties. \\nHowever,  for each language there are two variab les that describe the morphologi cal co mplexity, \\nviz.,  index of  synthesis (IS) that describes the am ount of affixation in an individual language, i .e., \\nthe average num ber of morphemes per w ord in th e lan guage;  and i ndex of f usion (IF) that \\ndescribes the ease with which two m orphem es can be separated in a la nguage. Pir kola (2001) \\nshows that calculation of t he ISs and IFs in a la nguag e is a relatively sim ple task, and once the y \\nhave been established,  the y could be utilized  fruitfully in em pirical  IR resea rch a nd system  \\ndevelopm ent.   \\n \\nVariations in presenting subject matter  greatly  affect IR and hence linguistic vari ation of \\ndocum ent tex ts is one of the greatest cha llenges to  IR. In order to inves tigate how consistently \\nnewspapers c hoose words and concepts to descr ibe an event, Lehtokangas & Jar velin (2001) \\nchose article s on the sam e news fro m three Finnish newspapers. Their experiment revealed that \\nfor short newswire  the consistency  was 83%  and fo r long articles 47% . It was also revealed that  \\nthe newspape rs were very  consis tent in using concepts to represent events, with a level of \\nconsistency  varying between 92-97% . \\n \\n \\n 13 Khoo et al.. ( 2001) rep ort an experim ent that investig ates whether inform ation obtained b y \\nmatching cause-eff ect relat ions expresse d in docu ments with the cause-effe ct rel ations expressed \\nin user queries can be used to im prove re sults in document retrieval  com pared wi th the use of \\nonly the keywords withou t considering the rela tions. Their experiment with the Wall Street \\nJournal full te xt database re vealed that ca usal relations matching where either the cause or the \\neffect is a wildcard can be used to im prove info rmation retrieval effectiveness if t he appropriate \\nweight for each ty pe of m atching can be deter mined for each query . However, the  authors stress \\nthat the results of this stud y were not as strong as the y had expected it to be .  \\n \\nChandrasekar  & Srinivas (1998) propose that c oherent text contai ns significant latent \\ninform ation, such as sy ntactic structure and patte rns of language u se, and this in formation could \\nbe used  to i mprove the perform ance of inform ation retrieval sy stems. They  describe a sy stem , \\ncalled Glean , that uses sy ntactic infor mation for eff ectively filtering irrele vant documents, and \\nthereby  improving  the pre cision of info rmation retrieval sy stem s.   \\n \\nA num ber of tracks (research groups or themes) in the TREC series of experiments deal directly \\nor indirectl y with NLP and inform ation retrieva l, such as the cross-language track, filtering tr ack, \\ninteractive tra ck,  question-answering tra ck, and the w eb track.  Reports of progress of the NLIR \\n(Natural Lan guage Information Retrieval) project are  available in the TREC reports (Perez-\\nCarballo & Strzalkowski, 2000;  S trzalkowski. et al.., 1997, 1998, 1999).   The major goal of t his \\nproject has been to dem onstrate that robust NLP techniques used f or indexi ng and searching of \\ntext docum ents perfor m better co mpared to the si mple key word and string-based methods used in \\nstatistical full-text retrieval (Strzalkowski, T. et  al.., 1999). However, results indicate that sim ple \\nlinguisticall y motivated indexing (LMI)  did n ot prove to be m ore effective than well-executed \\nstatistical approaches in English language texts.  Never theless, it was noted that m ore detailed \\nsearch topic statements responded well t o LMI com pared to  terse one-sentence search queries.  \\nThus, it was concluded t hat quer y expans ion, us ing NLP techniques,  leads to a sustainable \\nadvances in IR effectiveness (Strzalkowski et al.., 1999).   \\n \\n \\nNatural Language Interfaces \\n \\nA natural language interface is one that accepts  query  statements or commands i n natural \\nlanguage and sends data to so me system , typical ly a retrieval sy stem, which then r esults in \\n 14 appropriate  r esponses to the commands or quer y statements. A nat ural language interface should \\nbe able to translate the natural language statem ents into appro priate actions for th e system .  \\nA large num ber of natural language interfaces that work reasonably well  in narrow dom ains have \\nbeen reported in the literature (for review of  such s ystems see Chowdhur y, 1999b, Chapter \\n19;Haas, 1996; Stock, 2000).  \\n \\nMuch of the e fforts in natur al language interface desig n to date have  focused on handling rather  \\nsimple natural language qu eries. A nu mber of questio n answering sy stem s are now being \\ndeveloped tha t aim to provi de answers to  natural langu age questions, as opposed t o docum ents \\ncontaining i nformation related to the q uestion. Su ch system s often use a variety  of IE and IR \\noperations usi ng NLP tools  and techniqu es to get the correct answer  from  the source texts. Bre ck \\net al. (1999) r eport  a question answering sy stem  that uses techniques fro m knowledge \\nrepresentatio n, inform ation retrieval, and NLP. Th e authors claim  that this com bination enables \\ndomain independence and robustness in t he face of te xt variabilit y, both i n the question and in the \\nraw text documents used as knowledge s ources. Research reported i n the Question Answering \\n(QA) track of   TREC  (Text Retrieval C onferences) s how some interesting resul ts.   The basic \\ntechnolog y used by the participants in th e QA track included several steps. First, cue \\nwords/phrase like ‘who’ (as in ‘who is the prime minister of Japan’ ), ‘when’  (as in ‘When did the \\nJurassic perio d end’ ) were identified to guess wh at was needed; and then a small portion of the  \\ndocum ent collection was retrieved using standard te xt retrieval tech nology. This was followed  by \\na shallow parsing of the ret urned docum ents for id entify ing  the entities required for an answer. If \\nno appropriate answer ty pe was found then best matching passage was retri eved. This approa ch \\nworks well as long as the q uery types  recognized by the system  have broad cov erage, and the \\nsystem  can classify  questions reasonably accura tely (Voorhees,1999). In TREC-8, the first QA \\ntrack of TREC,  the m ost accurate Q A system s could answer more than 2/3 of the questions \\ncorrectly . In the second QA  track (TREC-9), the best perform ing QA sy stem , the Falcon sy stem \\nfrom  Southern Methodist University ,  was able to answer 65% of the questions  (Voorhees, 2000). \\nThese results are quite i mpressive in a domain- independent question answering environm ent. \\nHowever, the questions were still si mple in the first t wo QA tracks. In the fut ure more co mplex \\nquestions requiring answers to be obtained from  more than one documents will be handled by  QA \\ntrack rese archers.    \\n \\n Owei (2000)  argues that the drawbacks of m ost natural language interfaces to database systems \\nstem  primarily from  their weak interpretative power  which is caused by  their  i nabilit y to deal \\n 15 with the nuances in human use of natural language . The author further argues that the difficult y \\nwith NL database query  languages (DBQLs) can be overcome by  combining  c oncept based \\nDBQL paradi gms with NL approaches to enhan ce the overall ease-o f-use of the query  interface . \\n \\nZadrozn y et al. (200 0) sug gest that in an  ideal information retrieval envi ronm ent, users should be \\nable to express their interes ts or queries direc tly and naturally , by speaking, typing, and/or \\npointi ng; the computer sy stem then should be able  to provide i ntelligent answers or ask relevant \\nquestions. Ho wever, they  comment that even thou gh we build natu ral language sy stem s, this goal \\ncannot be full y achieved due to lim itatio ns of  science,  technology, business knowledge, and \\nprogramming environm ents. The specifi c problem s include (Zadrozny  et al., 2000):  \\n• Limitations of NL understanding;   \\n• Managing the  complexities of interaction (for exam ple, when using NL on devic es with \\ndiffering ban dwidth);  \\n• Lack of precise user models (for exam ple, knowing how dem ographics and personal \\ncharacteristics of a person should be reflected in the type of lang uage and dialog ue the \\nsystem  is usi ng with the user), and  \\n• Lack of m iddleware and toolkits.  \\n \\nNLP Software \\n \\nA num ber of  specific NLP  software products have  been developed  over the past decades, so me \\nof which are available for free, while others  are avail able co mmer cially. Many such NLP \\nsoftware pack ages and tool s have alread y been m entioned in t he discussions through out this \\nchapter. Some more NLP t ools and soft ware are mentioned in this section. \\n \\nPasero & Sab atier (1998)  describe principles underly ing ILLICO, a generic natural-language \\nsoftware tool for buil ding larger applications  for perfo rming specific linguistic tasks such as \\nanaly sis, synthesis, and gui ded com position. Lid dy (1998) and  Liddy et al. (20 00) discuss the \\ncommercial u se of NLP in IR with the exam ple of DR-LINK (Document Retrieval Using \\nLINguistic Knowledge) system  dem onstrating the capabilities of NLP for IR. Detailed product  \\ninform ation a nd a dem o of DR-LINK are now available online (http://www.textwise.co m/dr-\\n 16 link.htm l).   Nerbonne et al. (1998) report on GLOSSER, an intelligent assistant for Dutch \\nstudents for learning to read French. Scott (1999) describes the Kana Customer M essaging \\nSystem  that can categorize inbou nd e-m ails, forward th em to the rig ht department and generally \\nstrea mline the response process. Kana  also has an auto-suggestion function that helps a customer \\nservice repr esentative answer questions on unf amiliar t erritory . Scott  (1999)  describes another \\nsystem , called Brightware, that  uses NLP techniques  to elicit meaning from  groups of words or \\nphrases and reply  to som e e-mails automatic ally.  NLPWin is an NLP sy stem  from Microsoft  that \\naccepts sente nces and delivers detailed sy ntactic analy sis, together with a logical form   \\nrepresenting an abstraction of the m eaning (E lworthy, 2000). Scarle tt and Szpakowicz (2000) \\nreport a diagnostic evaluati on of DIPETT, a br oad-coverage parser  of English sentences.  \\n \\nThe Natural Language Processing Laboratory , Center  for Intelligent  Inform ation Retrieval at t he \\nUniversity  of Massachusett s,  distributes sour ce codes and executables to support IE sy stem  \\ndevelopm ent efforts at other sites. Each module is designed to be us ed in a dom ain-specific an d \\ntask-specific custo mizable IE system . Available software includes (Natural Lan guage …, n.d .) \\n• MARMOT Text Brackettin g Module ,  a text file translator which segments arbitrary  text \\nblocks into sentences, applies low-level s pecialists suc h as date reco gnizers, assoc iates \\nwords with part-of-speech tags, and brack ets the text into annotated noun phrases, \\nprepositional phrases, and verb phrases.  \\n• BADGER Extraction M odule, that analy zes bracketed t ext and produces c ase fr ame \\ninstantiations according to application-specific domai n guidelines.  \\n• CRYSTAL Dictionary Induction Module , that learns text extraction rules, suitable for use \\nby BADGER, from  annotated training te xts.  \\n• ID3-S Inducti ve Learning M odule , a variant on ID3 w hich induces decision trees  on the \\nbasis of training exam ples.  \\n \\n \\nWaldrop (2001) briefly  describes the features of  three NLP softwar e packages, viz.  \\n \\n• Jupiter ,  a product of t he MIT resear ch Lab that  works in the fiel d of weather forecast \\n• Movieline , a product of Carnegie Mellon that talks ab out local m ovie schedules, and   \\n 17 • MindNet from  Microsoft Rese arch, a s ystem  for automatically  extracting a massively  \\nhyperlinked  web of concepts, from , say, a standard d ictionary. \\n \\nFeldman (199 9)  m entions a num ber of NLP software packages, such as \\n• ConQuest , a part of Excalibur , that incorporates a lexicon that is im plemented as a \\nsemantic network  \\n• InQuery that parses sentences, stems words and recognizes proper nouns and concepts \\nbased on term  co-occurre nce  \\n• The Linguist X parser  from  XERO X PARC that extracts sy ntactic information, and is \\nused in InfoSeek   \\n• Text m ining sy stem s like NetOwl from SRA and KNOW-IT  from TextWise .  \\n \\nA recent surv ey of 68 E uropean university centres in com putational linguistics and NLP, carried \\nout under the auspices of a Socrates Wor king Grou p on Advanced Computing in the Humanities, \\nrevealed that Java has alr eady reached the status of second m ost commonly  taught programmi ng \\nlanguage (Black et al., 2000). In addi tion, Jav a based program s are being used to develop \\ninteractive instructional materials.  Black et al . (2000) review so me Java-bas ed coursewar e in use \\nand  discuss  the issues involved in m ore com plex nat ural language processing applications that \\nuse Java.   \\n \\n \\nInternet, Web and Digital  Library Applications of NLP  \\n \\nThe Internet and the web h ave brought  significant improvem ents in the way  we create, look for \\nand use infor mation. A hu ge volum e of inform ation is now available throug h the  Internet and \\ndigital libraries. However, these developments ha ve made so me proble ms relat ed to inform ation \\nprocessing and retrieval more prom inent.  According to a recent Survey  (Global Reach, 2001), \\n55% of the Internet users are non-Engl ish speak ers an d this is increasing rapidl y, thereby \\nreducing the percentage of net users who are na tive English speakers. However, about 8 0%  of \\nthe Internet and digital library  resources availabl e today are in English (Bian, Guo-Wei & Che n, \\n2000).  This c alls for the urgent need for the est ablishment of  m ultilingual inf ormation sy stems \\nand CLIR facilities.   How to m anipulate the large vol ume of multilingual data has beco me a \\nmajor research question. In fact, several issues ar e involved here. At the user interface level, th ere \\nhas to be a qu ery translatio n system  that should transla te the quer y from the user’s native \\n 18 language to t he language o f the sy stem . Seve ral approaches have been proposed f or quer y \\ntranslation. The dictionar y based approach uses a bilingual dictionary  to convert  terms from the \\nsource language to the target langua ge. Coverage and up-to-datene ss of the bilingual dictionar y is \\na major issue here. The corpus-based approach u ses parallel corpora for word selection, where the \\nproblem  lies with the dom ain and scale of the co rpora. Bian & Chen (2000)  propose a Chinese-\\nEnglish CLIR sy stem  on www, c alled MTIR,  that integrates the query  translation and document \\ntranslation. They  also address a nu mber of issues of  machine trans lation on the web, viz., the role \\nplayed by  the HTML tags in translation, the trade-off between the s peed and performance of the \\ntranslation syste m, and the form  in whic h the translated material i s presented.  \\n \\nStaab et al. (1999) describe the features of an  intellige nt inform ation agent called GETESS that \\nuses semantic methods and NLP capabilities in orde r to gather tourist  inform ation from  the web \\nand present it to the hum an user in an int uitive, user-friendl y way. Ceric (2000) reviews the \\nadvancem ents of the web search technology  and mentions that, among others, NLP technologies \\nwill have very good im pact on the success of th e sear ch engines.  Mock and Vem uri (1997) \\ndescribe the Intelligent News Filtering Organizationa l System  (INFOS) that is designed to filter \\nout unwanted news ite ms from  a Usenet.   INFOS  builds a profile of user interests  based on the \\nuser feedback . After the user browses e ach article , INFOS asks the user to rate the article, and \\nuses this as a criterion for selection (or rejection) of similar articl es next tim e rou nd. News \\narticles are cl assified by  a simple key word method, called the Global Hill Cli mbing (GHC), that \\nis used as a si mple quick-pass method. Articles that cannot be classified by  GHC are passed \\nthroug h a WordNet knowledgebase through a Case based reasoning (CBR) module which is a \\nslower but more accurate method. Very sm all-scale evaluation of INFOS suggests that the \\nindexing  pattern method, i. e., mapping o f the words from  the input text into t he correct concepts \\nin the WordN et abstraction hierarchy , correctly  classif ied 80% of the article s; the major rea sons \\nfor errors being the weakness of the sy stem to disa mbiguate pronouns.  \\n \\nOne of the major stum bling blocks of providin g perso nalized news delivery to us ers over the \\nInternet is the problem  involved in the autom atic association of related item s of different media \\ntype. Carrick and Watters (1997) describe a sy stem  that ai ms to det ermine to what degree any  two \\nnews ite ms re fer to the sa me news event.  This resear ch focused on deter mining the associatio n \\nbetween photographs and s tories b y usin g nam es. The algorithm  developed in co urse of this \\nresea rch wa s tested against  a test data se t as well as n ew data set s.  The pair of news ite ms an d \\n 19 photos generated b y the s ystem were checked b y human experts. The sy stem  performed,  in term s \\nof recall, precision and tim e, sim ilarly  on the new data sets as it did on t he traini ng set.  \\n \\nBecause of the volum e of text available on the web, many  researchers have proposed to use t he \\nweb as the testbed for NLP research. Grefenstette  (1999) argues that although noi sy, web text \\npresents language as it is used, and statistics de rived from  the web can have practical uses in \\nmany  NLP ap plications.  \\n \\n \\nMachine Translation and CLIR \\n \\nWith the prol iferation of the web and digita l libraries, m ultilingual inform ation retrieval has \\nbeco me a major challenge. There are two  sets of  issues here: (1) recogniti on, m anipulation an d \\ndisplay of m ultiple languages, and (2) cross-langua ge inform ation search and retrieval (Peter & \\nPicchi, 1997) .  The first set of issues relate to  the enabling technol ogy that will al low  users to \\naccess infor mation  in what ever language it is stored; while the second set im plies per mitting  \\nusers to specify  their infor mation needs in their preferred langua ge while retrieving inf ormation \\nin whatever language it is stored.  Text t ranslati on can take place at two levels: (1) translation of \\nthe full text fr om one language to anothe r for the pur pose of search and retrieval, and (2) \\ntranslation of queries fro m one language to one or  more different languages. The first option  is \\nfeasible for s mall collectio ns or for specific a pplications, as in mete orological reports (Oudet, \\n1997).  Translation of q ueries is a more practicab le approach and pro mising results have been \\nreported in the literature (discussed below).  \\n \\nOard (1997) comments that seeking infor mation from  a digital libra ry could benefit from  the \\nabilit y to query large collections once using a single l anguage. Furt herm ore, if the retrieved \\ninform ation is not available in a language that the user can read, some form  of translation will be \\nneeded. Multi lingual thesauri such  as EUROVOC help to address thi s challenge by facilitating \\ncontrolled vocabulary  search using terms fro m sever al languages, and services such as INSPEC \\nproduce Engl ish abstracts for docum ents in ot her lan guages (Oard, 1997).  However, as Oard \\nmentions, fully autom atic MT is presently neither sufficiently  fast nor sufficiently accurate to \\nadequately  support i nteractive cross-language info rmation seeking i n the web and digital librari es. \\nFortunatel y, an active and rapidl y growing r esearch co mmunity  has coalesced around t hese and \\n 20 other related issues, apply ing techniq ues drawn fro m several fields - notabl y IR a nd NLP - to \\nprovide access to large m ultilingual colle ctions. \\n \\nBorgman (1997) comments that we have hundreds (and som etimes thousands) o f years worth of \\ntextual materials in hundre ds of languag es, create d long before dat a encoding st andards existed. \\nShe  illustrates the multi-language DL challenge with exam ples drawn from  the r esearch library \\ncommunity , which t ypically handles col lections of materials in  ab out 4 00 differ ent languages.  \\n \\nRuiz and  Sri nivasan (1998) investigate an  autom atic method for C LIR that utili zes the \\nmultilingual Unified Medical Language S ystem  (UMLS) Metathesaurus to translate Spanish \\nnatural-language queries into English. They  conc lude that  the UMLS Metathesa urus-based CLIR \\nmethod is at least equivalent to, if not better,  than m ultilingual dictionar y based approaches. Dan-\\nHee  et al. (2 000), comm ent that there i s no re liable guideline as to how large machine readable \\ncorpus resources should be com piled to develop pr actical NLP soft ware package and/or com plete \\ndictionaries for hum ans an d com putational use. They propose  a ne w mathematical tool: a \\npiecewise cur ve-fitting algorithm , and suggest how  to determ ine the tolerance error of the \\nalgorithm  for good  predicti on, usin g a sp ecific corpus.  \\n \\nTwo Tele matics Application Program  projects in the Tele matics fo r Libraries se ctor, TRANS LIB \\nand CANAL/ LS, were acti ve between 1995 and 1997 (Oard,1997). Both these pr ojects \\ninvestigated cross-language searching in library  catalogs, and each included Engl ish, Spanish a nd \\nat least one other language: CANAL/LS added German and French, while TRANSLIB added \\nGreek. MULINEX, another European pr oject,  is concerned with the efficient use of m ultilingual \\nonline inf ormation. The pr oject ai ms  to process multilingual inf ormation and pr esent it to the \\nuser in a way  which facilitates finding and ev aluating t he desired information quic kly and \\naccurat ely (MULINEX, n.d.). TwentyOne , started in 1996, is a EU f unded pr oject which has the \\ntarget to develop a tool  for efficient dissemina tion of multimedia inform ation in the field of \\nsustainable development (Twenty One, n.d.). Deta ils of these and CLIR res earch projects in the \\nUS and other parts of the world have been reviewed by Oard & Diekam a (1998).  \\n \\nMagnini et al.   (2000) repor t two projects where NLP has been used for im proving  the \\nperformance i n the public adm inistration  sector. The first project, GIST, is concerned with \\nautomatic multilingual generation of inst ructiona l text s for form -filling. The second project, \\n 21 TAMIC, ai ms at providing an interface f or inte ractive acce ss to infor mation, centered on NLP \\nand supposed to be used by the clerk but with the active participation of the citizen. \\n \\nPowell and Fox (1998) describe a federa ted search sy stem, call ed SearchDB-M L Lite ,  for \\nsearching heterogeneous multilingual the ses and di ssertations collections on the World Wide Web \\nNDLTD: Net worked Digit al Library  of Theses and Dissertations ( NDLTD, n.d.). A markup \\nlanguage, called SearchDB , was developed for describing the characteristic s of a search engine  \\nand its interface, and a prot ocol was built  for re questing word trans lations between languages. A \\nreview of the results generated from  query ing over 50  sites sim ultaneously revealed that in some \\ncases more so phisticated query  mapping i s necessary  to retrieve resul ts sets that truly correspond \\nto the orig inal quer y. The authors report that an extended version o f the SearchDB  markup \\nlanguage is being develope d that can reflect th e default and available query m odifiers for each \\nsearch engine; work is also underway to implem ent a mapping s ystem  that uses t his inform ation \\n \\nA num ber of companies now provide m achine translation service, for exa mple (McMurchie, \\n1998): \\n• Berlitz International Inc. that offers pr ofessional translation service in 20 countri es \\n• Lernout & Hauspie  has an Internet Translation Division  \\n• Orange, Calif -based Language Force Inc.  that has a product called Universal tra nslator \\nDeluxe  \\n• IBM MT services through i ts WebSphere Translation Server. \\n \\nA large num ber of resear ch papers are av ailable that discuss various resea rch projects dealing \\nwith MT and CLIR with reference to specific  languages, for exam ple in Chinese (Kwok et al. \\n2000;  Lee et al., 1999), Japanese (Jie  & Akahor i , 2000; Ma, et al . 2000; Ogura  et al. 2000)  , \\nPortugese (Barahona & Al feres, 1999),  Sinhalese (Herath &  Herat h, 1999), Spanish (Weigard & \\nHoppenbro uwers,1998; M arquez et al., 2000),  Thai (I sahara et al.,2 000), T urkish  (Say , 1999), \\nand so on.  Some studies have considered m ore than two languages; see for exam ple Ide, 2000. \\nThese papers address various issues of MT, for example,  \\n \\n• Use of cue phrases in deter mining relationships am ong the lexical units in a disc ourse \\n(Say, 199 9); \\n 22 • Generation of semantic maps of terms (Ma et al., 200 0);  \\n \\n• Creation of language-specific seman tic dictionaries (Ogura et al., 2000); \\n \\n• Discourse an alysis (Jie  & Akahori, 2000); \\n \\n• Lexical analysis (Ide, 2000 ; Lee et al., 1999);  \\n \\n• Part-of-speech taggin g (Isahara et al., 2000; Marquez et al., 2000) \\n \\n• Query  translation (Kwok et al.,  2000) \\n \\n• Transliteratio n of foreign words for inf ormation retrieval (Jeong, et al., 1999) \\n \\nWeigard & Hoppenbr ouwers (1998) rep ort the way  an English/S panish lexicon,  includin g an \\nontology , is constructed for  NLP tasks in an ESPRIT project called TREVI.  E mphasizing the \\npoint t hat the re has not been an y stud y  of natu ral language information retrieval in Swedish, \\nHedlund et al. (2001) describe the features of Swedish language and point out a number of \\nresea rch proble ms. They  further stress th at separat e research in NL P in Swedish is required \\nbecause the r esear ch result s and tools for other langua ges do not  quite appl y to Swedish becau se \\nof the uni que features of the language.  \\n \\nCommenting on the pr ogress of MT research, Jurafsk y & Martin (2 000; p. 825) c omment that \\n“machine translation s ystem  design is hard wo rk, req uiring careful selection of m odels and \\nalgorithm s and com bination into a useful sy stem .” Th ey further comment that “ despite half a \\ncentury  of research, machine translation is fa r from solved; hum an language is a rich and \\nfascinating ar ea whose tre asures have only begun to be explored”.  \\n \\n \\n \\nEvaluation \\n \\nEvaluation is an im portant area in any  system development activity , and inform ation science \\nresearchers h ave long been struggling to come up with appropriate evaluation mechanisms for \\n 23 large-sc ale information sy stems. Conseq uently , NLP resear chers ha ve also been try ing to develop \\nreliable methods for evaluating robust NLP sy stems.  However, a  single set of evaluation criteria \\nwill not be applicable for all NLP tasks. Differe nt eval uation parameters may be required for each \\ntask, such as IE and automatic abstra cting which ar e significantly  different in nature co mpared to \\nsome other N LP tasks such as MT, CLIT or natural language user interfac es.  \\n \\nThe ELSE (Evaluation in Language and Speech Engineering) proj ect under the  contract from  the \\nEuropean Commi ssion aimed to study  the possible implementation of com parative evaluation in \\nNLP sy stems. Com parative evaluation in  Langua ge En gineering has been used since 1984 as a \\nbasic paradigm  in the DARPA research  program  in the US on h uman language technolog y since \\n1984 . Com parative evaluation consists o f a set of participants that com pare the results of their \\nsystems using sim ilar task s and related data with  metrics that were agreed upon.  Usually  this \\nevaluation is performed in a num ber of s uccessive evaluation ca mpaigns with more co mplex task \\nto perform  at every  cam paign. ELSE proposition departs fro m the DARPA research program in \\ntwo way s: first by consider ing usabilit y criteria in the evaluation, and second by trading \\ncompetitive aspects for more contrastive and collabor ative ones through the use of \\nmultidimensional results (Paroubek & Blasband, 1999). The ELS E consortium has identified the \\nfollowing fi ve types of eva luation (Paro ubek & Blasband, 19 99):  \\n• Basic r esearch evaluation : tries to validate a new idea or to assess t he am ount of \\nimprovem ent it brings o ver older m ethods.  \\n• Technolog y evaluation:  tries to asses s the perfor mance and appropriateness of a \\ntechnolog y for solving a problem  that is well-defined, sim plified and abstracted.  \\n• Usage evaluation: tries to a ssess the usability  of a technology for solving a real problem  \\nin the field. It  invol ves the end-users in the environm ent intended f or the deplo yment of \\nthe sy stem  under test.  \\n• Impact evalu ation: tries to measure the socio-econom ic consequences of a technology .  \\n• Program  evaluation:  attempts to determ ine how worth while a fundi ng pro gram has been \\nfor a given te chnolog y.  \\n \\nEAGLES (The Expert Advisory Group on La nguage E ngineering St andards – Evaluation \\nWorkgroup) ( Centre for .., 2000), phase one (EAGLES-I: 1993—1995) and pha se two \\n 24 (EAGLES-II:1997—1998) , is an Europe an Initiative  t hat proposed a user-centred evaluation of \\nNLP sy stems.   The EAGLES work takes as its starting point an exist ing Standard,  viz.  ISO 9126, \\nwhich is concerned primari ly with the de finiti on of quality  characteristics to be used in the \\nevaluation of software products. \\n \\nThe DiET project (1997-1999) was designed to  develop data, m ethods and t ools for the glass-box \\nevaluation of NLP co mponents, buildi ng on the r esults  of previo us projects covering different \\naspects of assessment and evaluation. T he webpage of the DiET project (DiET, 1997) sa ys that \\nthe project “will extend and develop test-suit es with annotated test item s for grammar, \\nmorphology  and discourse, for English,  French  and Germ an. DiET will provide user-support i n \\nterms of database technology , test-suite constr uction t ools and graphic interfaces.”, and that it \\n“will result in a tool-package for in-house and ex ternal quality  assurance and evaluation, which \\nwill enable the co mmerci al user to assess and com pare Language Technology pr oducts”. \\n \\nMUC, the Message Understanding Conference s, which have now ceased, was t he pioneer in \\nopening an international platfo rm for sharing researc h on NLP s ystems. In particular, MUC \\nresearchers were involved i n the evaluation of IE s ystems applied to a co mmon task.   The first \\nfive MUCs had focused on analy zing free text, iden tifying events of a specified t ype, and filli ng a \\ndata base te mplate with information about each  such event (MUC- 6, 1996). After MUC-5, a \\nbroad set of o bjectives was defined for t he fort hcom ing MUCs, such as:  to pus h inform ation \\nextraction sy stems towards greater portabilit y to new domains, and to encourage evaluations of \\nsome basic language anal ysis technologi es. In MUC-7 (the last MUC), the m ultilingual NE \\n(nam ed entities) evaluation was run using training and test articles from  comparable dom ains for \\nall languages (Chinchor, n .d.). The pape rs in the MUC-7 conference  report some interestin g \\nobservations by system  developers who were non- native speakers of the langua ge of their s ystem  \\nand sy stem  developers who were native speakers of the language o f their sy stem. Results of \\nMUC-3 throu gh MUC-7 ha ve been summarized by Chinchor (n .d.).  \\n \\n \\nConclusion \\n \\nResults of some NLP experim ents reported in this  paper show encouraging  results. However, one \\nshould not forget that m ost of these experi mental systems end in the lab; ver y few experimental  \\nsystems are converted to re al systems or products. On e of the major stum bling blocks  of NLP \\n 25 resea rch, as i n areas like information retrieval research, has been the absence of la rge test \\ncollections and re-usable experimental methods a nd tools. Fort unately, the situation has chan ged \\nover the past few y ears. Several national and internati onal research groups are now  working \\ntogether to build and re-use large test collections  and experimental tools and tech niques. Since the \\norigin of the Message Understanding Conferences, group research efforts have proliferated wi th \\nthe regular conferences and  workshops, f or exam ple,  the TREC seri es and other conferences \\norganized by  NAACL ( North Americ an Chapter of the Associ ation for Com putational \\nLinguistics), EACL (European ACL), and so on .  These group research efforts help research ers \\nshare their expertise by  building re-usable N LP tools, t est collections, and experimental \\nmethodologie s.  References to som e re-usable N LP tools and coope rative research grou ps hav e \\nbeen made earlier in this pa per (see under the heading Some Theoretical Developments ). \\n \\nSome recent studies on ev aluation also show prom ising results.  Very  small-sc ale evaluation of \\nINFOS suggests that the indexing pattern m ethod, i.e., mapping of the words from  the input text \\ninto the corre ct concepts in th e WordNet abstraction hierarchy , correctly  classifie d 80% of the \\narticles (Mock and Vem uri, 199 7). Som e large-scale experi ments with NLP als o show \\nencouraging r esults. For exam ple, Kwok  et al. (2000 ,1999) report  that their PIRCS sy stem  can \\nperform  the t asks of English-Chinese query  translation with an effectiveness of over 80%. \\nStrzalkowski et al. (TREC-8;199 8) repor t that b y using the algorith m of auto matic expansion of \\nqueries, using  NLP techniques, they obta ined a 37%  improvement of average precision over a \\nbaseline wher e no expansion was used. There are c onflicting results too. For  example, Elworth y \\n(2000) rep orts that the NLP sy stem , using the Micros oft prod uct NLPWin, perfo rmed much \\npoorer in t he TREC-9 test set co mpared with the TREC-8 test set. While tr ying to find out the \\nreasons for this discrepancy, Elworth y (2000) co mments that an important challenge for the \\nfuture work may be looki ng at how t o build a system  that merges definitive, pre-encoded \\nknowledge, a nd ad-hoc docu ments of unknown relia bility.  \\n \\nAs alre ady mentioned earlier (in the sect ion on  Abstracting), Craven’ s study  with TEXNET \\n(Craven, 199 6) shows a limited success (onl y 37%).  Gaizauska s and Wilks m ention that the  \\nperformance l evels of  the common IE syste ms, stand in the range of   50%  for co mbined recall \\nand precision. Such low succes s rate s are not accep table in  large-scale operatio nal inform ation \\nsystems.   \\n \\n 26 Smith (1998)  suggests that there are two possibl e scenarios  for the future relations between \\ncomputers an d hum ans: (1) in the user-friendlin ess scenario, com puters beco me smart enough  to \\ncommunicat e in natural language, and (2) in th e com puter friendliness s cenario humans adapt \\ntheir practices in order to c ommunicate with, and make use of, com puter s. He f urther argues that \\nthe use of com puter-fri endly encoding of natural language texts on the web is sym ptomatic of  a \\nrevolutionar y trend toward the com puterizati on of hum an knowledge .  Petreley  (2000, p.102) \\nraises a very  pertinent que stion about  natural la nguage user interfaces: “will the natural language \\ninterface hav e to wait until voice recognition b ecom es more commonplace?”. This statement \\nappears to be quite legitim ate when we see that  although a large num ber of natural language user \\ninterfaces were built, m ost at the laboratory  level,  and a few at the commer cial level (for details \\nof these see, Haas, 1996; Chowdhur y, 1999b, Chap ters 18-21), na tural language user interfaces \\nare not still very  comm on. The im pediments to pr ogress to the natur al language interfaces lie on \\nseveral planes including   language issues. Zadrozn y et al. (200 0) mention that ex cept for very \\nrestricted domains, we do not know how to compute the meaning of a sentence based on \\nmeanings of its words and its context. Anothe r problem  is caus ed by the lack of precise user \\nmodels. Zadrozny  et al. (2000) m aintain that even assum ing that we can have any piece of \\ninform ation about a person , we do not know how  could we use this knowledge t o make this \\nperson' s inter action with a dialogue s ystem most effective and pleasant. \\n \\nMT invol ves a num ber of  difficult probl ems, mainly  because hu man language i s at times quite \\nambiguous an d full of  special constructions, and excep tions to rules.   Despite that there has been \\na steady  developm ent, and MT resear ch has now reac hed a stage w here the benefits can be \\nenjoyed by people. A number of web search tools, vi z. Altavista, Google, L ycos and AOL offer \\nfree MT facil ities of web inform ation re sources. A num ber of co mpanies also provide MT \\nservices commerci ally. For exam ple, the IBM WebS phere Translation Server for Multiplatforms \\nis a machine translation ser vice available  comme rciall y for translating web documents in a \\nnumber of languages, such as English, French, It alian, Spanish, C hinese, Japanese and Korean.  \\nIn June 2001, Autodesk, a US software company began to offer MT services to its European \\ncusto mers at a cost which is 50% less co mpared to the hum an transl ation services  (Schenker, \\n2001).   Tho ugh  m achine translations are not alway s perfect and do not produce as good \\ntranslations a s hum an trans lators would produ ce, the results, and evidences of interests in \\nimproving t he perform ance level of MT sy stem s, are very enco uraging.  \\n 27 One area of a pplication of NLP that has drawn much resea rch attent ion, but where the results a re \\nyet to reach the general public with an acceptable level of performance, is the  natural language  \\nquestion-answering sy stem . While so me sy stems, as reported in this chapter, produce accept able \\nresults, there are still many failures and surprises.  Results of s ystems reported un der the QA track \\nof TREC (reported under t he heading of  natural langu age interfaces in this paper)  show prom ising \\nresults with some si mple type of natural  langua ge que ries. However,  these sy stems ar e still at  \\nexperi mental stages, and much resear ch is needed  before robust QA sy stems can be built that are \\ncapable of accepting user queries in any form  of natural language and pro ducin g natural lang uage \\nanswers r etrieved form  a num ber of distributed inform ation resources.  Scalabilit y and portabil ity \\nare the main challenges facing natural l anguage  text processing resear ch. Adams (2001) argues \\nthat current NLP sy stem s establish patterns th at are valid for a specific domain and for a \\nparticular tas k only; as soon as the topic, context or the user  changes, entirely  new patterns need \\nto be established. Sparck Jones (1999)  rightly warns t hat advanced NLP techniqu es such as \\nconcept extraction, are too expensive for larg e-scal e NLP applications. The resear ch co mmunity, \\nhowever, is making conti nuous efforts. The reason for not having reliable NLP systems that work \\nat a high level of perform ance with high  degr ee of sophistication may largely b e, not the \\ninefficiency  of the sy stem s or resear chers, but  the complexities and idios yncrasies of hum an \\nbehaviour an d comm unication patterns.  \\n \\nRefe renc es \\nAdam s, K.C. (2001). The Web as a database: New ex traction technologies & content \\nmanagement,  Online ; 25, 27-32 \\nAhonen, H.; Heinonen, O.; Klem ettinen , M. & Verkam o, A.I. (1998). Appl ying data mining \\ntechniques for descriptive phrase extract ion in digital docum ent collections.    IEEE International \\nForum on Re search and Technology. A dvances in Digital Librarie s - ADL'98,   22-24 April 1998,  \\nSanta Barbara, CA.  Los      Alam itos, CA: IEEE Com puter Societ y,  pp. 2-11  \\n \\nAmsler, R.A.(1984).  Machine-readable dictionaries. In: M. E. Will iams, (ed.) Annual  Review of \\nInformation Science and Technology (AR IST: Volum e 19, White Pla ins, NY: Knowledge \\nIndustr y Publications Inc. for the American Society  for Inform ation  Science. pp.161-2 09.  \\nArgam on, S.; Dagan, I. & Kry molowski, Y. (199 8). A memory-based approach to learning \\nshallow natural language p atterns. In 17t h Intern ationa l Conference on Com putational Ling uistics \\n(COLING '98), August 1 0-14, 1 998, Uni versité  de Montréal, Montr éal, Québec, Canada , \\nMontreal: ACL.  pp.  67-7 3. \\nBangalore, S. & Joshi, A.K. (199 9). Su pertagging: an approach to alm ost parsing. Computatio nal \\nLinguistics , 25, 237-265. \\n 28 Barahona, P.& Alfer es, J.J. (Eds.). (1999).  Progress i n Artificial Intelligence. 9th Portuguese \\nConference o n Artificial Intelligence, EPIA'99. Proceedings , 21-2 4 Sept. 1 999    Evora, Port ugal.  \\nBerlin: Springer-Verlag. \\nBarker, K.& Cornacchia, N. (2000).  Using noun phrase heads to extract document ke yphrases In: \\nH.J. Hamilton (Ed.) Advances in Artifici al Intelligenc e. Proceedings of 13th Biennial Conference \\nof the Canadi an Society for Computati onal  Studies of Intelligence, AI 2000 . 14-17 May 20 00,  \\nMontreal,    B erlin: Springe r-Verlag.  pp. 40-52   \\n \\nBenoit, G. (2 001) Data m ining. I n: Cro nin, B. (ed.).  Annual Revi ew of Information Science and \\nTech nology (ARI ST): Volume 36 . Med ford, NJ: Inform ation toda y for ASIS, pp.  \\nBian, Guo-Wei & Chen, Hsin-Hsi (2000). Cross-language inform ation access to multilingual \\ncollections on the Internet . Journal of the American S ociety for Information Scie nce, 51, 2 81-296.  \\nBlack, W.J.; Rinaldi, F. &  McNaught, J. (2000). Natural language processing in Java: \\napplications in education and knowledg e managemen t. Proceedings of the Second International \\nConference o n the Practical Application of Java.  12-14 April 2000,     Manche ster.  Practical \\nApplication Company: Blackpool.     pp. 157-70  \\n \\nBondale, N.; Maloor, P.; V aidyanathan , A.; Sengupta,  S. &  Rao, P. V.S. (1999) . Extraction of \\ninform ation from  open-ended questionna ires usi ng natural language processing techniques.  \\nComputer Science and Info rmatics ,    29,  15-22 . \\n \\nBorgman, C.L. (1997). M ulti-Media, Multi-Cultura l, and Multi- Lingual Digit al Libraries: Or \\nHow Do We Exchange Data In 400  Languages? D-Lib M agazine . [Online ] Avail able \\nhttp://www.dlib.org/dlib/j une97/06borgman.ht ml \\nBreck, E.; Burger, J.; House, D.; Light, M. & Mani, I. (1999) Question answering from  large \\ndocum ent collections.  Question Answering Systems. Papers from the 1999 AAAI Fall \\nSymposium,     5-7 Nov. 1999,     North Falmouth, MA.  Menlo Park, CA:  AAAI Press.  pp. 26-\\n31   \\n \\nCarrick, C. and Watters, C. (199 7). Au tomatic associ ation of news item s. Information Processing \\n& Management , 33, 615-632. \\nCentre for Language Technology ( 2000). EAG LES-ll Inform ation Page:  Evaluation of NLP \\nSystem s . [Online]  Available:  http://w ww.cst.ku.dk/projects/eagl es2.htm l \\nCeric, V. (2000). Advancements and trends in th e World Wide Web search. In: D.  Kalpic & V.H. \\nDobric (Eds.).  Proceedings of the 22nd International Conference o n Information Technology \\nInterfaces,  13-16 June 2000,    P ula, Croatia. SRCE University Com puter Centre, Univ. Zagr eb, \\npp. 2 11-20  \\n \\nChandrasekar , R. & Srinivas, B. (1998). Glean: usi ng syntactic infor mation in docu ment filteri ng. \\nInformation Processing & Management , 34, 623-640 \\nCharniak, E. (1995).  Natural language learni ng. ACM Computing S urveys, 27,  317-33 19. \\n 29 Chen, J.N. & Chang, J.S. ( 1998). Topic al clustering of MRD senses based on inform ation \\nretrieval tech niques. Comp utatio nal Li nguistics , 24, 61-96.  \\nChinchor,  N.  A. Overview of MUC-7/MET-2. [ Online]  Available: \\nhttp://www.itl.nist.gov/iaui /894.02/related_project s/muc/proceedings/ muc_7_pr oceedings/overvie\\nw.htm l \\nChowdhur y, G. G. (1999a) . Tem plate mining f or infor mation extraction from  digital docum ents. \\nLibrary Trends , 48, 182-208.  \\nChowdhur y, G.G. (1999b) . Introductio n to modern in formation retrieval . Londo n: Librar y \\nAssociation Publishing .  \\nChuang, W. & Yang, J. (2000).  E xtracting sente nce seg ments for text summarization: a machine \\nlearning appr oach. In:  Proceedings of the 23rd annual internation al ACM SIGIR \\nconfe rence on Research and  developm ent in info rmation retrie val, ACM, pp. \\n152-159.  \\n \\nCostantino, M . (1999). Natural language  processi ng and expert system techniques for equity derivatives \\ntrading: the IE-Exp ert system . In: D. Kalp ic & V. H.  Dobric (Eds). Proceedings of the 21st International \\nConference on  Information Technology Interfaces, Pula, Cr oatia,  15- 18 June, 1999.  Univ. Za greb , Zagreb, \\nCroatia,    pp. 63-9  \\n  \\nCowie, J. & Lehnert, W. ( 1996).  Infor mation extraction. Commu nications of  the ACM , 39, 80 – \\n91  \\nCraven, T. C. (2000) . Abstracts produced using com puter assistan ce. Journal of the American  \\nSociety for Information Sci ence,  51, 74 5-756  \\nCraven, T.C. (1988).  Text network displ ay editi ng wit h special reference to the production  of  \\ncusto mized a bstracts. Canadian  Journa l of Informati on Science , 13, 59-68. \\nCraven, T.C. (1996). An experi ment in the use of  tools for com puter-assist ed abstracting. In: \\nASIS’ 96: Proceedings of the 59th ASIS Annual Meeting 1 996. Baltim ore, MD, October 21-2 4, \\n1996 . Vol. 3 3, Medford, N J: Information Toda y, pp. 203-2 08.  \\nCraven, T.C. (1993).  A com puter-aided abstracting tool kit. Canadian Journa l of Information \\nScience , 18, 1 9-31. \\nDan-Hee,  Y.; Gomez, P.C. & Song, M.   (2000) . An algorithm  for predicting the  relationship \\nbetween le mmas and corpus size. ETRI Journal,  22,  20-31  \\n \\nDiET: Diagnostoc and Ev aluation Tool s for natu ral language appli cations (1997 ). [Online]  \\nAvailable: http://www.dfki.de/lt/projects/diet-e.htm l \\nDogru, S.&  Slagle, J.R.(1999).  Implementing a semantic lexicon.   In: W. Tepfen hart & W.  Cy re \\n(Eds.) Conceptual Structures: Standards and Prac tices. 7th International Confe rence on \\nConceptual Structures, IC CS'99  Proceedings,  12-15 July 1999,     Blacksburg, VA.    Berlin:  \\nSpringer-Verlag  pp. 154-67  \\n \\n 30 Elworthy, D. (2000). Question answering usi ng a large NLP system . The Ninth Text \\nREtrievalConference (T REC 9)   [Online]  Availa ble: \\nhttp://trec.nist.gov/pubs/tr ec9/papers/m src-qa.pdf \\nEvans, M. (1989). Com puter-readable Dictionaries.  . In: M.E. Willi ams (Ed). An nual Review of \\nInform ation Science and Technolog y (ARIST): Volume 24. Am sterdam , The Netherlands: \\nElsevier Science Publishers B.V. for the Am erican Society  for I nformation Science. 85-117.   \\nFellbaum , C. (ed.) (1998).  WordNet : an electronic le xical databas e. Cambridge , Mass : MIT \\nPress \\nFeldman, S. (1999).  NLP meets the jabberwocky . Online , 23, 62-72.  \\nFernandez, P.M. & Garci a-Serrano, A.M. (200 0). The role of knowledge-based technology in \\nlanguage appl ications developm ent. Expe rt System s with Applications 19, 31- 44  \\nGaizauskas, R. & Wilks, Y. (1998).  Information extraction: be yond docum ent retrieval. Journal \\nof Documentation , 54, 70- 105.  \\nGlasgow, B.; Mandell, A.; Binney, D.; Ghem ri, L. & Fisher, D. (1998). MITA: an inform ation-\\nextraction approach to the analy sis of free -form  text i n life insurance applications.  AI M agazine,    \\n 19,     59-71   \\n  \\nGlobal Reach  (2001) . Global Internet Statis tics (by  language). [ Online] . Availabl e: \\nhttp://www.eurom ktg.co m/globstats/ \\nGoldstein, J.; Kantrowitz, M.; Mittal, V. & Ca rbonell, J.  (1999). S ummarizing text docum ents: \\nsentence s election and eva luation m etrics. In: Proceeding of the 22nd Annual I nternational \\nConference on Resear ch and Development in In form ation Retrieval. ACM, pp. 121-128.  \\nGrefenstette, G. (1999).  The World Wid e Web as a resource for exam ple-based machine \\ntranslation tasks.  Translati ng and the C omputer 21. Proceedings of the Twenty-first International \\nConference o n Translating and t he Computer     10- 11 Nov. 19 99,     Lond on:  As lib/IMI , p p. 12  \\nGrish man, R.  & Kittredge, R. (Eds.) (1986). Analyzing language in restrict ed domains: \\nsublanguage descriptions and processi ng. London: Lawrenc e Erlbaum  Associ ates  \\nHaas, S. W. ( 1996). Natural language pr ocessing: toward large-scal e robust s ystems. In: M.E. \\nWilliam s (Ed.). Annual Review of Infor mation Science and Technology (ARIST): Volum e 31. \\nMedford, NJ: Learned Inform ation Inc. for the Am erican Society  for Inform ation  Science. pp. 83-\\n119.  \\nHayes, P. (1992)  Intellig ent high-volume text processing using sha llow, domain-specific techniques. \\nIn: J acobs, P. S., (ed. ). Text-based intellig ent systems , Hillsda le, NJ, Lawrence Erlbaum, pp. 227-241. \\nHayes,  P. & Weinstein,  S. (1991). Cons true-TIS : a system for c ontent-based inde xing of a database of \\nnews stories. I n: Rapp aport, A. & S mith, R. (eds.), Innova tive ap plications of artificial intellig ence 2, \\nCam bridge, MA , MIT Press, pp. 51-64. \\nHedlund, T.; Pirkola, A. & Jarvelin, K. (2001).  Aspects of Swedish m orpholog y and sem antics \\nfrom  the perspectives of mono- and  cross-language information retrieval. Information Processing \\n& Management , 37, 147-161.  \\nHeng-Hsou Chang; Yau-Hwang Ko & Jang-Pong Hs u (2000).  An  event-driven and ont ology-\\nbased approach for the delivery  and infor mation extraction of e-m ails. Proceedings International \\n 31 Symposium on Multimedia Software Engineering,     11-13 Dec. 2 000,  Taipei, Taiwan.  Los \\nAlam itos, CA: IEEE Com puter Society , pp. 103-9      \\n \\nHerath, S. & Herath, A. (1999). Alg orithm to de termine the subject in flexible w ord order \\nlanguage based machine tr anslations: a case study for  Sinhalese. Communicatio ns of COLIPS, 9,  \\n1-17  \\n \\nIde, N (2000). Cross-ling ual sense determination: can it wo rk? Computers a nd the Human ities,     34,  223-\\n34  \\n \\nIsahara, H.; Ma, Q.; Sornlertla mvanich,  V. & Ta kahashi, N. (200 0). ORCHID: b uilding lingui stic \\nresources in Thai.  Literary & Linguistic Computing, 15,  465-7 8    \\n \\nJelinek, F. (1999). Statistic al Methods fo r Speech Recogniti on (Language, Speech, and \\nCommunication). MIT Pre ss. \\nJeong, K.S.; Mayeng, S.H. ; Lee, J.S.; Choi, K.S.(1 999). Autom atic identification  and back-\\ntransliteration of foreign w ords for inf ormation retrieval. Informati on Processing &  Management , \\n35, 5 23-540. \\nJie Chi Yang & Akahori, K. (2000). A discourse struct ure analy sis of technical Ja panese texts a nd \\nits im plementation on the WWW. Computer Assisted Lan guage Learning,  13, 119-4 1  \\n \\nJin, Song and Dong-Yan,  Zhao (2000). Study  of automatic abstr acting based on corpus and \\nhierarchical d ictionary ,   Journal of Software, 11, 30 8-14  \\n \\nJurafsky , D. & Martin, J.H. (2000).  Speech and language processing: an introduction t o natural \\nlanguage processing, computati onal linguistics and s peech recognition . Upper Saddle River, NJ: \\nPrentice Hall.   \\nKam -Fai Wong; Lum , V.Y.&  Wai-Ip Lam  (1998). Chicon-a Chinese text manipulation language.  \\nSoftware - Practice and Experience, 28,  681-7 01  \\n \\nKazakov, D.; Manandhar, S. &  Erjavec, T. (19 99). Learning word segmentation rules for tag \\nprediction.  I n: S. Dzeroski, S. & P.  Fla ch (Eds.) Inductive Logic Programming. 9th  \\nInternational Workshop, ILP-99 Proceedings,  24-27 June 199 , Bled , Slovenia. B erlin:  Sprin ger-\\nVerlag , pp. 1 52-16 1  \\nKehler, A. (1997). Current  theories of centering fo r pronoun interpretation: a critical evaluation. \\nComputation al Ling uistics , 23, 467-475. \\nKhoo, C.S.G;  Myaeng, S.H  & Oddy, R. N (2001). Usi ng cause-effect relations in text to im prove \\ninform ation retrieval preci sion. Information Processing &  Management , 37, 1 19-145 \\nKim, T.; Si m, C.; Sanghwa, Y. & Jung, H. (1999). Fr om to-CLIR: web-based natural language \\ninterface for cross-language information retrieval. Information Processing &  Management , 35, \\n559-5 86 \\n 32 King, M. (1996). Evaluating natu ral language processing s ystems. Communications of t he ACM , \\n39, 7 3-80  \\nKornai, A. (ed.) (19 99). E xtended Finit e State M odels of  Languag e (Studies in Natural Language \\nProcessing), Ca mbridge  University  Press. \\n \\nKwok, K.L; Grunfeld, L.; Dinstl, N. & Chan, M. (20 00). TREC-9 cross language, web and \\nquestion-answering track experim ents using PIRCS.  The Ninth Te xt REtrieval Conference ( TREC \\n9). [Online]  Available:  http://trec.nist.gov/pubs/trec9/t9_proceedings.htm l \\nKwok, K.L.; Grunfield, L. & Chen, M. (1999).  TREC -8 Ad-hoc, query  filtering t rack experiments \\nusing PIRCS.  The Eighth  text retrieval Conference (TREC-8). [ Online]  Available:  \\nhttp://trec.nist .gov/ pubs/tre c8/papers/queenst8.pdf \\nLange, H. (1993). Speech Synthesis and Speech  Reco gnition: Tom orrow’ s Human-Co mputer \\nInterfaces? In: M.E. Willia ms (Ed.). Annual Review of Information Science and Technology  \\n(ARIST): Vo lume 28. Med ford, NJ: Learned Info rmation Inc. for the A merican S ociety  for \\nInform ation Science. pp.15 3-185 \\nLee, K.H; Ng , M.K.M &  Lu, Q. (1999). Te xt seg mentation for Chinese spell c hecking. Journal \\nof the Americ an Society for Information Science , 50, 7 51-75 9.  \\nLehmam, A. (1999).  Text structuration l eading to an a utomatic summary  system : RAFI. \\nInformation Processing & Management ,  35,  181-191  \\nLehtokangas, R. & Jarveli n, K. (2001). Consiste ncy of textual expression in newspaper artic les: \\nan argument for se mantically base query  expansion. Journal of Doc umentation , 57, 535-548  \\nLewis, D.D. & Sparck Jones, K. (1996). Natura l language processi ng for i nformation retrieval. \\nCommunications of the AC M, 39(1), 9 2 – 101   \\nLiddy , E. (1998). Enhanced text retr ieval using nat ural language pr ocessing. Bulletin of the \\nAmerican Society for Information Scien ce, 24, 14- 16.  \\nLiddy, E.; Diamond, T . & McKenna, M (2000). DR-LINK in TIPSTER  III.  Information Retrieva l,     3,  \\n291-311   \\n \\nLovis, C.; Baud , R.; Rassino ux, A.M.; Michel, P.A .& Sc herter, J.R. (1 998). Medical dictionaries for patient \\nencoding system s: a methodology.  Artificia l Intellig ence in  Medicine,  14, 201—214.  \\n \\nMa, Q.; Kanzaki, K.; Murata, M.; Uti yama, M.; Uchi moto, K. &  I sahara, H. Sel f-organizing \\nsemantic maps of Japanese nouns i n terms of adnom inal constituents. In: S. Herath & A. Herat h, \\n(Eds.) Proceedings of the IEEE-INNS-ENNS Interna tional Joi nt Conference on Neural Networks. \\nIJCNN 2000. Neural Computing: New Challe nges and Perspecti ves for the New Millennium . 24-\\n27 Jul y 2000.     Com o,  Italy.  Los Alam itos, CA: IEEE Com put. Soc , ,     pp. 91-96      \\n \\nMagnini, B.; Not, E.; Stoc k, O. & Strappara va, C. (2000). Natural language processing for \\ntransparent c ommunication between pu blic adm inistration and citizens. Artificial Intelligenc e and \\nLaw,    8, 1-34  \\nMani, I.  &  Maybury, M.T. (199 9). Advances in automatic text summarization . Cam bridge, MA: \\nMIT Press  \\n 33 Manning, C. D. & Schutze, H. (199 9). Foundations of statistical natural language processing . \\nCambridge, MA: MIT Press \\nMarquez, L.; Padro, L. &  Rodriguez, H. (200 0). A machine learning approach t o POS taggin g  \\nMachine Learning ,    39, 59-91  \\nMartinez, P.; de Miguel, A.; Cuadra, D.; Nieto,  C. & Castro, E. (2000).  Data conceptual \\nmodelling through natural l anguage: identification and validation of  relationship cardinalities.   \\nChallenges of  Information Technology Manageme nt in the 21st Ce ntury. 200 0 Information \\nResources Management A ssociation Int ernational C onference,     21-24 Ma y 2000,     Anch orage, \\nAK. Hershey , PA: Idea Group Pu blishing . pp. 500-504  \\n \\nMartinez, P. &  Garci a-Serrano, A. (1998) .   A know ledge-based methodology applied to \\nlinguistic eng ineering .  In:  R.N. Horspool (Ed.)  S ystems Implementation 2 000. IFIP TC2 \\nWG2.4 Work ing Conferen ce on S ystems Im plementa tion 2 000: Languages, Methods and T ools,  \\n   23-2 6 Feb. 1998 ,     Berli n. Lon don: Chapman & Hall  pp. 1 66-179      \\nMcMurchie, L.L (1998) Software speak s user’ s language. Computing Can ada, 24, 19-21. \\nMeyer , J.& Dale, R. (1999). Building hybrid knowledge represent ations from text. In: Edwards, J. \\n(ed.),  Proceedings of the 23r d Austr alasian Computer  Science Confer ence. ACSC 2000, IEEE \\nComput . Soc , Lo s Alamit os, CA , pp. 158 -65 \\n  \\nMihalcea, R. & Moldovan, D.I. (1999). Automatic acquisition of sense tagged corpora.  In:  A.N. \\nKumar & I. Russell (Eds.). Proceedings of the Twelfth Interna tional Florida AI Research Society \\nConference,      3-5 Ma y 1999,  Orlando , FL.   Menlo Park, CA: AAAI Press , pp. 29 3-7      \\nMock, K.J. &  Vem uri, V.R. (1997).  Informati on filtering via hi ll clim bing, wordnet and index \\npatterns. Information Processing & Management , 33, 633- 644. \\nMoens, Marie-Francine &  Uyttendaele, Caro line (199 7), Autom atic text structuring and \\ncategorizatio n as a first ste p in summ arizing legal cas es. Information Processing &  Management , \\n33,  727-7 37 \\nMorin, E. ( 1999). Autom atic acquisition of se mantic relations between terms from  technical \\ncorpora. In:  P. Sandrini(E d.). TKE'99. Terminology and Knowledge Engineering. Proceedings \\nFifth Internat ional Congress on Term inology and Knowledge Engi neering .     Innsbruck, Aust ria , \\n23-27 Au g. 1999.     Vienn a: TermNet   pp. 2 68-78  \\n \\nMUC-6 (1996). [Online ] Available : http:// www.cs. nyu.edu/cs/fa culty /grishm an/muc6.ht ml \\nMULINEX: Multilingual Indexing, Navigation and Editing E xtensions for the World Wide Web. \\n[Online] . Available: http:// mulinex.dfki.de/ \\nNarita, M.&  Ogawa, Y. (2 000). T he use of phrases from  query  texts in inform ation retrieval. \\nSIGIR Forum , 34, 318-20  \\nNatural Language Processi ng Laborator y, Universi ty of Massachusetts. [Online]  Available: \\nhttp://www-nlp.cs.umass. edu/nlplic.html \\nNDLTD: N etworked Digital Library  of Theses and D issertations. [Online ] Avail able:   \\nhttp://www.ndltd.org \\n 34 Nerbonne, J.; Dokter, D. & Sm it, P. (1998). Morphological Processi ng and Com puter-Assist ed \\nLanguage Learning.  Com puter Assisted Lan guage Learning , 11, 543-5 9  \\nOard, D. W. (1997).  Servin g users in m any lang uages: cross-language inform ation retrieval for \\ndigital librari es, D-Lib M agazine.  [Online]  Available:  \\nhttp://www.dlib.org/ dlib/decem ber97/oard/12oard.ht ml \\nOard, D. W & Diekama,  A.R. (1998). Cross-langua ge Inform ation Retrieval.  In: M.E. Willi ams \\n(Ed.). Annual Review of Information Scie nce and Tech nology (ARIS T): Volum e 33. Medford,  NJ: \\nLearned Information Inc. for the American So ciety  for Inform ation  Science. pp. 223-2 56   \\nOgura, K.; Nakaiwa, H.; Matsuo, Y.; Ooy ama, Y. &  Bond, F. ( 2000) T he electronic dictio nary. \\nGoi-Taikei-a Japanese l exicon and its applications. NTT Review, 12, 53- 8   \\nOudet, B. (1997). Multil ingualism  on the Internet. Scientific Ameri can, 276 (3), 77-78. \\nOwei, V. (2000) Natural language quer ying of databases: an information extraction appr oach in \\nthe conceptual query  language. International Jo urnal of Human-Co mputer Studie s, 53,  439-9 2 \\nParis, L.A.H. &  Tibbo, H.R. (1998). Fr eesty le vs. Boolean: a co mparison of par tial and exact \\nmatch retriev al systems. Information Pr ocessing & Management,  34, 175-90 \\nParoubek, P. &  Blasband, M. (199 9). Executive Summary  of a Blueprint for a General \\nInfrastructure for Natural Language Processing Sy stems Evaluation  \\nUsing Sem i-Automatic Quantitative Black B ox Approach in a Multilingual Environm ent.  \\n[Online]  Avai lable:  http://www.li msi.fr/TLP/ELSE/Pream bleXwh yXwhatXrev3.htm  \\nPasero, R. & Sabatier, P. (1998)  Lingui stic Ga mes fo r Language L earning: A Special Use of the \\nILLICO Library . Comp uter Assisted La ngua ge Learning , 11, 561 -85  \\n \\nPede rsen, T . &  Bruce, R . (1998).  Knowle dge lean  word-sense disambiguation.  Proceedings Fifteenth \\nNatio nal Conference on  Artifi cial In tellig ence (AAAI-98). Tenth Con ference on Innovative App licatio ns of \\nArtificial In tellig ence.     26-30 July 199 8, Madison. Men lo Park, CA: WI AAAI Press/MI T Press  pp. 800-\\n5  \\n \\nPerez-Ca rballo, J. &  Strzalkowski,  T. (20 00). Natural langu age info rmation  retrie val: p rogress report. \\nInformation Processi ng & Mana gement , 36, 155-178 \\nPeters, C. & Picchi, E. (1997).  Across Languages, Across Cultures: Issues in \\nMultilingu ality and Dig ital Libr aries,  D-Lib Magazine . [Online] Available: \\nhttp://www.dlib.org/dlib/m ay97/peters/05peters.htm l \\nPetreley , N. ( 2000).  Waiting for innovati ons to hi t the mainstream : What about natural language?  \\nInfoWorld, 22(4), 102 \\nPirkola, A. (2 001). M orphological t ypology of lan guag es for IR. Journal of Docu mentation , 57, \\n330-3 48  \\nPoesio, M. & Vieira, R. (1998). A corpus-based investigation of definite description use. \\nComputation al Ling uistics , 24, 183-216 \\n 35 Powell, J. &  Fox, E.A. (1998). Multilin gual federated searching across heterogeneous \\ncollections. D-Lib M agazin e. [Online]  Available: \\nhttp://www.dlib.org/ dlib/septem ber98/powell/09pow ell.htm l \\nQin, J. & Norton, M.J. (E ds.) (1999).  Introduction . Special Issue:  Knowledge discovery  in \\nbibliographic databases. Library Trends , 48, 1-8.  \\nRaghavan, V.V.; Deogun, J .S.; & Server, H. (Eds .) (19 98). Special t opical issue: Knowledge \\ndiscovery  and data mining. Journal of the American S ociety for Information Scie nce, 49(5). \\nRoche,  E. and Shabes, Y. (eds.) (1997). Fi nite-State  Language Pr ocessing (Language, Speech \\nand Comm unication),  MIT Press. \\n \\n \\nRosenfield, R. (2000). Two decades of st atisti cal language modeling: where do we go from  here? \\nProceedings of the IEEE. 88, 8, 1270-8. \\n \\nRoux, M.&  Ledoray , V. (2000)  Unders tanding of m edico-technical reports.  Artificial \\nIntelligence in Medicine,  18, 14 9-72  \\n \\nRuiz, M.E. &  Srinivasan, P. (1998).  Cr oss-La nguage Inform ation Retrieval: an analy sis of \\nerrors. Proceedings of t he 61st ASIS Annual Meeting , Pittsburgh, P A, October 25-29, pp.153- 65 \\nSay, B (1999). Modeling cue phrases in Turkish: a case study .  In: V. Matousek, V. et al (Eds.). \\nText, Speech and Dial ogue . Second Inter national Workshop, TDS'99 Proceedings , 13-17 Sept.  \\n1999,     Plze n, Czech Rep ublic.  Berlin:   Springer-Verlag  pp. 337-40      \\nScarlett, E.; & Szpakowicz, S (2000) .  The pow er of the TSNLP: lessons fro m a diagnostic \\nevaluation of a broad-coverage pa rser.  I n: H.J. Ha milton (Ed.) Advances in Artificial \\nIntelligence. 13th Biennial Conference of the Ca nadian Society for Computational Studies of \\nIntelligence, AI 2000 Proceedings, 14-17 May 20 00, Montreal. Berlin: Sprin ger-Verlag  pp. 1 38-\\n50  \\n \\nSchenker, J.L. (2001). The gist of translation: how long will it be be fore machines make the web \\nmultilingual? Time , 158, Ju ly 16,  2001, 54.  \\nScott, J. (199 9). E-m ail Managem ent: the key to reg aining contr ol.  Internet Business, De cember \\n1999 ,     60— 65   \\nSilber, H.G.& McCoy , K.F. (2000) Effi cient text summarization using lexical chains   In: H.  \\nLieber man(Ed.).  Proceedings of  IUI 2000 Inter national Conference on Intelligent User \\nInterfaces,   9-12 Jan. 20 00, New Orleans, LA. New York: ACM pp . 252- 5    \\nSmeaton A.F. (1999) . Using NLP or NLP Resources f or Inform ation Retrieval Tasks. In: T. \\nStrzalkowski (Ed.), Natura l Lang uage I nformation R etrieval , Klu wer Academ ic Publishers, 99-\\n111,  \\nSmeaton, A.F. (1997).  Inf ormation retrie val: still butti ng heads with natural language \\nprocessing?  In: M.T. Pazienza (Ed.). Information Ext raction. A Multidisciplinar y Approach t o an \\nEmerging Inf ormation Technolo gy Internation al Summer School, S CIE-97 ,   14-18 Jul y 1997, \\n 36 Frascati, Italy . Berlin: Springer-Verlag pp. 115-38  \\n \\nSmith, D. (19 98). Com puterizing Com puter Science. Communicatio ns of the AC M, 41, 21- 23  \\nSokol, L .; Murphy , K.; Brooks, W.& Mattox, D. (2 000).   Visualizing text-based data mining  \\nProceedings of the Fourth International Confer ence on the Practica l Application of Knowledge \\nDiscovery  and Data Mining, 11- 13 Apri l 200 0, Manc hester.  Black pool: Practical Application \\nCompany,  pp. 57-61  \\nSong Jin &  Zhao Dong-Y an (2000) . Study of automatic abstracting based on corpus and \\nhierarchical d ictionary. Journal of S oftware,11, 30 8-14 \\nSparck Jones, K. (1999) . What is the role for NLP in te xt retrieval. In T. Strzalko wski (Ed.). \\nNatural lan guage inf ormation retrieval . Kluwer, pp. 1—25. \\nStaab, S.; Braun, C.; Brude r, I.; Dusterhoft, A.; Heuer, A.; Klettke, M.; Neu mann, G.; Prager, B.; \\nPretzel, J.; Schnurr, H.-P.; Studer, R.; Uszkoreit, H.& Wrenger, B. (1999) GETE SS-searching the \\nWeb exploiti ng Germ an texts.  Cooperative Information Agents  III. Third International \\nWorkshop, CIA'99 Proceedings,  31 July-2 Aug. 1 999,     Upp sala, Sweden.  Ber lin: Springer-\\nVerlag  pp. 1 13-24  \\n \\nStock, O. (20 00). Natural language proc essing and int elligent interfaces. Annals of M athematics \\nand Artificial  Intelligence ,     28,  39-41  \\nStrzalkowski, T.;  Fang, L;  Perez-C arballo, J. & Jin, W. (1997). Natural La nguage Informati on \\nRetrieval TREC-6 Report ,  NIST Special  Publicatio n500-24 0: The Sixth Text REtrieval \\nConference ( TREC 6).  [Online]  Available: http://tre c.nist.gov/ pubs/trec6/t6_pr oceedings.ht ml \\nStrzalkowski, T.;  Perez-Ca rballo, J.; Karlgren, J. ; Hul th, A.  Tapanainen, P.; & Lahtinen, T.  \\n(1999).  Natur al language i nformation re trieval: TREC -8 report. NIST Special P ublication 500-\\n246:The Ei ghth Text REtrieval Conference (TREC 8)     [Online]  Available: \\nhttp://trec.nist .gov/ pubs/tre c8/papers/ge8adhoc2.pdf \\nStrzalkowski, T.; Stein, G.; Wise, G.B.; Perez-C arballo, J; Tapanainen, P.; Jarvinen, T.; \\nVoutilainen, A. & Karlgren, J. (1998). Natural languagee inform ation retrieval: TREC-7 report. \\nNIST Special Publication 500-242: The Seventh Text REtrieval Conference (TR EC 7) [Online]  \\nAvailable: http://trec.nist.gov/pubs/trec7/t7_proceedings.htm l \\nTolle, K.M. &  Chen, H. (2000).  Com paring no un phrasing techniques  for use with medical \\ndigital librar y tools. Journ al of the American Socie ty for Information Science , 51, 352-3 70.  \\nTrybula, W.J.  (1997) . Data mining and knowledge dis covery . In: M.E. William s (Ed.). Annual \\nReview of Informat ion Science and Technology (ARIST): Volume 32. M edford, NJ: Learned Information \\nInc. for the American S ociety  for Information  Science , pp.197-2 29. \\nTsuda, K.&  Nakam ura, M . (1999). The extraction method of the w ord meaning class. In: L.C. \\nJain, (Ed.)  Third Internati onal Conference on Knowl edge-Based Intelligent Inf ormation \\nEngineering Systems.  31 Aug.-1 Sept. 1999, Adelaide, SA, Australia.  Piscataway , NJ:     I EEE , \\npp. 5 34-7   \\n \\n 37  38Twenty-One: development of a multimedia information dissemination and transaction tool. \\n[Online] Available:   http://twentyone.tpd.tno.nl/twentyone/ \\nVickery, B. (1997). Knowledge discovery from databases: an introductory review. Journal of \\nDocumentation , 53, 107-122. \\nVoorhees, E. (1999). The TREC-8 question answering track report. [Online] Available: http://trec.nist.gov/pubs/trec8/papers/qa-report.pdf \\nVoorhees, E. (2000). The TREC-9 question answering track report. [Online] Available: http://trec.nist.gov/pubs/trec9/papers/qa-report.pdf \\nWaldrop, M.M (2001). Natural language processing, Technology Review , 104, 107-108 \\nWarner, A. J. (1987). Natural language processi ng. In: Williams, Martha E. ed. Annual Review of \\nInformation Science and Technology (ARIST): Volume 22. Amsterdam, The Netherlands: Elsevier Science \\nPublishers B.V. for the American Society for Information Science, 79-108. \\nWeigard, H.& Hoppenbrouwers, S. (1998). Experiences with a multilingual ontology-based \\nlexicon for news filtering. In: A.M. Tjoa & R.R.  Wagner (Eds.). Proceedings Ninth International \\nWorkshop on Database and Expert Systems Applications,  26-28 Aug. 1998, Vienna. Los \\nAlamitos, CA: IEEE Computer Society  pp. 160-5   \\nWilks, Y. (1996). Natural language processing, Communications of the ACM , 39, 60 \\nYang, Y. & Liu, X (1999). A re-examination of text categorization methods. In: SIGIR ’99 Proceedings of the 22\\nnd Annual International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval. ACM, pp. 42-49. \\nZadrozny, W.; Budzikowska, M.; Chai, J.& Kambha tla, N. (2000).  Natural language dialogue for \\npersonalized interaction. Communications of the ACM, 43, 116-120.  \\n \\nZweigenbaum, P.& Grabar, N. (1999) Automa tic acquisition of morphological knowledge for \\nmedical language processing.  In: W. Horn, et al (Eds.). Artificial Intelligence in Medicine. Joint \\nEuropean Conference on Artificial Intelligence in Medicine and Medical Decision Making, \\nAIMDM'99  Proceedings, 20-24 June 1999, Aalborg, Denmark.  Berlin: Springer-Verlag  pp. \\n416-20  \\n \\n \\n \"]],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'title': '자연어처리'}]],\n",
       " 'distances': [[1.1720916528233383]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = 'Natural Language'\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "\n",
    "results['metadatas'][0][0]['title']\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
