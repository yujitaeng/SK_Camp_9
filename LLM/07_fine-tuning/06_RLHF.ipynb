{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 선호에 맞는 시 창작 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions pydantic openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq, BitsAndBytesConfig, GenerationConfig, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import ORPOTrainer, ORPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.trainer.utils import DPODataCollatorWithPadding\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"          # wandb 비활성화\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # 병렬 토크나이저 경고 방지\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 설정 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-LoRA 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 Dataset 변환\n",
    "dataset_path = \"./korean_poetry_dataset.json\"\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    poem_data = json.load(f)\n",
    "\n",
    "processed_data = [{\"topic\": item[\"text\"][\"topic\"], \"poem\":item[\"text\"][\"poem\"]} \n",
    "                  for item in poem_data]\n",
    "\n",
    "train_dataset = Dataset.from_list(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 로드\n",
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 (토큰화 + labels 추가)\n",
    "def preprocess_text(sample):\n",
    "    input_texts = [f\"주제: {t}\\n시: {p}\" for t, p in zip(sample[\"topic\"], sample[\"poem\"])]\n",
    "    model_inputs = tokenizer(\n",
    "                        input_texts, \n",
    "                        padding=\"max_length\", \n",
    "                        max_length=512, \n",
    "                        truncation=True\n",
    "                    )\n",
    "    \n",
    "    model_inputs['labels'] = model_inputs[\"input_ids\"].copy()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    model_inputs['labels'] = [\n",
    "        [(l if l != pad_token_id else -100) for l in label] \n",
    "        for label in model_inputs['labels']\n",
    "    ]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 변환\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_text, \n",
    "    batched=True, \n",
    "    remove_columns=[\"topic\", \"poem\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VRAM 최적화를 위한 4-bit 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 모델 훈련을 위한 준비\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.train()    # 모델 학습 모드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./q_lora_poem\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델로 시 생성\n",
    "qlora_checkpoint = \"./q_lora_poem/checkpoint-xxx\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(qlora_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generate_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"바람\", \"비\", \"노을\", \"달빛\", \"안개\", \"사랑\", \"이별\", \"운명\", \"기다림\", \"후회\", \"추억\", \"시간\", \"청춘\", \"변화\", \"마지막 순간\", \"군중\", \"밤거리\", \"버스\", \"인생\", \"빌딩\", \"사람들\", \"거짓말\", \"욕망\", \"돈\", \"권력\", \"비밀\", \"죽음\", \"희망\", \"동물\", \"자연\", \"도시\", \"바다\", \"산\", \"하늘\", \"별\", \"꽃\", \"나무\", \"강\", \"바위\", \"흙\", \"눈\", \"빗방울\", \"눈물\", \"웃음\"]\n",
    "\n",
    "eval_file = 'rlhf_evaluation_data.json'\n",
    "\n",
    "try:\n",
    "    with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    eval_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 생성을 위한 변수 설정\n",
    "num_batches = 5\n",
    "batch_size = 20\n",
    "total_samples = num_batches * batch_size\n",
    "generated_samples = len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 생성 함수\n",
    "def generate_poem_batch():\n",
    "    batch_data = []\n",
    "\n",
    "    with tqdm(total=batch_size, desc=\"<시 생성 중>\", leave=False) as t:\n",
    "        for _ in range(batch_size):\n",
    "            topic = random.choice(topics)\n",
    "            input_text = f\"주제: {topic}\\n시:\"\n",
    "\n",
    "            start_time = time.time()\n",
    "            poem = generate_pipeline(\n",
    "                                        input_text,\n",
    "                                        max_new_tokens=100,\n",
    "                                        temperature=0.8,\n",
    "                                        top_p=0.9\n",
    "                                    )[0]['generated_text']\n",
    "            end_time = time.time()\n",
    "\n",
    "            gen_time = end_time - start_time\n",
    "            batch_data.append({\n",
    "                \"topic\": topic,\n",
    "                \"poem\": poem,\n",
    "                \"selected\": None\n",
    "            })\n",
    "\n",
    "            # tqdm\n",
    "            t.update(1)\n",
    "\n",
    "            global generated_samples\n",
    "            generated_samples += 1\n",
    "            complete_rate = (generated_samples / total_samples) * 100\n",
    "            remaining_time = ((total_samples - generated_samples) * gen_time) / 60\n",
    "\n",
    "            print(f'\\n{generated_samples}/{total_samples}개 완료 ({complete_rate:.2f}%)')\n",
    "            print(f'- 예상 남은 시간 : {remaining_time:.1f}분')\n",
    "            print('-' * 50)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 생성 및 json 저장\n",
    "for _ in tqdm(range(num_batches), desc=\"<전체 진행 상황>\", position=0):\n",
    "    eval_dataset.extend(generate_poem_batch())\n",
    "\n",
    "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(eval_dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward model\n",
    "- 앞서 생성한 시에 대해서 selected=true로 수정해 피드백 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 시+선호도 파일 로드\n",
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "\n",
    "reward_data = [\n",
    "    {'text_a': f'주제: {item[\"topic\"]}', 'text_b': item['poem']}\n",
    "    for item in evaluation_data if item['selected']\n",
    "]\n",
    "\n",
    "reward_dataset = Dataset.from_list(reward_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 (배치 데이터 처리)\n",
    "def preprocess_reward_data(sample):    \n",
    "    model_inputs = tokenizer(\n",
    "                        sample[\"text_a\"],\n",
    "                        text_pair=sample[\"text_b\"],\n",
    "                        padding=\"max_length\", \n",
    "                        max_length=512, \n",
    "                        truncation=True\n",
    "                    )\n",
    "    \n",
    "    model_inputs['labels'] = model_inputs[\"input_ids\"].copy()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    model_inputs['labels'] = [\n",
    "        [(l if l != pad_token_id else -100) for l in label] \n",
    "        for label in model_inputs['labels']\n",
    "    ]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "reward_dataset = reward_dataset.map(\n",
    "    preprocess_reward_data,\n",
    "    batched=True,\n",
    "    remove_columns=['text_a', 'text_b']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VRAM 최적화를 위한 4-bit 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 생성\n",
    "reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "reward_model = prepare_model_for_kbit_training(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 적용\n",
    "reward_model = get_peft_model(reward_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정\n",
    "reward_training_args = TrainingArguments(\n",
    "    output_dir=\"./reward_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "reward_trainer = Trainer(\n",
    "    model=reward_model,\n",
    "    args=reward_training_args,\n",
    "    train_dataset=reward_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLHF (ORPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(qlora_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "# 파라미터 학습 가능하게끔 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch CUDA 메모리 관리 최적화 (OOM 방지)\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO 데이터셋 준비\n",
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "\n",
    "orpo_data = []\n",
    "\n",
    "for item in evaluation_data:\n",
    "    if item['selected']:\n",
    "        prompt_text = f'주제: {item[\"topic\"]}\\n이 주제에 맞는 시를 작성해 주세요.'\n",
    "        chosen_text = item['poem']\n",
    "        rejected_text = \"\"\n",
    "\n",
    "        tokenized_prompt = tokenizer(prompt_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        tokenized_chosen = tokenizer(chosen_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        tokenized_rejected = tokenizer(rejected_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "        orpo_data.append({\n",
    "            \"prompt\": prompt_text,\n",
    "            \"chosen\": chosen_text,\n",
    "            \"rejected\": rejected_text,\n",
    "            \"prompt_input_ids\": tokenized_prompt['input_ids'].squeeze(0).cuda(),\n",
    "            \"prompt_attention_mask\": tokenized_prompt['attention_mask'].squeeze(0).cuda(),\n",
    "            \"chosen_input_ids\": tokenized_chosen['input_ids'].squeeze(0).cuda(),\n",
    "            \"chosen_attention_mask\": tokenized_chosen['attention_mask'].squeeze(0).cuda(),\n",
    "            \"rejected_input_ids\": tokenized_rejected['input_ids'].squeeze(0).cuda(),\n",
    "            \"rejected_attention_mask\": tokenized_rejected['attention_mask'].squeeze(0).cuda(),\n",
    "        })\n",
    "\n",
    "        orpo_dataset = Dataset.from_list(orpo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO 설정\n",
    "orpo_config = ORPOConfig(\n",
    "    output_dir='./orpo_output',\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-6,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터 설정\n",
    "data_collator = DPODataCollatorWithPadding(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    label_pad_token_id=-100,\n",
    "    is_encoder_decoder=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPOTrainer 초기화\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_config,\n",
    "    train_dataset=orpo_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "orpo_trainer.train()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_checkpoint = './orpo_output/checkpoint-xxx'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(orpo_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generate_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem_final(num_samples=5):\n",
    "    topics = [\"바람\", \"비\", \"노을\", \"달빛\", \"안개\", \"사랑\", \"이별\", \"운명\", \"기다림\", \"후회\", \"추억\", \"시간\", \"청춘\", \"변화\", \"마지막 순간\", \"군중\", \"밤거리\", \"버스\", \"인생\", \"빌딩\", \"사람들\", \"거짓말\", \"욕망\", \"돈\", \"권력\", \"비밀\", \"죽음\", \"희망\", \"동물\", \"자연\", \"도시\", \"바다\", \"산\", \"하늘\", \"별\", \"꽃\", \"나무\", \"강\", \"바위\", \"흙\", \"눈\", \"빗방울\", \"눈물\", \"웃음\"]\n",
    "    result = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        topic = random.choice(topics)\n",
    "        input_text = f'주제: {topic}\\n시:'\n",
    "        poem = generate_pipeline(\n",
    "                                    input_text,\n",
    "                                    max_new_tokens=100,\n",
    "                                    temperature=0.8,\n",
    "                                    top_p=0.9\n",
    "                                )[0]['generated_text']\n",
    "        result.append({\"topic\": topic, \"poem\": poem})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_poem = generate_poem_final(num_samples=10)\n",
    "generated_poem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
