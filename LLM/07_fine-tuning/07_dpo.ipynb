{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,PeftModel\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"hf_xxx\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화를 적용한 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 준비\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"mncai/orca_dpo_pairs_ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_text(sample):\n",
    "    input_enc = tokenizer(sample[\"question\"], padding=\"max_length\", max_length=256, truncation=True)\n",
    "    preferred_enc = tokenizer(sample[\"chosen\"], padding=\"max_length\", max_length=256, truncation=True)\n",
    "    dispreferred_enc = tokenizer(sample[\"rejected\"], padding=\"max_length\", max_length=256, truncation=True)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"preferred_ids\": preferred_enc[\"input_ids\"],\n",
    "        \"dispreferred_ids\": dispreferred_enc[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 토큰화 및 pytorch 텐서 형식으로 변환\n",
    "tokenized_dataset = dataset[\"train\"].map(\n",
    "    preprocess_text,\n",
    "    remove_columns=[\"id\", \"system\", \"question\", \"chosen\", \"rejected\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"preferred_ids\", \"dispreferred_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 배치 구성 함수\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"].clone().detach() for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"].clone().detach() for item in batch])\n",
    "\n",
    "    max_length = max(max(len(item['preferred_ids']) for item in batch), 1)\n",
    "\n",
    "    preferred_ids = torch.stack([\n",
    "        torch.tensor(\n",
    "            item['preferred_ids'].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['preferred_ids'])),\n",
    "            dtype=torch.long\n",
    "        ) if isinstance(item['preferred_ids'], torch.Tensor) else\n",
    "        torch.tensor(\n",
    "            item['preferred_ids'] + [tokenizer.pad_token_id] * (max_length - len(item['preferred_ids'])),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        for item in batch\n",
    "    ]).clone().detach()\n",
    "\n",
    "    dispreferred_ids = torch.stack([\n",
    "        torch.tensor(\n",
    "            item['dispreferred_ids'].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['dispreferred_ids'])),\n",
    "            dtype=torch.long\n",
    "        ) if isinstance(item['dispreferred_ids'], torch.Tensor) else\n",
    "        torch.tensor(\n",
    "            item['dispreferred_ids'] + [tokenizer.pad_token_id] * (max_length - len(item['dispreferred_ids'])),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        for item in batch\n",
    "    ]).clone().detach()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attetion_mask\": attention_mask,\n",
    "        \"preferred_ids\": preferred_ids,\n",
    "        \"dispreferred_ids\": dispreferred_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, beta=0.1, *args, **kwargs):\n",
    "        input_ids = inputs['input_ids'].to(model.device)\n",
    "        attention_mask = inputs['attention_mask'].to(model.device)\n",
    "        preferred_ids = inputs['preferred_ids'].to(model.device)\n",
    "        dispreferred_ids = inputs['despreferred_ids'].to(model.device)\n",
    "\n",
    "        preferred_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=preferred_ids)\n",
    "        dispreferred_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=dispreferred_ids)\n",
    "\n",
    "        preferred_loss = preferred_outputs.loss\n",
    "        dispreferred_loss = dispreferred_outputs.loss\n",
    "\n",
    "        loss = -F.logsigmoid(beta * (dispreferred_loss - preferred_loss)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./dpo_llama3_korean',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    optim='adamw_bnb_8bit',\n",
    "    max_grad_norm=0\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './dpo_llama3_korean/checkpoint-xxx'\n",
    "\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "sample_data = dataset['train'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, \n",
    "                       truncation=True, max_length=256).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(sample_data):\n",
    "    question = example['question']\n",
    "    preferred_answer = example['chosen']\n",
    "\n",
    "    generated_response = generate_response(question)\n",
    "\n",
    "    print(f\"질문: {question}\")\n",
    "    print(f\"정답 (선호 응답): {preferred_answer}\")\n",
    "    print(f\"실제 모델 응답: {generated_response}\")\n",
    "    print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
