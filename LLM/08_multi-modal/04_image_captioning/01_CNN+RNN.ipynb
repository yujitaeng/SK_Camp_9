{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JJ5gB_4FEPr_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vgg16\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어사전\n",
        "vocab = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"a\", 4: \"dog\", 5: \"is\", 6: \"sitting\", 7: \"on\", 8: \"grass\"}\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n"
      ],
      "metadata": {
        "id": "ez64j2zvFjyF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "image = Image.open('test.jpg').convert('RGB')\n",
        "image_tensor = transform(image).unsqueeze(0)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "orXu0JdLGwHM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16\n",
        "vgg = vgg16(pretrained=True).features\n",
        "for param in vgg.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "with torch.no_grad():\n",
        "  features = vgg(image_tensor)\n",
        "  features = features.view(features.size(0), -1).unsqueeze(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkowVZaTITgG",
        "outputId": "e560e86e-35b7-4ce3-e33e-4f717c508722"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 69.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caption = [1, 3, 4, 5, 6, 7, 8, 2]\n",
        "input_seq = torch.tensor([caption[:-1]])\n",
        "target_seq = torch.tensor([caption[1:]])"
      ],
      "metadata": {
        "id": "btf5T5XTLkId"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionGenerator(nn.Module):\n",
        "  def __init__(self, feature_dim, embed_dim, hidden_dim, vocab_size):\n",
        "    super(CaptionGenerator, self).__init__()\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "    self.init_linear = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embedded_feats = self.init_linear(features)\n",
        "    embeds = self.embed(captions)\n",
        "    inputs = torch.cat((embedded_feats, embeds), dim=1)\n",
        "    hiddens, _ = self.lstm(inputs)\n",
        "    outputs = self.decoder(hiddens)\n",
        "    return outputs[:, 1:, :]\n"
      ],
      "metadata": {
        "id": "1F1piMqfL5L0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "model = CaptionGenerator(feature_dim=25088, embed_dim=256, hidden_dim=512, vocab_size=len(vocab))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  outputs = model(features, input_seq)\n",
        "  loss = criterion(outputs.squeeze(0), target_seq.squeeze(0))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "QpDzMJerOFjM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  generated = []\n",
        "  input_word = torch.tensor([[1]])\n",
        "  embed_feat = model.init_linear(features)\n",
        "  hidden = None\n",
        "\n",
        "  for _ in range(10):\n",
        "    embed_input = model.embed(input_word)\n",
        "    lstm_input = torch.cat((embed_feat, embed_input), dim=1) if len(generated) == 0 else embed_input\n",
        "    out, hidden = model.lstm(lstm_input, hidden)\n",
        "    pred = model.decoder(out[:, -1, :])\n",
        "    pred_id = pred.argmax(dim=-1).item()\n",
        "\n",
        "    if pred_id == 2:\n",
        "      break\n",
        "\n",
        "    generated.append(pred_id)\n",
        "    input_word = torch.tensor([[pred_id]])\n",
        "    embed_feat = None\n",
        "\n",
        "  sentence = ' '.join([vocab[idx] for idx in generated])\n",
        "  print('생성된 캡션: ', sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUF0ToqFPiqv",
        "outputId": "c8b96b76-9bf2-4b92-e867-55877a2cf5b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 캡션:  a dog is sitting on grass\n"
          ]
        }
      ]
    }
  ]
}