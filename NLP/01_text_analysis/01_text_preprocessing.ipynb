{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 NLP | 텍스트 분석 Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리 : 사람이 사용하는 언어 전반에 대해서 이해하고, 처리하는 분야\n",
    "    - 음성인식, 번역, 감정분석, 요약, 질의응답, 언어생성 등 포괄적 분야  \n",
    "- 텍스트 분석 : 언어적 비정형 데이터에서 정보를 추출하고, 분석하는 작업 \n",
    "    - 텍스트 통계적분석, 주제분류, 텍스트 군집, 유사도분석 등   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 텍스트분석 패키지 \n",
    "\n",
    "| **로고 이미지**                                                                                                 | **패키지**   | **설명**                                            | **주요 특징 및 기능**                                                   | **API 문서 URL**                                  |\n",
    "|------------------------------------------------------------------------------------------------------------|--------------|---------------------------------------------------|-----------------------------------------------------------------------|-------------------------------------------------|\n",
    "| ![nltk](https://images.javatpoint.com/tutorial/ai/images/natural-language-toolkit2.png)                   | **nltk**     | 가장 오래된 NLP 라이브러리 중 하나로, 다양한 자연어 처리 도구와 코퍼스 제공 | 토큰화, 품사 태깅, 어간 추출, 불용어 제거, 문법 구조 분석, 감정 분석 등에 유용 | [NLTK API Docs](https://www.nltk.org/api/nltk.html) |\n",
    "| ![gensim](https://radimrehurek.com/gensim/_static/images/gensim.png)                                       | **gensim**   | 주로 텍스트의 토픽 모델링과 문서 유사도 분석을 위한 라이브러리            | Word2Vec, FastText, LDA, 유사도 측정, 대용량 텍스트 처리에 최적화    | [Gensim API Docs](https://radimrehurek.com/gensim/) |\n",
    "| ![spacy](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/320px-SpaCy_logo.svg.png) | **spacy**    | 빠르고 효율적인 NLP 처리를 위해 개발된 라이브러리로, 산업용 프로젝트에 적합     | 빠른 토큰화, 품사 태깅, NER, 구문 분석, 벡터 표현 제공              | [SpaCy API Docs](https://spacy.io/api)             |\n",
    "| ![TextBlob](https://textblob.readthedocs.io/en/dev/_static/textblob-logo.png)                              | **TextBlob** | 간단한 NLP 작업을 위한 라이브러리로, 감정 분석과 텍스트 정제 등 지원  | 문법 교정, 감정 분석, 텍스트 번역 등과 같은 간단한 작업에 적합      | [TextBlob API Docs](https://textblob.readthedocs.io/en/dev/) |\n",
    "| ![KoNLPy](https://konlpy.org/en/latest/_static/konlpy.png)                                                 | **KoNLPy**   | 한국어 자연어 처리를 위한 라이브러리로, 여러 형태소 분석기를 제공          | Kkma, Hannanum, Komoran, Twitter, Mecab 형태소 분석기 지원            | [KoNLPy API Docs](https://konlpy.org/en/latest/)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit)\n",
    "- 파이썬에서 텍스트 처리 및 자연어 처리를 쉽게 다룰 수 있게 해주는 오픈 소스 라이브러리\n",
    "- NLTK는 다양한 언어 리소스와 알고리즘을 포함하고 있으며, 텍스트 마이닝, 텍스트 분석, 그리고 자연어 처리를 공부하거나 구현할 때 유용\n",
    "\n",
    "**주요 기능**\n",
    "1. **토큰화(Tokenization)**: 문장을 단어 또는 문장 단위로 나누는 작업\n",
    "    - 예를 들어, `\"I love NLP.\"`를 `['I', 'love', 'NLP', '.']`와 같이 나누는 기능을 제공한다.\n",
    "2. **품사 태깅(Part-of-Speech Tagging)**: 각 단어에 대해 해당 품사를 태깅하는 작업\n",
    "    - 예를 들어, `\"I love NLP.\"`에 대해 `[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('.', '.')]`와 같이 태깅한다.\n",
    "3. **명사구 추출(Chunking)**: 문장에서 명사구와 같은 특정 구문을 추출하는 작업\n",
    "4. **어근 추출(Lemmatization) 및 어간 추출(Stemming)**: 단어의 기본 형태를 찾는 작업으로, 동사의 기본형을 찾거나 복수형을 단수형으로 변환하는 등의 작업 수행\n",
    "5. **텍스트 분류(Classification)**: Naive Bayes, MaxEnt 등의 분류 모델을 사용해 텍스트 분류 가능\n",
    "6. **코퍼스(corpus) 제공**: 영화 리뷰, 뉴스 기사 등 여러 텍스트 데이터셋을 포함하고 있어 학습과 실습에 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install nltk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.__version__\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = 'NLTK is a powerful library for NLP!!!!!'\n",
    "word_tokenize(text)    # 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "sent_tokenize(text)    # 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별 단어 토큰화\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "result = tokenize_text(text)\n",
    "print(len(result), len(result[0]), len(result[1]), len(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram\n",
    "from nltk import ngrams\n",
    "\n",
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "bigram = ngrams(tokens, 2)\n",
    "print([token for token in bigram])\n",
    "\n",
    "trigram = ngrams(tokens, 3)\n",
    "print([token for token in trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어(stopwords) 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stopwords.fileids()\n",
    "\n",
    "print(len(stopwords.words('english')))\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "tokens = []\n",
    "for word in word_tokenize(text):    # 토큰화\n",
    "    word = word.lower()    # 소문자 변환\n",
    "    if word not in stopwords_list:    # 불용어 처리\n",
    "        tokens.append(word)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특성 벡터화 Feature Vectorization\n",
    "1. BOW(Bag of Words): 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델이다.\n",
    "    \n",
    "   <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*S8uW62e3AbYw3gnylm4eDg.png\" width=\"500px\">\n",
    "\n",
    "2. Word Embedding: 단어를 밀집 벡터(dense vector)로 표현하는 방법으로, 단어의 의미와 관계를 보존하며 벡터로 표현한다.\n",
    "    \n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jpnKO5X0Ii8PVdQYFO2z1Q.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "| **구분**             | **Bag-of-Words (BOW)**                             | **Word Embedding**                             |\n",
    "|----------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **개념**             | 문서를 단어의 출현 빈도로 표현                   | 단어를 실수 벡터로 표현                       |\n",
    "| **특징**             | - 단어의 순서와 의미를 고려하지 않음             | - 단어 간 의미적 유사성을 반영                |\n",
    "|                      | - 고차원, 희소 벡터 생성                         | - 밀집된 저차원 벡터 생성                     |\n",
    "| **대표 방법**        | Count Vector, TF-IDF                             | Word2Vec, GloVe, FastText                     |\n",
    "| **장점**             | - 구현이 간단하고 이해하기 쉬움                  | - 문맥 정보 반영 가능                         |\n",
    "|                      | - 단순 텍스트 데이터 분석에 유용                 | - 유사한 단어를 벡터 공간 상에서 가깝게 위치 |\n",
    "| **단점**             | - 의미적 관계와 단어의 순서 정보 없음            | - 많은 데이터와 학습 시간 필요                |\n",
    "|                      | - 고차원 희소 벡터 문제                          | - 구현이 상대적으로 복잡                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BOW - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text1 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "text2 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe \\\n",
    "You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "\n",
    "texts = [text1, text2]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(texts)\n",
    "text_vecs = count_vectorizer.transform(texts)\n",
    "\n",
    "print(text_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 등장 횟수 구하기\n",
    "word_counts = text_vecs.toarray().sum(axis=0)\n",
    "\n",
    "# 단어 데이터프레임에 빈도 추가\n",
    "vocab_df['count'] = vocab_df['idx'].apply(lambda i: word_counts[i])\n",
    "\n",
    "# idx 열 제거 (불필요함)\n",
    "vocab_df = vocab_df.drop(columns=['idx'])\n",
    "\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer(stop_words='언어') : 불용어 제거\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'believe', 'pill', 'rabbit hole', 'red', 'red pill', 'room',\n",
       "       'room window', 'stay', 'stay wonderland', 'story', 'story ends',\n",
       "       'television', 'television feel', 'wake', 'wake bed', 'want',\n",
       "       'want believe', 'window', 'window television'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),    # n-gram 범위 지정 (최소값, 최대값) / 기본값 (1, 1)\n",
    "    max_features=20    # 빈도수 상위인 n개의 데이터 사용\n",
    ")\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)\n",
    "\n",
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BOW - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF == Term Frequency-Inverse Document Frequency\n",
    "\n",
    "**용어** \n",
    "- $tf(t, d)$: 특정 단어 $t$가 문서 $d$에서 등장한 횟수 (Term Frequency)\n",
    "- $df(t)$: 특정 단어 $t$가 등장한 문서의 수 (Document Frequency)\n",
    "- $N$: 전체 문서의 수\n",
    "\n",
    "**TF (Term Frequency)**\n",
    "- 단어 $t$의 문서 $d$에서의 빈도를 계산하는데, 가장 일반적인 방법은 해당 단어의 단순 빈도로 정의한다.\n",
    "\n",
    "$\n",
    "tf(t, d) = \\frac{\\text{단어 } t \\text{의 문서 } d \\text{ 내 등장 횟수}}{\\text{문서 } d \\text{의 전체 단어 수}}\n",
    "$\n",
    "\n",
    "**IDF (Inverse Document Frequency)**\n",
    "- 단어가 전체 문서에서 얼마나 중요한지를 계산한다.\n",
    "- 특정 단어가 많은 문서에서 등장하면, 이 단어는 중요도가 낮아진다. 이를 반영하기 위해 아래와 같은 식을 사용한다.\n",
    "\n",
    "$\n",
    "idf(t) = \\log\\left(\\frac{N}{1 + df(t)}\\right)\n",
    "$\n",
    "\n",
    "- 여기서 $1$을 더하는 이유는, 특정 단어가 모든 문서에 등장하지 않을 경우 $df(t) = 0$이 되어, 분모가 $0$이 되는 것을 방지하기 위함이다.\n",
    "  - 예를 들어, $\\log(5/(1+1))$과 $\\log(5/(1+2))$를 계산하면, 각각 $0.3979$와 $0.2218$이 된다.\n",
    "\n",
    "**TF-IDF 계산**\n",
    "- 위의 TF와 IDF를 결합하여 TF-IDF 가중치를 계산한다.\n",
    "\n",
    "$\n",
    "\\text{tf-idf}(t, d) = tf(t, d) \\times idf(t)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfVectorizer의 주요 파라미터**\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "  <tr style=\"background-color: #f2f2f2;\">\n",
    "    <th>Parameter</th>\n",
    "    <th>Description</th>\n",
    "    <th>Default Value</th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #ffeb99;\">\n",
    "    <td><b>max_df</b></td>\n",
    "    <td>문서의 비율 값으로서, 해당 비율 이상 나타나는 단어를 무시한다. <br> 예를 들어, max_df=0.8이면, 80% 이상의 문서에서 나타나는 단어는 제외된다.</td>\n",
    "    <td>1.0</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #ffeb99;\">\n",
    "    <td><b>min_df</b></td>\n",
    "    <td>문서의 비율 값 또는 정수로, 해당 비율 이하 나타나는 단어를 무시한다. <br> 예를 들어, min_df=2이면, 두 개 이하의 문서에서만 나타나는 단어는 제외된다.</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #ffeb99;\">\n",
    "    <td><b>ngram_range</b></td>\n",
    "    <td>(min_n, max_n) 형식으로, 사용할 n-gram의 범위를 정의한다. <br> 예를 들어, (1, 2)로 설정하면 unigram과 bigram을 고려한다.</td>\n",
    "    <td>(1, 1)</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #ffeb99;\">\n",
    "    <td>stop_words</td>\n",
    "    <td>불용어를 지정할 수 있다. \"english\"로 설정하면 영어 불용어를 사용한다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>max_features</td>\n",
    "    <td>벡터화할 때 고려할 최대 단어 수를 설정한다. 빈도순으로 상위 단어들이 선택된다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>use_idf</td>\n",
    "    <td>IDF(역문서 빈도)를 사용할지 여부를 지정한다. False로 설정하면 단순히 TF 값만 사용한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>smooth_idf</td>\n",
    "    <td>IDF 계산 시, 0으로 나누는 것을 피하기 위해 추가적인 smoothing을 수행한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sublinear_tf</td>\n",
    "    <td>TF 값에 대해 sublinear scaling (1 + log(tf))를 적용할지 지정한다.</td>\n",
    "    <td>False</td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "        0.        , 0.33333333, 0.        , 0.        , 0.33333333,\n",
       "        0.33333333, 0.        , 0.        , 0.        , 0.33333333,\n",
       "        0.        , 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "        0.        , 0.33333333, 0.        , 0.33333333],\n",
       "       [0.21821789, 0.43643578, 0.21821789, 0.        , 0.21821789,\n",
       "        0.21821789, 0.        , 0.21821789, 0.21821789, 0.        ,\n",
       "        0.        , 0.43643578, 0.21821789, 0.21821789, 0.        ,\n",
       "        0.21821789, 0.21821789, 0.        , 0.        , 0.21821789,\n",
       "        0.21821789, 0.        , 0.21821789, 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'believe', 'blue', 'church', 'deep', 'ends', 'feel', 'goes',\n",
       "       'hole', 'matrix', 'pay', 'pill', 'rabbit', 'red', 'room', 'stay',\n",
       "       'story', 'taxes', 'television', 'wake', 'want', 'window',\n",
       "       'wonderland', 'work'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "texts_vecs = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "display(texts_vecs.toarray())\n",
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
