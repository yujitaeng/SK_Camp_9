{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰화 목적**\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fascinating. It has many applications in real-world scenarios.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n",
      "['NLP is fascinating.', 'It has many applications in real-world scenarios.']\n",
      "['NLP', 'is', 'fascinating', '.']\n",
      "['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 단어 토큰화\n",
    "print(nltk.word_tokenize(text))\n",
    "\n",
    "# 문장 토큰화\n",
    "print(nltk.sent_tokenize(text))\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword 토큰화\n",
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 => 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BertTokenizer.from_pretrained('bert-base-uncased')로 로드된 설정 파일 설명\n",
    "1. **`tokenizer_config.json`**  \n",
    "   - 토크나이저의 기본 설정(예: `do_lower_case=True`, `model_max_length=512`)\n",
    "   - 토크나이저가 어떻게 동작해야 하는지 정의\n",
    "\n",
    "2. **`vocab.txt`**  \n",
    "   - BERT의 사전(vocabulary) 파일로, 모델이 인식하는 단어 목록을 포함\n",
    "   - 예: `'unhappiness'` 같은 단어가 없으면 `'un'`, `'##happiness'` 같은 서브워드로 분리\n",
    "\n",
    "3. **`tokenizer.json`**  \n",
    "   - 단어와 서브워드 토큰을 매핑하는 JSON 파일로, WordPiece 토크나이저 정보 포함\n",
    "\n",
    "4. **`config.json`**  \n",
    "   - 모델의 주요 설정값(예: hidden size, attention heads)을 포함하는 파일로, 토크나이저보다는 모델 가중치와 관련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 분리 과정\n",
    "1. WordPiece 토크나이저는 사전에 등록된 단어 우선 사용\n",
    "    - Bert의 WordPiece 토크나이저는 최대한 긴 단어를 먼저 찾으려는 방식으로 동작\n",
    "    - 'happy'가 사전에 있으면 그대로 사용, 없으면 가장 긴 서브워드 조합 찾아 나눔\n",
    "2. 'happy'가 사전에 포함되지 않은 경우\n",
    "    - 일반적으로 'happy'는 BERT의 사전에 포함되어 있지만, 학습된 모델에 없을 수 있음\n",
    "    - 'happy'가 사전에 없는 경우 사전에 있는 조각(서브워드)로 분해\n",
    "3. 그래서 과정은...\n",
    "    1. \"unhappiness\" 검색 -> 없음\n",
    "    2. \"un\" 검색 -> 있음 -> 유지\n",
    "    3. \"happiness\" 검색 -> 없음\n",
    "    4. \"ha\" 검색 -> 있음\n",
    "    5. \"pp\" 검색 -> 있음\n",
    "    6. \"iness\" 검색 -> 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 주의사항\n",
    "1. 구두점이나 특수 문자를 단순 제외하면 안됨\n",
    "2. 줄임말, 단어 내 띄어쓰기 유의\n",
    "\n",
    "**표준 토큰화 예제 Penn Treebank Tokenization**\n",
    "- 규칙 1: 하이픈으로 구성된 단어는 하나로 유지한다.\n",
    "- 규칙 2: doesn't 와 같이 '가 있는 단어는 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점을 제외한 단어 토큰화\n",
    "import re\n",
    "\n",
    "text = \"Time flies like an arrow; fruit flies like a banana.\"\n",
    "re.findall(r'\\b\\w+\\b', text)\n",
    "# r'' : 파이썬 raw text (이스케이핑 문자를 문자 그대로 처리)\n",
    "# \\b : 경계 문자 (boundary / 공백, 구두점, ...)\n",
    "# \\w : 글자 (word / 영문자, 숫자, _)\n",
    "# \\w+ : 수량자 (하나 이상)\n",
    "# => 경계 문자로 감싸진 하나 이상의 글자를 전부 찾아라~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "# 단어/구두점으로 토큰을 구분 (', - 포함 단어도 분리)\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜', '표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다. \\\n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/02/18 날짜 표현에 사용될 수 있다. \\\n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "print(treebank_word_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kss==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['배경은 1920년대의 경성부이다.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문.\",\n",
       " '멍청이, 꼰대...가 아닐수 없다.',\n",
       " '사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kss (Korean Sentence Splitter)\n",
    "# 한국어 문장 또는 한국어/영어 혼합 문장 등에 문장단위 토큰 생성 지원\n",
    "import kss\n",
    "\n",
    "text = \"배경은 1920년대의 경성부이다. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문. 멍청이, 꼰대...가 아닐수 없다. 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pos_tag**\n",
    "\n",
    "pos_tag는 자연어 처리(NLP)에서 단어에 품사를 태깅하는 함수로, 주로 NLTK(Natural Language Toolkit)와 같은 라이브러리에서 사용된다. <br>\n",
    "\n",
    "- nltk pos_tag() 주요 품사 태깅<br>\n",
    "\n",
    "1. **NN (Noun, Singular)**  \n",
    "   단수 명사를 나타낸다. 하나의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cat\", \"book\", \"apple\"\n",
    "\n",
    "2. **NNS (Noun, Plural)**  \n",
    "   복수 명사를 나타낸다. 두 개 이상의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cats\", \"books\", \"apples\"\n",
    "\n",
    "3. **NNP (Proper Noun, Singular)**  \n",
    "   단수 고유 명사를 나타낸다. 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Alice\", \"London\", \"NASA\"\n",
    "\n",
    "4. **NNPS (Proper Noun, Plural)**  \n",
    "   복수 고유 명사를 나타낸다. 두 개 이상의 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Smiths\", \"United Nations\"\n",
    "\n",
    "5. **VB (Verb, Base Form)**  \n",
    "   동사의 원형을 나타낸다. 일반적으로 현재 시제와 함께 사용된다.  \n",
    "   예시: \"run\", \"eat\", \"play\"\n",
    "\n",
    "6. **VBD (Verb, Past Tense)**  \n",
    "   동사의 과거형을 나타낸다.  \n",
    "   예시: \"ran\", \"ate\", \"played\"\n",
    "\n",
    "7. **VBG (Verb, Gerund or Present Participle)**  \n",
    "   동명사 또는 현재 분사를 나타낸다. 일반적으로 \"-ing\" 형태이다.  \n",
    "   예시: \"running\", \"eating\", \"playing\"\n",
    "\n",
    "8. **VBN (Verb, Past Participle)**  \n",
    "   동사의 과거 분사형을 나타낸다. 주로 완료 시제와 함께 사용된다.  \n",
    "   예시: \"run\" (as in \"has run\"), \"eaten\", \"played\"\n",
    "\n",
    "9. **VBZ (Verb, 3rd Person Singular Present)**  \n",
    "   3인칭 단수 현재형 동사를 나타낸다. 주어가 3인칭 단수일 때 사용된다.  \n",
    "   예시: \"runs\", \"eats\", \"plays\"\n",
    "\n",
    "10. **JJ (Adjective)**  \n",
    "    형용사를 나타낸다. 명사를 수식하여 그 특성을 설명한다.  \n",
    "    예시: \"big\", \"blue\", \"happy\"\n",
    "\n",
    "11. **JJR (Adjective, Comparative)**  \n",
    "    비교급 형용사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"bigger\", \"bluer\", \"happier\"\n",
    "\n",
    "12. **JJS (Adjective, Superlative)**  \n",
    "    최상급 형용사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"biggest\", \"bluest\", \"happiest\"\n",
    "\n",
    "13. **RB (Adverb)**  \n",
    "    부사를 나타낸다. 동사, 형용사 또는 다른 부사를 수식한다.  \n",
    "    예시: \"quickly\", \"very\", \"well\"\n",
    "\n",
    "14. **RBR (Adverb, Comparative)**  \n",
    "    비교급 부사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"more quickly\", \"better\"\n",
    "\n",
    "15. **RBS (Adverb, Superlative)**  \n",
    "    최상급 부사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"most quickly\", \"best\"\n",
    "\n",
    "16. **IN (Preposition or Subordinating Conjunction)**  \n",
    "    전치사 또는 종속 접속사를 나타낸다. 명사와의 관계를 나타내거나 종속절을 시작한다.  \n",
    "    예시: \"in\", \"on\", \"because\"\n",
    "\n",
    "17. **DT (Determiner)**  \n",
    "    한정사를 나타낸다. 명사의 수와 상태를 정의한다.  \n",
    "    예시: \"the\", \"a\", \"some\"\n",
    "\n",
    "18. **PRP (Personal Pronoun)**  \n",
    "    인칭 대명사를 나타낸다. 사람, 사물 등을 대체할 때 사용된다.  \n",
    "    예시: \"I\", \"you\", \"he\", \"they\"\n",
    "\n",
    "19. **PRP$ (Possessive Pronoun)**  \n",
    "    소유 대명사를 나타낸다. 소유 관계를 나타낸다.  \n",
    "    예시: \"my\", \"your\", \"his\", \"their\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Time flies like an arrow.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spacy 주요 품사 태깅**\n",
    "\n",
    "| 태그 | 설명                 | 예시                 |\n",
    "|------|----------------------|----------------------|\n",
    "| ADJ  | 형용사               | big, nice           |\n",
    "| ADP  | 전치사               | in, to, on          |\n",
    "| ADV  | 부사                 | very, well          |\n",
    "| AUX  | 조동사               | is, have (조동사로 사용될 때) |\n",
    "| CONJ | 접속사               | and, or             |\n",
    "| DET  | 한정사/관사          | the, a              |\n",
    "| INTJ | 감탄사               | oh, wow             |\n",
    "| NOUN | 명사                 | dog, table          |\n",
    "| NUM  | 숫자                 | one, two, 3         |\n",
    "| PART | 소사                 | 'to' (to fly에서), not |\n",
    "| PRON | 대명사               | he, she, it         |\n",
    "| PROPN| 고유명사             | John, France        |\n",
    "| PUNCT| 구두점               | ., !, ?             |\n",
    "| SCONJ| 종속 접속사          | because, if         |\n",
    "| SYM  | 기호                 | $, %, @             |\n",
    "| VERB | 동사                 | run, eat            |\n",
    "| X    | 알 수 없는 품사       | 외국어 단어, 잘못된 형식 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# spacy : 자연어처리 지원 라이브러리\n",
    "import spacy\n",
    "\n",
    "spacy.cli.download('en_core_web_sm')      # 직접 다운로드 (사용 전 1회는 반드시 다운로드)\n",
    "spacy_nlp = spacy.load('en_core_web_sm')  # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 형태소 분석\n",
    "  1. Hannanum: 한국어 형태소 분석기\n",
    "  2. Kkma: 한국어 형태소 분석기\n",
    "  3. Mecab: 빠르고 효율적인 형태소 분석기\n",
    "  4. Komoran: 빠른 속도의 형태소 분석기\n",
    "  5. Okt (Open Korean Text): Twitter에서 사용되던 형태소 분석기\n",
    "\n",
    "- 품사 태깅\n",
    "\n",
    "  1. **N (명사)**  \n",
    "    일반적으로 명사를 나타낸다. 사물, 사람, 개념 등을 지칭한다.\n",
    "\n",
    "  2. **NNG (일반 명사)**  \n",
    "    고유명사가 아닌 일반 명사를 나타낸다. 예를 들어, \"사람\", \"책\" 등이 해당된다.\n",
    "\n",
    "  3. **J (조사)**  \n",
    "    명사 뒤에 붙어 그 명사의 역할을 나타내는 조사이다. 예: \"이\", \"가\", \"을\" 등이 있다.\n",
    "\n",
    "  4. **JX (주격 조사)**  \n",
    "    주격을 나타내는 조사를 의미한다. 주어를 나타내는 데 사용된다. 예: \"이\", \"가\".\n",
    "\n",
    "  5. **M (부사)**  \n",
    "    부사를 나타낸다. 동사, 형용사, 다른 부사를 수식하는 역할을 한다.\n",
    "\n",
    "  6. **MAG (일반 부사)**  \n",
    "    일반적인 부사를 나타낸다. 예: \"빠르게\", \"조용히\".\n",
    "\n",
    "  7. **A (형용사)**  \n",
    "    형용사를 나타낸다. 명사를 수식하여 그 특성을 설명한다.\n",
    "\n",
    "  8. **VA (형용사)**  \n",
    "    형용사를 나타내며, 주로 상태를 표현하는 형용사이다.\n",
    "\n",
    "  9. **E (종결 어미)**  \n",
    "    문장의 끝을 맺는 종결 어미를 나타낸다. 예: \"습니다\", \"다\".\n",
    "\n",
    "  10. **EFN (평서형 종결 어미)**  \n",
    "      평서문에서 사용되는 종결 어미를 나타낸다. 일반적인 서술형 문장에서 사용된다.\n",
    "\n",
    "  11. **S (구두점)**  \n",
    "      문장 내에서 의미를 구분하는 구두점을 나타낸다. 예: \",\", \":\", \";\".\n",
    "\n",
    "  12. **SF (문장 부호)**  \n",
    "      문장의 끝을 나타내는 문장 부호를 의미한다. 예: \".\", \"!\", \"?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '버스', '를', '기다리고', '있다', '.', '지각', '은', '면', '하겠군', '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = \"나는 버스를 기다리고 있다. 지각은 면하겠군!\"\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('버스', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('기다리고', 'Verb'),\n",
       " ('있다', 'Adjective'),\n",
       " ('.', 'Punctuation'),\n",
       " ('지각', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('면', 'Noun'),\n",
       " ('하겠군', 'Verb'),\n",
       " ('!', 'Punctuation')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '버스', '지각', '면']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
