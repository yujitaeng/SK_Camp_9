{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Word Embedding**은 단어를 고정된 차원의 벡터로 변환하는 기술로, 단어 간의 의미적 유사성을 반영하도록 학습된 벡터를 말한다.  \n",
    "- 이 기술은 자연어 처리에서 문장을 처리하고 이해하는 데 활용된다.  \n",
    "- 숫자로 표현된 단어 목록을 통해 감정을 추출하는 것도 가능하다.  \n",
    "- 연관성 있는 단어들을 군집화하여 다차원 공간에 벡터로 나타낼 수 있으며, 이는 단어나 문장을 벡터 공간에 매핑하는 과정이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Matrix 예시**\n",
    "\n",
    "*아래 표의 벡터 값들은 모두 기계 학습을 통해 학습된 결과이다.*  \n",
    "\n",
    "| Dimension | Man (5391) | Woman (9853) | King (4914) | Queen (7157) | Apple (456) | Orange (6257) |\n",
    "|-----------|------------|--------------|-------------|--------------|-------------|---------------|\n",
    "| 성별      | -1         | 1            | -0.95       | 0.97         | 0.00        | 0.01          |\n",
    "| 귀족      | 0.01       | 0.02         | 0.93        | 0.95         | -0.01       | 0.00          |\n",
    "| 나이      | 0.03       | 0.02         | 0.7         | 0.69         | 0.03        | -0.02         |\n",
    "| 음식      | 0.04       | 0.01         | 0.02        | 0.01         | 0.95        | 0.97          |\n",
    "\n",
    "<br>\n",
    "\n",
    "*아래는 전치된 표이다.*\n",
    "\n",
    "| Word          | 성별   | 귀족   | 나이   | 음식   |\n",
    "|---------------|--------|--------|--------|--------|\n",
    "| Man (5391)    | -1.00  | 0.01   | 0.03   | 0.04   |\n",
    "| Woman (9853)  | 1.00   | 0.02   | 0.02   | 0.01   |\n",
    "| King (4914)   | -0.95  | 0.93   | 0.70   | 0.02   |\n",
    "| Queen (7157)  | 0.97   | 0.95   | 0.69   | 0.01   |\n",
    "| Apple (456)   | 0.00   | -0.01  | 0.03   | 0.95   |\n",
    "| Orange (6257) | 0.01   | 0.00   | -0.02  | 0.97   |\n",
    "\n",
    "- **의미적 유사성 반영**  \n",
    "  - 단어를 고정된 크기의 실수 벡터로 표현하며, 비슷한 의미를 가진 단어는 벡터 공간에서 가깝게 위치한다.  \n",
    "  - 예를 들어, \"king\"과 \"queen\"은 비슷한 맥락에서 자주 사용되므로 벡터 공간에서 가까운 위치에 배치된다.  \n",
    "\n",
    "- **밀집 벡터(Dense Vector)**  \n",
    "  - BoW, DTM, TF-IDF와 달리 Word Embedding은 저차원 밀집 벡터로 변환되며, 차원이 낮으면서도 의미적으로 풍부한 정보를 담는다.  \n",
    "  - 벡터 차원은 보통 100 또는 300 정도로 제한된다.  \n",
    "\n",
    "- **문맥 정보 반영**  \n",
    "  - Word Embedding은 단어 주변의 단어들을 학습해 단어의 의미를 추론한다.  \n",
    "  - 예를 들어, \"bank\"라는 단어가 \"river\"와 함께 나오면 \"강둑\"을, \"money\"와 함께 나오면 \"은행\"을 의미한다고 학습한다.  \n",
    "\n",
    "- **학습 기반 벡터**  \n",
    "  - Word Embedding은 대규모 텍스트 데이터에서 단어 간 연관성을 학습해 벡터를 생성한다.  \n",
    "  - 반면, BoW나 TF-IDF는 단순한 규칙 기반 벡터화 방법이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 표현(Sparse Representation) | 분산 표현(Distributed Representation)\n",
    "- 원-핫 인코딩으로 얻은 원-핫 벡터는 단어의 인덱스 값만 1이고 나머지는 모두 0으로 표현된다.\n",
    "- 이렇게 대부분의 값이 0인 벡터나 행렬을 사용하는 표현 방식을 희소 표현(sparse representation)이라고 한다.  \n",
    "- 희소 표현은 단어 벡터 간 유의미한 유사성을 표현할 수 없다는 단점이 있다.\n",
    "- 이를 해결하기 위해 단어의 의미를 다차원 공간에 벡터화하는 분산 표현(distributed representation)을 사용한다.\n",
    "- 분산 표현으로 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라고 하며, 이렇게 변환된 벡터를 임베딩 벡터(embedding vector)라고 한다.  \n",
    "- **원-핫 인코딩 → 희소 표현**  \n",
    "- **워드 임베딩 → 분산 표현**  \n",
    "\n",
    "**분산 표현(Distributed Representation)**\n",
    "- 분산 표현은 분포 가설(distributional hypothesis)에 기반한 방법이다.\n",
    "- 이 가설은 \"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\"는 내용을 전제로 한다.\n",
    "- 예를 들어, '강아지'라는 단어는 '귀엽다', '예쁘다', '애교' 등의 단어와 함께 자주 등장하며, 이를 벡터화하면 해당 단어들은 유사한 벡터값을 갖게 된다.\n",
    "- 분산 표현은 단어의 의미를 여러 차원에 걸쳐 분산하여 표현한다.  \n",
    "- 이 방식은 원-핫 벡터처럼 단어 집합 크기만큼의 차원이 필요하지 않으며, 상대적으로 저차원으로 줄어든다.\n",
    "- 예를 들어, 단어 집합 크기가 10,000이고 '강아지'의 인덱스가 4라면, 원-핫 벡터는 다음과 같다:\n",
    "  \n",
    "- **강아지 = [0 0 0 0 1 0 0 ... 0]** (뒤에 9,995개의 0 포함)  \n",
    "- 그러나 Word2Vec으로 임베딩된 벡터는 단어 집합 크기와 무관하며, 설정된 차원의 수만큼 실수값을 가진 벡터가 된다:  \n",
    "- **강아지 = [0.2 0.3 0.5 0.7 0.2 ... 0.2]**  \n",
    "\n",
    "**요약하면,**\n",
    "- 희소 표현은 고차원에서 각 차원이 분리된 방식으로 단어를 표현하지만, 분산 표현은 저차원에서 단어의 의미를 여러 차원에 분산시켜 표현한다.\n",
    "- 이를 통해 단어 벡터 간 유의미한 유사도를 계산할 수 있으며, 대표적인 학습 방법으로 Word2Vec이 사용된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Vector 시각화 wevi\n",
    "https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 2013년 구글에서 개발한 Word Embedding 방법\n",
    "- 최초의 neural embedding model\n",
    "- 매우 큰 corpus에서 자동 학습\n",
    "    - 비지도 지도 학습 (자기 지도학습)이라 할 수 있음\n",
    "    - 많은 데이터를 기반으로 label 값 유추하고 이를 지도학습에 사용\n",
    "- ex) \n",
    "    - **이사금**께 충성을 맹세하였다.\n",
    "    - **왕**께 충성을 맹세하였다.\n",
    "\n",
    "**WordVec 훈련방식에 따른 구분**\n",
    "1. CBOW : 주변 단어로 중심 단어를 예측\n",
    "2. Skip-gram : 중심 단어로 주변 단어를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words)  \n",
    "- CBOW는 원-핫 벡터를 사용하지만, 이는 단순히 위치를 가리킬 뿐 vocabulary를 직접적으로 참조하지 않는다.  \n",
    "\n",
    "**예시:**  \n",
    "\n",
    "> The fat cat sat on the mat  \n",
    "\n",
    "주어진 문장에서 'sat'이라는 단어를 예측하는 것이 CBOW의 주요 작업이다.  \n",
    "- **중심 단어(center word):** 예측하려는 단어 ('sat')  \n",
    "- **주변 단어(context word):** 예측에 사용되는 단어들  \n",
    "\n",
    "중심 단어를 예측하기 위해 앞뒤 몇 개의 단어를 참고할지 결정하는 범위를 **윈도우(window)**라고 한다.  \n",
    "예를 들어, 윈도우 크기가 2이고 중심 단어가 'sat'라면, 앞의 두 단어(fat, cat)와 뒤의 두 단어(on, the)를 입력으로 사용한다.  \n",
    "윈도우 크기가 n일 경우, 참고하는 주변 단어의 개수는 총 2n이다. 윈도우를 옆으로 이동하며 학습 데이터를 생성하는 방법을 **슬라이딩 윈도우(sliding window)**라고 한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "\n",
    "**훈련 과정**\n",
    "\n",
    "CBOW는 embedding 벡터를 학습하기 위한 구조를 갖는다. 초기에는 가중치가 임의의 값으로 설정되며, 역전파를 통해 최적화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "Word2Vec은 은닉층이 하나뿐인 얕은 신경망(shallow neural network) 구조를 사용한다.  \n",
    "학습 대상이 되는 주요 가중치는 두 가지이다:  \n",
    "\n",
    "1. **투사층(projection layer):**  \n",
    "   - 활성화 함수가 없으며 룩업 테이블 연산을 담당한다.  \n",
    "   - 입력층과 투사층 사이의 가중치 W는 V × M 행렬로 표현되며, 여기서 **V는 단어 집합의 크기, M은 벡터의 차원**이다.  \n",
    "   - W 행렬의 각 행은 학습 후 단어의 M차원 임베딩 벡터로 간주된다.  \n",
    "   - 예를 들어, 벡터 차원을 5로 설정하면 각 단어의 임베딩 벡터는 5차원이 된다.  \n",
    "\n",
    "2. **출력층:**  \n",
    "   - 투사층과 출력층 사이의 가중치 W'는 M × V 행렬로 표현된다.  \n",
    "   - 이 두 행렬(W와 W')은 서로 독립적이며, 학습 전에는 랜덤 값으로 초기화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "\n",
    "**예측 과정**\n",
    "1. CBOW는 계산된 룩업 테이블의 평균을 구한 뒤, 출력층의 가중치 W'와 내적한다.  \n",
    "2. 결과값은 **소프트맥스(softmax)** 활성화 함수에 입력되어, 중심 단어일 확률을 나타내는 예측값으로 변환된다.  \n",
    "3. 출력된 예측값(스코어 벡터)은 실제 타겟 원-핫 벡터와 비교되며, **크로스 엔트로피(cross-entropy)** 함수로 손실값을 계산한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "\n",
    "손실 함수 식:  \n",
    "$\n",
    "cost(\\hat{y}, y) = -\\sum_{j=1}^{V} y_{j} \\cdot log(\\hat{y}_{j})\n",
    "$  \n",
    "\n",
    "여기서, $\\hat{y}_{j}$는 예측 확률, $y_{j}$는 실제 값이며, V는 단어 집합의 크기를 의미한다.  \n",
    "\n",
    "\n",
    "**학습 결과**  \n",
    "- 역전파를 통해 가중치 W와 W'가 학습된다. \n",
    "- 학습이 완료되면 W 행렬의 각 행을 단어의 임베딩 벡터로 사용하거나, W와 W' 모두를 이용해 임베딩 벡터를 생성할 수 있다.  \n",
    "- CBOW는 주변 단어를 기반으로 중심 단어를 예측하는 구조를 갖추고 있으며, 이를 통해 단어 간 의미적 관계를 효과적으로 학습할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-gram\n",
    "- Skip-gram은 중심 단어에서 주변 단어를 예측한다.\n",
    "- 윈도우 크기가 2일 때, 데이터셋은 다음과 같이 구성된다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
    "\n",
    "- 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다.\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 영어 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 취득 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1DCgLPJsfyLGZ99lB-aF8EvpKIWSZYgp4\"\n",
    "output = \"ted_en.xml\"\n",
    "\n",
    "# gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24222849\n",
      "24062319\n"
     ]
    }
   ],
   "source": [
    "# xml 데이터 처리\n",
    "f = open('ted_en.xml', 'r', encoding='utf-8')\n",
    "xml = etree.parse(f)\n",
    "\n",
    "contents = xml.xpath('//content/text()')    # content 태그 하위 텍스트\n",
    "# contents[:5]\n",
    "\n",
    "corpus = '\\n'.join(contents)\n",
    "print(len(corpus))\n",
    "\n",
    "# 정규식을 이용해 (Laughter), (Applause) 등 키워드 제거\n",
    "corpus = re.sub(r'\\([^)]*\\)', '', corpus)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'reasons', 'companies', 'fail', 'new'],\n",
       " ['real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'figuring',\n",
       "  'balance',\n",
       "  'two',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'exploitation'],\n",
       " ['necessary', 'much', 'good', 'thing'],\n",
       " ['consider', 'facit'],\n",
       " ['actually', 'old', 'enough', 'remember']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 (토큰화/대소문자 정규화/불용어 처리)\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "preprocessed_sentences = []\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z0-9]', ' ', sentence)    # 영소문자, 숫자 외 제거\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    preprocessed_sentences.append(tokens)\n",
    "\n",
    "preprocessed_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21462, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=preprocessed_sentences, # corpus\n",
    "    vector_size=100,                  # 임베딩 벡터 차원\n",
    "    sg=0,                             # 학습 알고리즘 선택 (0=CBOW, 1=Skip-gram)\n",
    "    window=5,                         # 주변 단어 수 (앞뒤로 n개 고려)\n",
    "    min_count=5                       # 최소 빈도 (빈도 n개 미만은 제거)\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>-0.507626</td>\n",
       "      <td>-0.442687</td>\n",
       "      <td>-1.167928</td>\n",
       "      <td>0.701634</td>\n",
       "      <td>0.086201</td>\n",
       "      <td>-0.765696</td>\n",
       "      <td>0.442688</td>\n",
       "      <td>0.754525</td>\n",
       "      <td>-2.556752</td>\n",
       "      <td>-0.735238</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>-0.066485</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>-0.032708</td>\n",
       "      <td>-0.160991</td>\n",
       "      <td>0.496461</td>\n",
       "      <td>-0.888984</td>\n",
       "      <td>-0.569103</td>\n",
       "      <td>-1.482977</td>\n",
       "      <td>1.365504</td>\n",
       "      <td>-0.038934</td>\n",
       "      <td>0.962820</td>\n",
       "      <td>-0.073200</td>\n",
       "      <td>0.330002</td>\n",
       "      <td>-1.470577</td>\n",
       "      <td>0.181802</td>\n",
       "      <td>-0.449173</td>\n",
       "      <td>0.451726</td>\n",
       "      <td>0.134219</td>\n",
       "      <td>-0.885450</td>\n",
       "      <td>0.718595</td>\n",
       "      <td>0.833681</td>\n",
       "      <td>-1.326100</td>\n",
       "      <td>-0.224466</td>\n",
       "      <td>0.843157</td>\n",
       "      <td>0.434043</td>\n",
       "      <td>-0.757573</td>\n",
       "      <td>-0.069115</td>\n",
       "      <td>-1.689192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627114</td>\n",
       "      <td>0.801336</td>\n",
       "      <td>-0.682833</td>\n",
       "      <td>-0.122913</td>\n",
       "      <td>0.261963</td>\n",
       "      <td>0.640149</td>\n",
       "      <td>-0.004366</td>\n",
       "      <td>-0.173722</td>\n",
       "      <td>-0.111575</td>\n",
       "      <td>0.327797</td>\n",
       "      <td>-0.118447</td>\n",
       "      <td>0.050972</td>\n",
       "      <td>-0.269864</td>\n",
       "      <td>-0.169801</td>\n",
       "      <td>-0.620474</td>\n",
       "      <td>-1.721967</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>-0.120760</td>\n",
       "      <td>-0.254427</td>\n",
       "      <td>0.940067</td>\n",
       "      <td>0.754327</td>\n",
       "      <td>1.280342</td>\n",
       "      <td>-0.881259</td>\n",
       "      <td>-0.576295</td>\n",
       "      <td>-1.628928</td>\n",
       "      <td>-0.392792</td>\n",
       "      <td>-0.135950</td>\n",
       "      <td>-0.654950</td>\n",
       "      <td>-0.466230</td>\n",
       "      <td>1.808225</td>\n",
       "      <td>1.030128</td>\n",
       "      <td>0.823591</td>\n",
       "      <td>-0.585958</td>\n",
       "      <td>0.257513</td>\n",
       "      <td>1.025655</td>\n",
       "      <td>-1.890258</td>\n",
       "      <td>-0.806140</td>\n",
       "      <td>-0.435385</td>\n",
       "      <td>0.692131</td>\n",
       "      <td>0.848384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>-1.427202</td>\n",
       "      <td>0.529370</td>\n",
       "      <td>-0.490544</td>\n",
       "      <td>0.512964</td>\n",
       "      <td>-0.504665</td>\n",
       "      <td>-1.441672</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>-0.975314</td>\n",
       "      <td>-2.028039</td>\n",
       "      <td>0.794434</td>\n",
       "      <td>-0.399200</td>\n",
       "      <td>0.127624</td>\n",
       "      <td>-0.713485</td>\n",
       "      <td>1.505122</td>\n",
       "      <td>0.408434</td>\n",
       "      <td>0.381780</td>\n",
       "      <td>2.050386</td>\n",
       "      <td>-0.434477</td>\n",
       "      <td>0.536018</td>\n",
       "      <td>1.089365</td>\n",
       "      <td>-1.239129</td>\n",
       "      <td>2.258339</td>\n",
       "      <td>0.208337</td>\n",
       "      <td>0.093448</td>\n",
       "      <td>-0.469139</td>\n",
       "      <td>-0.045763</td>\n",
       "      <td>-1.200611</td>\n",
       "      <td>0.861557</td>\n",
       "      <td>-0.218029</td>\n",
       "      <td>0.620924</td>\n",
       "      <td>-0.925302</td>\n",
       "      <td>-0.716246</td>\n",
       "      <td>-0.199752</td>\n",
       "      <td>-0.115392</td>\n",
       "      <td>1.890724</td>\n",
       "      <td>1.394664</td>\n",
       "      <td>-0.418755</td>\n",
       "      <td>-0.880868</td>\n",
       "      <td>1.481014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714839</td>\n",
       "      <td>-0.255614</td>\n",
       "      <td>-1.066218</td>\n",
       "      <td>-0.851919</td>\n",
       "      <td>-1.292182</td>\n",
       "      <td>1.185493</td>\n",
       "      <td>0.790945</td>\n",
       "      <td>0.164187</td>\n",
       "      <td>0.419567</td>\n",
       "      <td>2.510297</td>\n",
       "      <td>-0.102671</td>\n",
       "      <td>0.052144</td>\n",
       "      <td>1.495816</td>\n",
       "      <td>0.849155</td>\n",
       "      <td>0.174237</td>\n",
       "      <td>1.243476</td>\n",
       "      <td>-0.001722</td>\n",
       "      <td>-0.087473</td>\n",
       "      <td>-0.868375</td>\n",
       "      <td>-1.440067</td>\n",
       "      <td>-2.079520</td>\n",
       "      <td>-0.192073</td>\n",
       "      <td>0.018215</td>\n",
       "      <td>-0.104120</td>\n",
       "      <td>-0.530741</td>\n",
       "      <td>0.600678</td>\n",
       "      <td>-0.022934</td>\n",
       "      <td>-1.272094</td>\n",
       "      <td>-0.250519</td>\n",
       "      <td>-0.116203</td>\n",
       "      <td>1.077553</td>\n",
       "      <td>0.666987</td>\n",
       "      <td>-1.494577</td>\n",
       "      <td>-0.226486</td>\n",
       "      <td>-0.775491</td>\n",
       "      <td>-0.085792</td>\n",
       "      <td>-0.324845</td>\n",
       "      <td>-1.191003</td>\n",
       "      <td>-1.717081</td>\n",
       "      <td>0.895945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.207078</td>\n",
       "      <td>-0.131140</td>\n",
       "      <td>-0.775172</td>\n",
       "      <td>-1.501590</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>-0.063267</td>\n",
       "      <td>0.460151</td>\n",
       "      <td>0.498318</td>\n",
       "      <td>-1.797880</td>\n",
       "      <td>1.119787</td>\n",
       "      <td>-1.526861</td>\n",
       "      <td>0.338633</td>\n",
       "      <td>1.063064</td>\n",
       "      <td>0.014577</td>\n",
       "      <td>1.204981</td>\n",
       "      <td>-0.554407</td>\n",
       "      <td>0.429454</td>\n",
       "      <td>0.750728</td>\n",
       "      <td>-0.002742</td>\n",
       "      <td>-0.184297</td>\n",
       "      <td>-0.150572</td>\n",
       "      <td>-0.443660</td>\n",
       "      <td>-0.298882</td>\n",
       "      <td>-1.578368</td>\n",
       "      <td>0.890639</td>\n",
       "      <td>-1.104240</td>\n",
       "      <td>2.586069</td>\n",
       "      <td>0.347849</td>\n",
       "      <td>-1.034564</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>0.978287</td>\n",
       "      <td>-0.442359</td>\n",
       "      <td>-0.820679</td>\n",
       "      <td>0.053675</td>\n",
       "      <td>-0.363577</td>\n",
       "      <td>-0.770446</td>\n",
       "      <td>-0.052837</td>\n",
       "      <td>1.311918</td>\n",
       "      <td>-0.326769</td>\n",
       "      <td>0.050285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354943</td>\n",
       "      <td>-0.261340</td>\n",
       "      <td>1.031166</td>\n",
       "      <td>-0.830391</td>\n",
       "      <td>-0.754320</td>\n",
       "      <td>2.160495</td>\n",
       "      <td>0.293686</td>\n",
       "      <td>0.183012</td>\n",
       "      <td>-1.748434</td>\n",
       "      <td>0.573363</td>\n",
       "      <td>0.418451</td>\n",
       "      <td>0.143926</td>\n",
       "      <td>1.100607</td>\n",
       "      <td>-0.704678</td>\n",
       "      <td>0.664372</td>\n",
       "      <td>1.060452</td>\n",
       "      <td>1.329798</td>\n",
       "      <td>0.409646</td>\n",
       "      <td>-0.849035</td>\n",
       "      <td>0.407822</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>0.798178</td>\n",
       "      <td>-1.852511</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>-0.088404</td>\n",
       "      <td>-0.698820</td>\n",
       "      <td>-1.237223</td>\n",
       "      <td>0.726062</td>\n",
       "      <td>0.779122</td>\n",
       "      <td>-0.323033</td>\n",
       "      <td>-0.780076</td>\n",
       "      <td>0.665731</td>\n",
       "      <td>-0.297956</td>\n",
       "      <td>-0.063771</td>\n",
       "      <td>-0.388630</td>\n",
       "      <td>0.711679</td>\n",
       "      <td>0.318711</td>\n",
       "      <td>-0.325640</td>\n",
       "      <td>0.840729</td>\n",
       "      <td>-0.541419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>-0.782230</td>\n",
       "      <td>0.082809</td>\n",
       "      <td>-0.382945</td>\n",
       "      <td>0.238039</td>\n",
       "      <td>0.295448</td>\n",
       "      <td>0.189652</td>\n",
       "      <td>-0.517759</td>\n",
       "      <td>0.156646</td>\n",
       "      <td>-0.935600</td>\n",
       "      <td>-0.951888</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>-0.412244</td>\n",
       "      <td>-0.078371</td>\n",
       "      <td>-0.796105</td>\n",
       "      <td>0.779983</td>\n",
       "      <td>-0.578469</td>\n",
       "      <td>0.209035</td>\n",
       "      <td>-0.051398</td>\n",
       "      <td>-0.130366</td>\n",
       "      <td>-1.012064</td>\n",
       "      <td>-0.287448</td>\n",
       "      <td>0.151739</td>\n",
       "      <td>-0.059339</td>\n",
       "      <td>0.858964</td>\n",
       "      <td>0.061323</td>\n",
       "      <td>1.174370</td>\n",
       "      <td>1.209591</td>\n",
       "      <td>-0.366828</td>\n",
       "      <td>-0.484767</td>\n",
       "      <td>-0.171466</td>\n",
       "      <td>0.796183</td>\n",
       "      <td>0.109727</td>\n",
       "      <td>-0.300429</td>\n",
       "      <td>-0.166398</td>\n",
       "      <td>0.143674</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.468853</td>\n",
       "      <td>-1.128786</td>\n",
       "      <td>-0.327399</td>\n",
       "      <td>-0.146518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218287</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>0.706968</td>\n",
       "      <td>-0.057708</td>\n",
       "      <td>-1.350490</td>\n",
       "      <td>0.645816</td>\n",
       "      <td>0.610823</td>\n",
       "      <td>0.656669</td>\n",
       "      <td>-1.140041</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>1.072479</td>\n",
       "      <td>0.354713</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>0.180266</td>\n",
       "      <td>0.593578</td>\n",
       "      <td>0.380466</td>\n",
       "      <td>-0.056427</td>\n",
       "      <td>0.215206</td>\n",
       "      <td>-0.864214</td>\n",
       "      <td>-0.084126</td>\n",
       "      <td>-0.897058</td>\n",
       "      <td>0.832352</td>\n",
       "      <td>-1.315659</td>\n",
       "      <td>0.750415</td>\n",
       "      <td>-0.874311</td>\n",
       "      <td>0.324391</td>\n",
       "      <td>0.409033</td>\n",
       "      <td>1.211542</td>\n",
       "      <td>0.127888</td>\n",
       "      <td>-0.507821</td>\n",
       "      <td>0.202856</td>\n",
       "      <td>0.041537</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>-0.378668</td>\n",
       "      <td>-0.291373</td>\n",
       "      <td>0.317489</td>\n",
       "      <td>0.226503</td>\n",
       "      <td>-0.684926</td>\n",
       "      <td>0.746383</td>\n",
       "      <td>-0.223859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>-1.130339</td>\n",
       "      <td>0.754498</td>\n",
       "      <td>0.083723</td>\n",
       "      <td>-0.349729</td>\n",
       "      <td>1.642845</td>\n",
       "      <td>0.379064</td>\n",
       "      <td>-0.678453</td>\n",
       "      <td>1.569615</td>\n",
       "      <td>-1.044017</td>\n",
       "      <td>-0.937708</td>\n",
       "      <td>-0.643921</td>\n",
       "      <td>-0.972029</td>\n",
       "      <td>0.671734</td>\n",
       "      <td>1.165268</td>\n",
       "      <td>0.356800</td>\n",
       "      <td>-1.897946</td>\n",
       "      <td>0.336356</td>\n",
       "      <td>0.157773</td>\n",
       "      <td>0.735090</td>\n",
       "      <td>-1.238956</td>\n",
       "      <td>-0.826559</td>\n",
       "      <td>-1.908656</td>\n",
       "      <td>-0.334208</td>\n",
       "      <td>1.649768</td>\n",
       "      <td>0.249633</td>\n",
       "      <td>-0.863758</td>\n",
       "      <td>-0.939677</td>\n",
       "      <td>0.438178</td>\n",
       "      <td>-0.737797</td>\n",
       "      <td>-0.224734</td>\n",
       "      <td>1.134578</td>\n",
       "      <td>1.137345</td>\n",
       "      <td>0.100246</td>\n",
       "      <td>0.112718</td>\n",
       "      <td>0.464756</td>\n",
       "      <td>0.284342</td>\n",
       "      <td>-0.281611</td>\n",
       "      <td>-0.185200</td>\n",
       "      <td>-0.064977</td>\n",
       "      <td>0.050386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>1.023091</td>\n",
       "      <td>1.053219</td>\n",
       "      <td>-0.235591</td>\n",
       "      <td>-2.265276</td>\n",
       "      <td>0.437619</td>\n",
       "      <td>0.840162</td>\n",
       "      <td>0.971098</td>\n",
       "      <td>-0.681899</td>\n",
       "      <td>0.120063</td>\n",
       "      <td>-1.035932</td>\n",
       "      <td>1.029499</td>\n",
       "      <td>-0.065781</td>\n",
       "      <td>0.754960</td>\n",
       "      <td>0.071952</td>\n",
       "      <td>1.818600</td>\n",
       "      <td>1.643349</td>\n",
       "      <td>1.890043</td>\n",
       "      <td>-0.466175</td>\n",
       "      <td>0.137639</td>\n",
       "      <td>-0.582862</td>\n",
       "      <td>-0.161146</td>\n",
       "      <td>-0.666676</td>\n",
       "      <td>-1.263704</td>\n",
       "      <td>-0.953896</td>\n",
       "      <td>0.636947</td>\n",
       "      <td>0.890287</td>\n",
       "      <td>0.467035</td>\n",
       "      <td>-1.870485</td>\n",
       "      <td>-0.400472</td>\n",
       "      <td>0.564086</td>\n",
       "      <td>-0.904401</td>\n",
       "      <td>-0.055513</td>\n",
       "      <td>1.697131</td>\n",
       "      <td>-0.062900</td>\n",
       "      <td>-0.257962</td>\n",
       "      <td>0.435354</td>\n",
       "      <td>0.190390</td>\n",
       "      <td>-1.113422</td>\n",
       "      <td>0.255285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>-0.509070</td>\n",
       "      <td>-0.412272</td>\n",
       "      <td>0.692741</td>\n",
       "      <td>-0.518409</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>-0.849973</td>\n",
       "      <td>0.030227</td>\n",
       "      <td>-0.285951</td>\n",
       "      <td>-1.078182</td>\n",
       "      <td>-0.966130</td>\n",
       "      <td>0.189743</td>\n",
       "      <td>-1.202058</td>\n",
       "      <td>0.284056</td>\n",
       "      <td>-0.888331</td>\n",
       "      <td>0.236127</td>\n",
       "      <td>-0.807115</td>\n",
       "      <td>-0.599416</td>\n",
       "      <td>-0.617170</td>\n",
       "      <td>-0.027399</td>\n",
       "      <td>-0.419478</td>\n",
       "      <td>-0.560431</td>\n",
       "      <td>0.411597</td>\n",
       "      <td>1.175526</td>\n",
       "      <td>0.535902</td>\n",
       "      <td>0.368917</td>\n",
       "      <td>0.445410</td>\n",
       "      <td>0.221428</td>\n",
       "      <td>-0.131979</td>\n",
       "      <td>-0.118100</td>\n",
       "      <td>0.996133</td>\n",
       "      <td>0.645934</td>\n",
       "      <td>-0.071236</td>\n",
       "      <td>0.595299</td>\n",
       "      <td>-0.509688</td>\n",
       "      <td>-0.215432</td>\n",
       "      <td>1.723459</td>\n",
       "      <td>0.948665</td>\n",
       "      <td>0.398660</td>\n",
       "      <td>-0.289545</td>\n",
       "      <td>-1.087636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040588</td>\n",
       "      <td>0.129383</td>\n",
       "      <td>1.364531</td>\n",
       "      <td>-0.195364</td>\n",
       "      <td>-0.854206</td>\n",
       "      <td>0.989114</td>\n",
       "      <td>0.786308</td>\n",
       "      <td>-0.326702</td>\n",
       "      <td>-1.198498</td>\n",
       "      <td>-0.146261</td>\n",
       "      <td>1.192360</td>\n",
       "      <td>1.309325</td>\n",
       "      <td>0.219950</td>\n",
       "      <td>0.581725</td>\n",
       "      <td>0.436682</td>\n",
       "      <td>0.248794</td>\n",
       "      <td>-0.175063</td>\n",
       "      <td>-0.036784</td>\n",
       "      <td>0.039753</td>\n",
       "      <td>-0.030889</td>\n",
       "      <td>-0.870519</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-1.203302</td>\n",
       "      <td>-0.012457</td>\n",
       "      <td>-0.656578</td>\n",
       "      <td>-0.119583</td>\n",
       "      <td>0.390574</td>\n",
       "      <td>1.437569</td>\n",
       "      <td>0.130566</td>\n",
       "      <td>0.363299</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>1.383334</td>\n",
       "      <td>-0.425625</td>\n",
       "      <td>0.165726</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>-0.308536</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>-0.863224</td>\n",
       "      <td>0.042337</td>\n",
       "      <td>-0.116391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.257161</td>\n",
       "      <td>-0.129006</td>\n",
       "      <td>0.482975</td>\n",
       "      <td>-1.347949</td>\n",
       "      <td>-0.899297</td>\n",
       "      <td>-0.962455</td>\n",
       "      <td>0.080054</td>\n",
       "      <td>0.909067</td>\n",
       "      <td>-1.830676</td>\n",
       "      <td>0.850075</td>\n",
       "      <td>-0.073360</td>\n",
       "      <td>-0.543554</td>\n",
       "      <td>-0.094335</td>\n",
       "      <td>0.858040</td>\n",
       "      <td>0.938841</td>\n",
       "      <td>-1.352835</td>\n",
       "      <td>0.128303</td>\n",
       "      <td>-0.569660</td>\n",
       "      <td>1.572959</td>\n",
       "      <td>-0.911368</td>\n",
       "      <td>0.879652</td>\n",
       "      <td>-0.051598</td>\n",
       "      <td>0.187312</td>\n",
       "      <td>-0.023154</td>\n",
       "      <td>1.241177</td>\n",
       "      <td>1.347843</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.229387</td>\n",
       "      <td>-0.499892</td>\n",
       "      <td>0.608323</td>\n",
       "      <td>-0.058787</td>\n",
       "      <td>1.022565</td>\n",
       "      <td>-0.560106</td>\n",
       "      <td>1.078127</td>\n",
       "      <td>0.309961</td>\n",
       "      <td>1.254737</td>\n",
       "      <td>-0.296157</td>\n",
       "      <td>-1.667353</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.039516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656880</td>\n",
       "      <td>0.060775</td>\n",
       "      <td>-0.460970</td>\n",
       "      <td>0.863397</td>\n",
       "      <td>-0.717029</td>\n",
       "      <td>-0.453252</td>\n",
       "      <td>-0.393620</td>\n",
       "      <td>-0.822204</td>\n",
       "      <td>-0.284825</td>\n",
       "      <td>0.147337</td>\n",
       "      <td>0.265105</td>\n",
       "      <td>0.743014</td>\n",
       "      <td>-0.299271</td>\n",
       "      <td>-0.250645</td>\n",
       "      <td>0.536162</td>\n",
       "      <td>-0.407188</td>\n",
       "      <td>0.347387</td>\n",
       "      <td>-0.197838</td>\n",
       "      <td>-0.125123</td>\n",
       "      <td>1.141522</td>\n",
       "      <td>0.432652</td>\n",
       "      <td>-0.274082</td>\n",
       "      <td>-0.867984</td>\n",
       "      <td>0.691310</td>\n",
       "      <td>0.299132</td>\n",
       "      <td>0.990966</td>\n",
       "      <td>0.590682</td>\n",
       "      <td>1.290969</td>\n",
       "      <td>0.221561</td>\n",
       "      <td>0.223187</td>\n",
       "      <td>-0.504160</td>\n",
       "      <td>0.908535</td>\n",
       "      <td>0.486973</td>\n",
       "      <td>0.669019</td>\n",
       "      <td>0.298825</td>\n",
       "      <td>0.135384</td>\n",
       "      <td>0.908060</td>\n",
       "      <td>-0.858874</td>\n",
       "      <td>0.427333</td>\n",
       "      <td>0.149720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.399128</td>\n",
       "      <td>0.473840</td>\n",
       "      <td>0.767682</td>\n",
       "      <td>-1.008862</td>\n",
       "      <td>1.488817</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>-0.691542</td>\n",
       "      <td>0.024907</td>\n",
       "      <td>-1.135851</td>\n",
       "      <td>-0.163176</td>\n",
       "      <td>-0.365925</td>\n",
       "      <td>-0.173582</td>\n",
       "      <td>0.349240</td>\n",
       "      <td>0.908904</td>\n",
       "      <td>-0.516420</td>\n",
       "      <td>-1.169889</td>\n",
       "      <td>0.297156</td>\n",
       "      <td>1.398615</td>\n",
       "      <td>-0.912876</td>\n",
       "      <td>-0.917497</td>\n",
       "      <td>0.054514</td>\n",
       "      <td>-0.332387</td>\n",
       "      <td>-0.599138</td>\n",
       "      <td>-0.523539</td>\n",
       "      <td>0.273564</td>\n",
       "      <td>-1.013528</td>\n",
       "      <td>-0.392055</td>\n",
       "      <td>-0.259261</td>\n",
       "      <td>-0.652026</td>\n",
       "      <td>-1.096553</td>\n",
       "      <td>-0.217578</td>\n",
       "      <td>2.118106</td>\n",
       "      <td>-0.858938</td>\n",
       "      <td>-0.399984</td>\n",
       "      <td>1.006813</td>\n",
       "      <td>0.253177</td>\n",
       "      <td>-1.263870</td>\n",
       "      <td>-0.381258</td>\n",
       "      <td>-0.612148</td>\n",
       "      <td>0.871903</td>\n",
       "      <td>...</td>\n",
       "      <td>2.093039</td>\n",
       "      <td>0.102087</td>\n",
       "      <td>-0.665174</td>\n",
       "      <td>-0.897960</td>\n",
       "      <td>-0.223073</td>\n",
       "      <td>0.403748</td>\n",
       "      <td>0.360143</td>\n",
       "      <td>0.511336</td>\n",
       "      <td>-0.680796</td>\n",
       "      <td>0.161001</td>\n",
       "      <td>1.281649</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.693277</td>\n",
       "      <td>-0.596323</td>\n",
       "      <td>-0.439033</td>\n",
       "      <td>1.523359</td>\n",
       "      <td>2.760970</td>\n",
       "      <td>-0.396036</td>\n",
       "      <td>0.222093</td>\n",
       "      <td>-1.050966</td>\n",
       "      <td>-1.620689</td>\n",
       "      <td>0.240872</td>\n",
       "      <td>-1.681879</td>\n",
       "      <td>-1.192966</td>\n",
       "      <td>-0.915414</td>\n",
       "      <td>0.806651</td>\n",
       "      <td>1.950321</td>\n",
       "      <td>1.078330</td>\n",
       "      <td>1.330450</td>\n",
       "      <td>0.853955</td>\n",
       "      <td>1.294555</td>\n",
       "      <td>-0.379817</td>\n",
       "      <td>-0.826987</td>\n",
       "      <td>1.227198</td>\n",
       "      <td>-0.625762</td>\n",
       "      <td>1.029177</td>\n",
       "      <td>-1.316557</td>\n",
       "      <td>-0.849583</td>\n",
       "      <td>-1.013820</td>\n",
       "      <td>-1.074842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>-2.467759</td>\n",
       "      <td>-0.681253</td>\n",
       "      <td>-0.160718</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>-0.171753</td>\n",
       "      <td>-0.212609</td>\n",
       "      <td>1.133500</td>\n",
       "      <td>1.166160</td>\n",
       "      <td>-0.331638</td>\n",
       "      <td>-0.305692</td>\n",
       "      <td>0.068299</td>\n",
       "      <td>-0.549261</td>\n",
       "      <td>1.349175</td>\n",
       "      <td>-0.079367</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>-0.098609</td>\n",
       "      <td>-0.598418</td>\n",
       "      <td>0.221684</td>\n",
       "      <td>0.184906</td>\n",
       "      <td>-1.291424</td>\n",
       "      <td>-0.523924</td>\n",
       "      <td>0.606377</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.203924</td>\n",
       "      <td>1.131022</td>\n",
       "      <td>1.175176</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>-1.628961</td>\n",
       "      <td>0.386452</td>\n",
       "      <td>0.236225</td>\n",
       "      <td>1.587291</td>\n",
       "      <td>-0.685720</td>\n",
       "      <td>1.554574</td>\n",
       "      <td>-0.099598</td>\n",
       "      <td>0.042995</td>\n",
       "      <td>1.044548</td>\n",
       "      <td>0.678254</td>\n",
       "      <td>0.832545</td>\n",
       "      <td>-0.198887</td>\n",
       "      <td>-0.300057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078856</td>\n",
       "      <td>-0.101608</td>\n",
       "      <td>1.616599</td>\n",
       "      <td>-0.745953</td>\n",
       "      <td>-2.338244</td>\n",
       "      <td>0.351252</td>\n",
       "      <td>0.916544</td>\n",
       "      <td>0.288724</td>\n",
       "      <td>-0.560802</td>\n",
       "      <td>1.111418</td>\n",
       "      <td>0.230266</td>\n",
       "      <td>0.737359</td>\n",
       "      <td>0.337242</td>\n",
       "      <td>0.438271</td>\n",
       "      <td>0.164968</td>\n",
       "      <td>-0.010665</td>\n",
       "      <td>-0.187749</td>\n",
       "      <td>-0.353332</td>\n",
       "      <td>0.665881</td>\n",
       "      <td>-0.152519</td>\n",
       "      <td>-0.180341</td>\n",
       "      <td>-0.385241</td>\n",
       "      <td>-0.421603</td>\n",
       "      <td>0.188443</td>\n",
       "      <td>-0.104898</td>\n",
       "      <td>0.567142</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>0.300105</td>\n",
       "      <td>0.644998</td>\n",
       "      <td>0.182097</td>\n",
       "      <td>0.470145</td>\n",
       "      <td>-0.445953</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>0.457778</td>\n",
       "      <td>0.591208</td>\n",
       "      <td>-0.101192</td>\n",
       "      <td>-0.297990</td>\n",
       "      <td>-1.721852</td>\n",
       "      <td>-0.406554</td>\n",
       "      <td>-0.113943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>-2.240158</td>\n",
       "      <td>-1.401547</td>\n",
       "      <td>-1.398121</td>\n",
       "      <td>-1.050263</td>\n",
       "      <td>0.026443</td>\n",
       "      <td>-1.102116</td>\n",
       "      <td>-0.490727</td>\n",
       "      <td>1.809742</td>\n",
       "      <td>-0.235827</td>\n",
       "      <td>-1.559935</td>\n",
       "      <td>0.649605</td>\n",
       "      <td>0.076903</td>\n",
       "      <td>-2.269274</td>\n",
       "      <td>0.636442</td>\n",
       "      <td>0.590709</td>\n",
       "      <td>0.564088</td>\n",
       "      <td>-1.250176</td>\n",
       "      <td>-0.538048</td>\n",
       "      <td>0.943611</td>\n",
       "      <td>-0.269954</td>\n",
       "      <td>-0.054070</td>\n",
       "      <td>-0.309885</td>\n",
       "      <td>-0.145944</td>\n",
       "      <td>1.168802</td>\n",
       "      <td>0.811147</td>\n",
       "      <td>-0.096404</td>\n",
       "      <td>0.677176</td>\n",
       "      <td>0.907101</td>\n",
       "      <td>1.138353</td>\n",
       "      <td>-0.666579</td>\n",
       "      <td>0.701767</td>\n",
       "      <td>0.066697</td>\n",
       "      <td>0.562785</td>\n",
       "      <td>0.196302</td>\n",
       "      <td>-0.667223</td>\n",
       "      <td>1.730841</td>\n",
       "      <td>-0.918647</td>\n",
       "      <td>-0.679560</td>\n",
       "      <td>0.115835</td>\n",
       "      <td>0.138921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527976</td>\n",
       "      <td>2.181037</td>\n",
       "      <td>0.641664</td>\n",
       "      <td>-0.131852</td>\n",
       "      <td>-1.579503</td>\n",
       "      <td>-0.096560</td>\n",
       "      <td>0.589749</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>-0.922896</td>\n",
       "      <td>-0.069473</td>\n",
       "      <td>0.105649</td>\n",
       "      <td>1.294301</td>\n",
       "      <td>0.580285</td>\n",
       "      <td>-0.593713</td>\n",
       "      <td>0.025643</td>\n",
       "      <td>-0.253912</td>\n",
       "      <td>0.145541</td>\n",
       "      <td>0.730842</td>\n",
       "      <td>-0.056078</td>\n",
       "      <td>0.468946</td>\n",
       "      <td>-0.405948</td>\n",
       "      <td>-0.666638</td>\n",
       "      <td>-0.633250</td>\n",
       "      <td>0.569055</td>\n",
       "      <td>1.304197</td>\n",
       "      <td>0.010399</td>\n",
       "      <td>0.126651</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.052867</td>\n",
       "      <td>1.165161</td>\n",
       "      <td>-0.026789</td>\n",
       "      <td>-0.486165</td>\n",
       "      <td>-0.335890</td>\n",
       "      <td>0.688642</td>\n",
       "      <td>0.495889</td>\n",
       "      <td>0.311467</td>\n",
       "      <td>0.113702</td>\n",
       "      <td>-0.312463</td>\n",
       "      <td>-0.912927</td>\n",
       "      <td>0.555738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2   ...        97        98        99\n",
       "one    -0.507626 -0.442687 -1.167928  ... -0.435385  0.692131  0.848384\n",
       "people -1.427202  0.529370 -0.490544  ... -1.191003 -1.717081  0.895945\n",
       "like   -0.207078 -0.131140 -0.775172  ... -0.325640  0.840729 -0.541419\n",
       "know   -0.782230  0.082809 -0.382945  ... -0.684926  0.746383 -0.223859\n",
       "going  -1.130339  0.754498  0.083723  ...  0.190390 -1.113422  0.255285\n",
       "think  -0.509070 -0.412272  0.692741  ... -0.863224  0.042337 -0.116391\n",
       "see     0.257161 -0.129006  0.482975  ... -0.858874  0.427333  0.149720\n",
       "would   0.399128  0.473840  0.767682  ... -0.849583 -1.013820 -1.074842\n",
       "really -2.467759 -0.681253 -0.160718  ... -1.721852 -0.406554 -0.113943\n",
       "get    -2.240158 -1.401547 -1.398121  ... -0.312463 -0.912927  0.555738\n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(model.wv.vectors, index=model.wv.index_to_key).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 임베딩 모델 저장\n",
    "model.wv.save_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 로드\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "load_model = KeyedVectors.load_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9017196893692017),\n",
       " ('daughter', 0.7984911203384399),\n",
       " ('girl', 0.7933292984962463),\n",
       " ('lady', 0.7810963988304138),\n",
       " ('boy', 0.777847409248352),\n",
       " ('son', 0.7718625664710999),\n",
       " ('father', 0.7517499327659607),\n",
       " ('grandfather', 0.7490587830543518),\n",
       " ('grandmother', 0.7343164086341858),\n",
       " ('sister', 0.7308599352836609)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')\n",
    "# model.wv.most_similar('abracadabra')    # 임베딩 벡터에 없는 단어로 조회 시 KeyError 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9017196893692017),\n",
       " ('daughter', 0.7984911203384399),\n",
       " ('girl', 0.7933292984962463),\n",
       " ('lady', 0.7810963988304138),\n",
       " ('boy', 0.777847409248352),\n",
       " ('son', 0.7718625664710999),\n",
       " ('father', 0.7517499327659607),\n",
       " ('grandfather', 0.7490587830543518),\n",
       " ('grandmother', 0.7343164086341858),\n",
       " ('sister', 0.7308599352836609)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.most_similar('man')    # Word2Vec.wv = KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7067673"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'husband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5571891 , -0.24520032,  1.2072326 ,  1.6726993 , -0.9124343 ,\n",
       "       -0.20811436, -0.53903896,  1.4176455 , -0.46985295, -0.9732159 ,\n",
       "        0.10648848,  0.69618565, -0.18309641,  0.5128085 ,  0.9872698 ,\n",
       "       -0.5602139 ,  0.7148856 ,  0.09455778, -1.0495578 , -0.03309458,\n",
       "        0.7899805 ,  0.9872811 , -0.12219412, -0.5620203 ,  0.4235168 ,\n",
       "        0.40387183, -0.8787664 , -0.6562188 , -0.2712405 ,  1.2257813 ,\n",
       "       -1.1729699 , -1.162402  ,  0.01518461, -1.4339955 , -0.10510466,\n",
       "        1.0898159 , -0.22241019, -0.42248482,  0.72029775, -0.03143048,\n",
       "        1.1148102 ,  0.28222924,  0.74960387,  0.5037726 ,  1.87225   ,\n",
       "        0.10271132, -0.89295954,  0.4083665 ,  0.10096797, -0.36101657,\n",
       "        0.80612326, -0.4279562 ,  0.20755748, -0.9981928 ,  0.17306508,\n",
       "        0.57417077,  0.25590542,  0.44442192, -0.23800258, -0.1565756 ,\n",
       "       -0.05000425, -0.7322015 , -1.7507703 ,  1.1291964 , -0.98587924,\n",
       "        0.78607404,  0.04865063,  0.25423446,  0.54226995,  1.5563413 ,\n",
       "       -0.621037  , -1.5657871 ,  0.3811379 , -0.8709946 ,  0.2698216 ,\n",
       "        1.2578759 ,  0.20818433,  0.6286007 ,  0.95      , -0.20409223,\n",
       "       -0.5103027 ,  0.31260878, -1.3255204 ,  1.3432045 , -0.4022786 ,\n",
       "        0.130037  , -0.5693116 , -0.5627297 ,  0.19565864,  0.44295636,\n",
       "       -0.048855  , -0.841922  , -0.16230719, -1.4364055 ,  0.22420023,\n",
       "        0.27848113,  0.79717207, -0.17246342, -0.09151105, -1.4641612 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 시각화\n",
    "\n",
    "https://projector.tensorflow.org/\n",
    "\n",
    "- embedding vector(tensor) 파일 (.tsv)\n",
    "- metadat 파일 (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 12:06:26,054 - word2vec2tensor - INFO - running c:\\Users\\USER\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input ted_en_w2v --output ted_en_w2v\n",
      "2025-02-20 12:06:26,054 - keyedvectors - INFO - loading projection weights from ted_en_w2v\n",
      "2025-02-20 12:06:27,173 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21462, 100) matrix of type float32 from ted_en_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-02-20T12:06:26.914735', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-02-20 12:06:27,903 - word2vec2tensor - INFO - 2D tensor file saved to ted_en_w2v_tensor.tsv\n",
      "2025-02-20 12:06:27,903 - word2vec2tensor - INFO - Tensor metadata file saved to ted_en_w2v_metadata.tsv\n",
      "2025-02-20 12:06:27,904 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input ted_en_w2v --output ted_en_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 한국어 Word Embedding\n",
    "- NSMC (Naver Sentiment Movie Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('naver_movie_ratings.txt', <http.client.HTTPMessage at 0x24750a24ef0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\",\n",
    "    filename=\"naver_movie_ratings.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 생성\n",
    "ratings_df = pd.read_csv('naver_movie_ratings.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    8\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결측치 확인 및 처리(제거)\n",
    "display(ratings_df.isnull().sum())\n",
    "\n",
    "ratings_df = ratings_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    많은 생각을 할 수 있는 영화~ 시간여행류의 스토리를 좋아하는 사람이라면 빠트릴 수...\n",
       "201    고소한 19 정말 재미있게 잘 보고 있습니다^^ 방송만 보면 털털하고 인간적이신 것...\n",
       "202                                                  가연세\n",
       "203                         goodgoodgoodgoodgoodgoodgood\n",
       "204                                           이물감. 시 같았다\n",
       "                             ...                        \n",
       "295                                   박력넘치는 스턴트 액션 평작이다!\n",
       "296                                      엄청 재미있다 명작이다 ~~\n",
       "297    나는 하정우랑 개그코드가 맞나보다 엄청 재밌게봤네요 특히 단발의사샘 장면에서 계속 ...\n",
       "298                                                적당 ㅎㅎ\n",
       "299                                    배경이 이쁘고 캐릭터도 귀엽네~\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df['document'][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글이 아닌 데이터 제거\n",
    "ratings_df['document'] = ratings_df['document'].replace(r'[^0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [08:59<00:00, 370.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "from tqdm import tqdm    # 진행도 시각화\n",
    "\n",
    "okt = Okt()\n",
    "ko_stopwords = ['은', '는', '이', '가', '을', '를', '와', '과', '들', '도',\n",
    "                '부터', '까지', '에', '나', '너', '그', '걔', '얘']    # 한국어 불용어\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in tqdm(ratings_df['document']):\n",
    "    tokens = okt.morphs(sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16841, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=preprocessed_data,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0    # CBOW\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화관', 0.9396284222602844),\n",
       " ('틀어주다', 0.8187416195869446),\n",
       " ('케이블', 0.7927759289741516),\n",
       " ('학교', 0.7759482264518738),\n",
       " ('티비', 0.7276421785354614),\n",
       " ('투니버스', 0.695606529712677),\n",
       " ('개봉관', 0.6917786002159119),\n",
       " ('인터넷', 0.683275043964386),\n",
       " ('메가박스', 0.6818107962608337),\n",
       " ('방금', 0.6805811524391174)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('극장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7329322"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('김혜수', '박서준')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.wv.save_word2vec_format('naver_movie_ratings_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 12:42:01,986 - word2vec2tensor - INFO - running c:\\Users\\USER\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v\n",
      "2025-02-20 12:42:01,986 - keyedvectors - INFO - loading projection weights from naver_movie_ratings_w2v\n",
      "2025-02-20 12:42:03,001 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (16841, 100) matrix of type float32 from naver_movie_ratings_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-02-20T12:42:02.718774', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-02-20 12:42:03,576 - word2vec2tensor - INFO - 2D tensor file saved to naver_movie_ratings_w2v_tensor.tsv\n",
      "2025-02-20 12:42:03,576 - word2vec2tensor - INFO - Tensor metadata file saved to naver_movie_ratings_w2v_metadata.tsv\n",
      "2025-02-20 12:42:03,577 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
