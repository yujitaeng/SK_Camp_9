{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "- 자연어 처리: 컴퓨터가 인간의 언어를 이해하고, 생성하며 활용하는 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.NLP 기술 적용분야\n",
    "\n",
    "1. 자연어 이해\n",
    "    * 감성분석 Sentiment Analysis (이 리뷰가 긍정인가 부정인가)\n",
    "    * 주제분류 Topic Modeling (이 뉴스의 내용을 정치/스포츠/과학 등으로 분류)\n",
    "    * 형태소 분석\n",
    "    * 개체명인식 Named Entity Recognition 사람이름/지역 이름 등을 구분\n",
    "    * 철자법 교정 Spelling correction\n",
    "    * 스팸 탐지 Spam Detection\n",
    "\n",
    "2. 자연어 생성\n",
    "    * 문장 생성 Text Generation\n",
    "    * 번역 Text Translation\n",
    "    * 대화 Chatbot\n",
    "    * 요약 Text Summarization\n",
    "    * 이미지/비디오 설명 Image/Video Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.발전단계\n",
    "\n",
    "#### NLP에 대한 접근 방법 (Language Modeling)\n",
    "| **구분**            | **기존 방식 (Data-Driven Approach)**                                                  | **최근 방식 (Neural Network)**                                                                                 |\n",
    "|---------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| **주요 특징**       | 2010년대 이전 전통적 방식<br>언어 전문가에 의한 고품질 sample data 확보가 중요          | Deep Learning 이용<br>Word Embedding 기반 전체 입력 문장 단위 처리<br>어순, 단어의 의미, 문맥 파악을 스스로 학습 |\n",
    "| **접근 방식**       | 규칙기반 (Rule-Based): 언어학 기반의 rule-based program                                | 언어 전문가 불필요                                                                                            |\n",
    "| **기술 요소**       | 통계기반 (Statistical Machine Translation):<br>말뭉치(Corpus) 기반 통계 모델 및 전통적 ML 적용 | Word Embedding<br>Bidirectional Recurrent Neural Network<br>Encoder-decoder (seq2seq) model<br>Attention model<br>Transformer model<br>BERT/GPT-3 등 |\n",
    "\n",
    "\n",
    "\n",
    "##### 해결된 / 대부분 해결된 / 좋은 성과를 내는 NLP분야\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <th>Resolved</th>\n",
    "    <th>Mostly Resolved</th>\n",
    "    <th>Making Good Progress</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>스팸분류, 품사결정, 개체명 인식, 기본 문서 생성, 간단한 질의 응답</td>\n",
    "    <td>감성분석, 구문분석, 기계번역, 정보추출, 요약, 이미지-텍스트 통합</td>\n",
    "    <td>복잡한 질의응답, 의역, 대화, 비디오-텍스트 통합, 다중모달 감성 분석, 창의적 문서 생성</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\" colspan=\"2\"><strong>전통적 기법<strong> 규칙기반, 통계기반, 전통적 ML기반</td>\n",
    "    <td align=\"center\">-</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td  align=\"center\" colspan=\"3\"><strong>딥러닝 기법</strong> 최근 NLP 기술</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**대부분의 자연어 처리 문제는 \"분류\" 문제이다.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.용어\n",
    "\n",
    "**말뭉치 Corpus (collection of texts)**\n",
    "- 자연어 처리에서 분석하거나 학습에 사용하는 텍스트 데이터의 모음이다.\n",
    "- 특정 주제의 뉴스 기사 모음, 소셜 미디어 게시글 모음, 법률 문서 등.\n",
    "- 일반적으로 특정 도메인에 따라 구성되며, 크고 다양한 데이터를 포함하는 것이 이상적이다.\n",
    "- 예전에는 품사, 형태소 등의 보조적 의미를 추가하여 구조적인 형태로 정리해 놓은 것 포함했었다.\n",
    "\n",
    "**토큰 Token**\n",
    "- 자연어 문서를 분석하기 위해 긴 문자열을 작은 단위로 나눈 것.\n",
    "- 텍스트를 분석하기 위해 나누는 기본 단위로, 일반적으로 단어, 문자, 구두점 등이 포함될 수 있다.\n",
    "- 문장 \"I love NLP!\"의 토큰은 [\"I\", \"love\", \"NLP\", \"!\"]로 나눌 수 있다.\n",
    "- Tokenize: 문자열을 여러 개의 조각, 즉 여러 개의 Token(토큰, 단어)들로 쪼개는 것을 말한다.\n",
    "- 특수 token: `<START>`, `<EOS>`, `<UNK>`, `<PAD>`, etc\n",
    "\n",
    "**문장 Sentence**\n",
    "- 자연어 텍스트에서 문장부호나 문법적 단위에 따라 나눠지는 문맥의 단위이다. (Sequence of words)\n",
    "- \"NLP is fascinating. I enjoy learning it.\"는 두 개의 문장으로 나뉜다.\n",
    "- 문장 분리(Sentence Segmentation)는 일반적으로 구두점을 기준으로 하지만, 특정 언어에서 더 복잡할 수 있다.\n",
    "\n",
    "**단어 Words**\n",
    "- 문장에서 의미를 가지는 최소 단위이다.(Sequence of meaningful characters)\n",
    "- \"I love NLP.\"의 단어는 [\"I\", \"love\", \"NLP\"]이다.\n",
    "- 띄어쓰기나 언어 규칙에 따라 나뉜다.\n",
    "    - 영어의 경우 공백이나 구두점(,;) 으로 구분할 수 있다.   \n",
    "    - 일부 언어(중국어, 일본어 등)는 띄어쓰기가 없어 추가 처리가 필요하다\n",
    "    - 한국어, 일본어의 경우는 보통 공백 없이 문장을 작성해도 단어 단위로 읽을 수 있다. (한국어에 띄어쓰기가 적용되기 시작한것은 1900년대 이후이다.)\n",
    "\n",
    "**형태소 Morpheme**\n",
    "- 의미를 가지는 가장 작은 언어 단위로, 단어보다 더 세부적으로 나뉠 수 있다.\n",
    "- 영어의 \"unbreakable\"은 [un-, break, -able]로 나뉜다.\n",
    "    - un-: 접두사(부정 의미)\n",
    "    - break: 어근\n",
    "    - -able: 접미사(가능 의미)\n",
    "- 형태소 분석(Morphological Analysis)을 통해 어근, 접사 등을 파악할 수 있다. 한국어와 같이 형태소가 중요한 언어에서 많이 사용된다. 한글 형태소 분석기나 Sub-word 기법이 사용된다.\n",
    "\n",
    "**불용어 Stop-words**\n",
    "- 불용어. 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들\n",
    "- 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없다.\n",
    "- NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.단어표현\n",
    "\n",
    "![](https://d.pr/i/zLBglx+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 표현방식 구분\n",
    "1. 국소 표현방식 (이산 표현방식)\n",
    "    \n",
    "    국소 표현방식은 각 단어를 고유한 숫자나 코드로 표현하는 방법이다. 즉, 단어 그 자체를 보고 고유한 값을 맵핑하여 표현한다. 예를 들어, 'puppy(강아지)', 'cute(귀여운)', 'lovely(사랑스러운)'라는 단어에 각각 1번, 2번, 3번 같은 숫자를 매핑하는 방식이다. 이렇게 하면 단어 간의 관계나 의미를 반영하지 않기 때문에 의미를 잘 전달하기 어려운 단점이 있다.\n",
    "\n",
    "2. 분산 표현방식 (연속 표현방식)\n",
    "\n",
    "    분산 표현방식은 단어를 주변 단어들과의 관계를 참고하여 표현하는 방법이다. 예를 들어, 'puppy(강아지)'라는 단어는 그 주변에 자주 등장하는 'cute(귀여운)'나 'lovely(사랑스러운)' 같은 단어들이 있으므로, 'puppy'는 'cute', 'lovely'한 느낌을 가지고 있다고 표현할 수 있다. 이렇게 주변 단어들을 참고하여 단어의 의미나 뉘앙스를 더 잘 반영할 수 있다. 분산 표현방식은 단어의 문맥을 반영할 수 있기 때문에 더 정교하고 의미를 잘 전달할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count-based Word Representation (빈도 기반 단어 표현) - 국소 표현방식\n",
    "- 빈도 기반 단어 표현은 단어의 출현 빈도나 패턴을 기준으로 단어를 벡터로 표현하는 방법이다. \n",
    "- 대표적인 방식으로는 **Bag of Words (BoW)** 와 **TF-IDF**가 있다.\n",
    "\n",
    "**Bag of Words (BoW)**<br>\n",
    "BoW는 문서에서 등장한 단어들의 빈도수를 기반으로 단어를 벡터로 변환한다. 이 방식은 단어의 순서나 문맥은 무시하고, 단어가 몇 번 등장했는지만을 중요시한다. 예를 들어, '강아지', '고양이', '새'라는 단어가 있을 때, 각 단어의 등장 빈도수를 세서 벡터로 표현한다.\n",
    "  \n",
    "**TF-IDF**<br>\n",
    "TF-IDF는 단어의 빈도수(TF)와 역문서 빈도수(IDF)를 결합하여 단어의 중요도를 계산한다. 자주 등장하는 단어는 가중치를 줄여주고, 드물게 등장하는 단어는 더 높은 가중치를 부여하여 벡터로 변환한다.\n",
    "\n",
    "- 이 방식들은 기본적으로 **단어 빈도**에 기반하여 단어를 벡터로 표현한다.\n",
    "- 하지만 이 방식의 단점은 **희소 벡터**(sparse vector)를 생성한다는 것이다.\n",
    "- 즉, 단어 집합이 클수록 벡터의 차원이 커지고, 단어 간의 문맥적 유사성을 반영하지 못하는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding (임베딩) - 분산 표현방식\n",
    "- 임베딩은 단어를 고차원 공간에서 저차원 밀집 벡터(dense vector)로 변환하는 방법이다.\n",
    "- 이 방법은 주로 **신경망**을 사용해서 학습된 벡터를 말하며, 대표적인 임베딩 방법으로 **Word2Vec**, **GloVe**, **FastText**, **BERT** 등이 있다.\n",
    "\n",
    "**Word2Vec**<br>\n",
    "Word2Vec은 단어의 의미적 유사성을 반영하기 위해 주변 단어들과의 관계를 학습하여 단어를 저차원 벡터로 표현한다. Skip-gram 모델이나 CBOW 모델을 사용하여 단어 임베딩을 학습한다. 예를 들어, 'puppy'와 'dog'는 의미가 비슷하므로, 두 단어는 벡터 공간에서 가까운 위치를 갖게 된다.\n",
    "\n",
    "**GloVe**<br>\n",
    "GloVe는 단어 간의 공기 확률(co-occurrence probability)을 기반으로 단어 벡터를 학습하는 방법이다. 단어들이 함께 등장하는 빈도수를 분석해 벡터를 생성하고, 문맥에서의 통계적 정보도 반영하여 밀집 벡터를 만든다.\n",
    "\n",
    "**BERT 등 Transformer 모델들**<br>\n",
    "BERT는 문맥과 문법 정보를 활용해 단어를 동적으로 임베딩한다. 이전 방식들과 달리 BERT는 단어가 사용된 문맥에 따라 서로 다른 벡터로 변환될 수 있다. 예를 들어, 같은 단어라도 문맥에 따라 다른 의미를 가질 수 있기 때문에, BERT는 문맥에 맞는 임베딩을 제공한다.\n",
    "\n",
    "- 임베딩 방식은 **의미적 유사성**을 반영한 밀집 벡터를 생성할 수 있다.\n",
    "- 이러한 벡터는 보통 수십에서 수백 차원으로 표현되며, 단어의 의미적 유사성과 문맥 정보를 효과적으로 반영할 수 있어 Count-based 방식보다 성능이 뛰어나다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
